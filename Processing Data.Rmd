---
title: "NutNet Phenology with NDVI"
author: "Ellen Esch"
date: "`r format(Sys.time(), '%d %B %Y')`"
# output:
#   word_document: default
#   pdf_document: default
#   html_document: default
output: 
  github_document:
    toc: true
urlcolor: blue
---

```{r packages, include=F, echo=F, message=F}
library("tidyverse")
library("sf")
library("rnaturalearth")
library("cowplot")
library("lubridate")
library("raster")
```

# Overview

This document walks through the steps necessary to:
-
-


Please change `process_raw` equal to `TRUE` if you need to download and process raw data. In most cases, this will likely remain as `FALSE`, becuase intermediate (processed) files *are* stored on GitHub. 

You'll need to read in the location of the sites being used in this analysis.
You'll also need to store some files on your local machine, so set a path to your desired location.

```{r filelocations, }
sites <- read_csv('./data/NutNetGreening_2019.10.15_ee.csv', col_types = cols())
localdir <- "/Users/ellen/Desktop/Ellen/Guelph/Project_Andrew phenology/pheno_localdata"
dropboxdir <- "/Users/ellen/Dropbox/NutNet data"

process_raw <- FALSE #TRUE
```

```{r moresetup, echo=F, include = F}
#for some of the analyses need to change the spatial form
sites_nutnet <- sites %>%
  dplyr::select(site_code, "GEE coordinates") %>%
  separate("GEE coordinates", sep = ',', into = c('long', 'lat')) %>%
  filter(!is.na(long)) %>%
  mutate(lat = as.numeric(lat), long = as.numeric(long))
sites_nutnet_sp <- sites_nutnet
coordinates(sites_nutnet_sp) <- ~long + lat

sites_nutnet_sp2<- sites_nutnet_sp %>% st_as_sf()
```

And just for fun, here is a map of the sites in this analysis

```{r sitemap, echo=F}
world <- ne_countries(scale = "medium", returnclass = "sf")
(ggplot(data = world, label = site)+
  geom_sf() +
  geom_point(data = sites, aes (x = longitude, y = latitude), col = "orange2") +
  theme_bw()+
  labs(title = "111 sites in analysis here"))
```

## Process weather & climate data

**Skip to step 5 if not needing to re-create data. Steps 1-4 walk through raw data downloads.**

1) Download montly [precipitation data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.04/data/pre) onto your local machine (large files). You will have to either create an account or log in. Download 4 time periods (you want the files with the 'nc' in the name):

- 1981-1990
- 1991-2000
- 2001-2010
- 2011-2019

```{r precipdata, echo=F}
### create precipitation data for the 4 distinct time periods
if (process_raw == TRUE) {
cruv <- "pre"
timeframe <- '1981.1990.'
source("./get-CRU-TS.R")

timeframe <- '1991.2000.'
source("./get-CRU-TS.R")

timeframe <- '2001.2010.'
source("./get-CRU-TS.R")

timeframe <- '2011.2019.'
source("./get-CRU-TS.R")

} else print("not processing raw data")
```


2) Repeat with montly [temperature data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.04/data/tmp).


```{r temperaturedata, echo = F}
if (process_raw == TRUE) {
cruv <- 'tmp'
timeframe <- '1981.1990.'
source("./get-CRU-TS.R")

timeframe <- '1991.2000.'
source("./get-CRU-TS.R")

timeframe <- '2001.2010.'
source("./get-CRU-TS.R")

timeframe <- '2011.2019.'
source("./get-CRU-TS.R")
} else print("not processing raw data")

```

3) Write a dataframe with merged monthly temperature and precipitation data

```{r climatedf, echo=F}
if (process_raw == TRUE) {
eightys <- read_csv("./data/weather/CRU-monthly-tmp1981.1990.csv", col_types = cols()) %>% 
  full_join(read_csv("./data/weather/CRU-monthly-pre1981.1990.csv", col_types = cols())) %>%
  dplyr::select(-date, -month, -year)

ninetys <- read_csv("./data/weather/CRU-monthly-tmp1991.2000.csv", col_types = cols()) %>% 
  full_join(read_csv("./data/weather/CRU-monthly-pre1991.2000.csv", col_types = cols())) %>%
  dplyr::select(-date, -month, -year)

thousands <- read_csv("./data/weather/CRU-monthly-tmp2001.2010.csv", col_types = cols()) %>% 
  full_join(read_csv("./data/weather/CRU-monthly-pre2001.2010.csv", col_types = cols())) %>%
  dplyr::select(-date, -month, -year)

tens <- read_csv("./data/weather/CRU-monthly-tmp2011.2019.csv", col_types = cols()) %>% 
  full_join(read_csv("./data/weather/CRU-monthly-pre2011.2019.csv", col_types = cols())) %>%
  dplyr::select(-date, -month, -year)

monthlyweather <- bind_rows(eightys, ninetys, thousands, tens) %>%
  rename(sitename = site_code) %>% 
  mutate(Month = month(plotdate)) 

write.csv(
  monthlyweather,
  './data/weather/monthlyweather.csv',
  row.names = F
)
} else print("not processing raw data")

```

4) Download [30 year averages from WorldClim](https://www.worldclim.org/data/worldclim21.html) onto your local machine. Download the most detailed spatial level (30 seconds) for:

- average temperature (tavg_30s)
- precipitaiton (preci_30s)

```{r worldclim, echo=F}
if (process_raw == TRUE) {

#####
# temperature
#####

WC1 <- raster(paste0(localdir, '/wc2.1_30s_tavg_01.tif'))
wc1 <- raster::extract(WC1, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC2 <- raster(paste0(localdir, '/wc2.1_30s_tavg_02.tif'))
wc2 <- raster::extract(WC2, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC3 <- raster(paste0(localdir, '/wc2.1_30s_tavg_03.tif'))
wc3 <- raster::extract(WC3, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC4 <- raster(paste0(localdir, '/wc2.1_30s_tavg_04.tif'))
wc4 <- raster::extract(WC4, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC5 <- raster(paste0(localdir, '/wc2.1_30s_tavg_05.tif'))
wc5 <- raster::extract(WC5, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC6 <- raster(paste0(localdir, '/wc2.1_30s_tavg_06.tif'))
wc6 <- raster::extract(WC6, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC7 <- raster(paste0(localdir, '/wc2.1_30s_tavg_07.tif'))
wc7 <- raster::extract(WC7, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC8 <- raster(paste0(localdir, '/wc2.1_30s_tavg_08.tif'))
wc8 <- raster::extract(WC8, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC9 <- raster(paste0(localdir, '/wc2.1_30s_tavg_09.tif'))
wc9 <- raster::extract(WC9, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC10 <- raster(paste0(localdir, '/wc2.1_30s_tavg_10.tif'))
wc10 <- raster::extract(WC10, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC11 <- raster(paste0(localdir, '/wc2.1_30s_tavg_11.tif'))
wc11 <- raster::extract(WC11, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

WC12 <- raster(paste0(localdir, '/wc2.1_30s_tavg_12.tif'))
wc12 <- raster::extract(WC12, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)


AvgClimate <- full_join(wc1, wc2, by = c("ID", "sitename")) %>%
  full_join(wc3, by = c("ID", "sitename")) %>%
  full_join(wc4, by = c("ID", "sitename")) %>%
  full_join(wc5, by = c("ID", "sitename")) %>%
  full_join(wc6, by = c("ID", "sitename")) %>%
  full_join(wc7, by = c("ID", "sitename")) %>%
  full_join(wc8, by = c("ID", "sitename")) %>%
  full_join(wc9, by = c("ID", "sitename")) %>%
  full_join(wc10, by = c("ID", "sitename")) %>%
  full_join(wc11, by = c("ID", "sitename")) %>%
  full_join(wc12, by = c("ID", "sitename"))  %>%
  gather(Month, AvgTempLTA, -ID, -sitename) %>%
  mutate(Month = recode(Month, "wc2.1_30s_tavg_01" = 1,
                        "wc2.1_30s_tavg_02" = 2, 
                        "wc2.1_30s_tavg_03" = 3,
                        "wc2.1_30s_tavg_04" = 4, 
                        "wc2.1_30s_tavg_05" = 5,
                        "wc2.1_30s_tavg_06" = 6,
                        "wc2.1_30s_tavg_07" = 7, 
                        "wc2.1_30s_tavg_08" = 8, 
                        "wc2.1_30s_tavg_09" = 9,
                        "wc2.1_30s_tavg_10" = 10,
                        "wc2.1_30s_tavg_11" = 11,
                        "wc2.1_30s_tavg_12" = 12)) 


####
#precip
#####

preWC1 <- raster(paste0(localdir, '/wc2.1_30s_prec_01.tif'))
prewc1 <- raster::extract(preWC1, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC2 <- raster(paste0(localdir, '/wc2.1_30s_prec_02.tif'))
prewc2 <- raster::extract(preWC2, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC3 <- raster(paste0(localdir, '/wc2.1_30s_prec_03.tif'))
prewc3 <- raster::extract(preWC3, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC4 <- raster(paste0(localdir, '/wc2.1_30s_prec_04.tif'))
prewc4 <- raster::extract(preWC4, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC5 <- raster(paste0(localdir, '/wc2.1_30s_prec_05.tif'))
prewc5 <- raster::extract(preWC5, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC6 <- raster(paste0(localdir, '/wc2.1_30s_prec_06.tif'))
prewc6 <- raster::extract(preWC6, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC7 <- raster(paste0(localdir, '/wc2.1_30s_prec_07.tif'))
prewc7 <- raster::extract(preWC7, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC8 <- raster(paste0(localdir, '/wc2.1_30s_prec_08.tif'))
prewc8 <- raster::extract(preWC8, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC9 <- raster(paste0(localdir, '/wc2.1_30s_prec_09.tif'))
prewc9 <- raster::extract(preWC9, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC10 <- raster(paste0(localdir, '/wc2.1_30s_prec_10.tif'))
prewc10 <- raster::extract(preWC10, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC11 <- raster(paste0(localdir, '/wc2.1_30s_prec_11.tif'))
prewc11 <- raster::extract(preWC11, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)

preWC12 <- raster(paste0(localdir, '/wc2.1_30s_prec_12.tif'))
prewc12 <- raster::extract(preWC12, sites_nutnet_sp2, fun = max, df = TRUE) %>% 
  mutate(sitename = sites_nutnet_sp2$site_code)


AvgClimatePrecip <- full_join(prewc1, prewc2, by = c("ID", "sitename")) %>%
  full_join(prewc3, by = c("ID", "sitename")) %>%
  full_join(prewc4, by = c("ID", "sitename")) %>%
  full_join(prewc5, by = c("ID", "sitename")) %>%
  full_join(prewc6, by = c("ID", "sitename")) %>%
  full_join(prewc7, by = c("ID", "sitename")) %>%
  full_join(prewc8, by = c("ID", "sitename")) %>%
  full_join(prewc9, by = c("ID", "sitename")) %>%
  full_join(prewc10, by = c("ID", "sitename")) %>%
  full_join(prewc11, by = c("ID", "sitename")) %>%
  full_join(prewc12, by = c("ID", "sitename"))  %>%
  gather(Month, AvgPrecipLTA, -ID, -sitename) %>%
  mutate(Month = recode(Month, "wc2.1_30s_prec_01" = 1,
                        "wc2.1_30s_prec_02" = 2, 
                        "wc2.1_30s_prec_03" = 3,
                        "wc2.1_30s_prec_04" = 4, 
                        "wc2.1_30s_prec_05" = 5,
                        "wc2.1_30s_prec_06" = 6,
                        "wc2.1_30s_prec_07" = 7, 
                        "wc2.1_30s_prec_08" = 8, 
                        "wc2.1_30s_prec_09" = 9,
                        "wc2.1_30s_prec_10" = 10,
                        "wc2.1_30s_prec_11" = 11,
                        "wc2.1_30s_prec_12" = 12)) 


longterm_climate <- full_join(AvgClimate, AvgClimatePrecip)

write.csv(
  longterm_climate,
  './data/weather/longterm_avgclimate.csv',
  row.names = F
)

} else print("not processing raw data")


```

5) Look at the monthly deviations from the long term average. 

A plot illustrate a problem here (with an easy solution). At Cowichan there are no consistent longitudinal trends, but it is obvious that worldclim (long term average) and ceda (montly) data don't always necessarily align (expected becuase they have differnt methods, resolutions, etc.). This suggests that ceda should be used to calculate averages (1981-2019, 39 years) as well as anomolies. 


```{r weatherdev, echo=F, message=F, warning=F}
worldclim <- read_csv("./data/weather/longterm_avgclimate.csv")
montlycru <- read_csv("./data/weather/monthlyweather.csv")

#ceda precip in mm/month
#world clim is also in mm/month
ClimChange_wc <- montlycru %>% 
  full_join(worldclim) %>%
  mutate(TempDifference = tmp_degrees_Celsius - AvgTempLTA) %>%
  mutate(PrecipDifference = `pre_mm/month` - AvgPrecipLTA) %>%
  separate(sitename, into = c('site', 'b'), sep = "[.]") %>%
  dplyr::select(-b)

# ClimChange_wc %>% filter(site=="sval") %>%
#   ggplot(aes(x=plotdate, y=TempDifference))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   facet_wrap(~Month)+
#   labs(y = "Anomaly from 1970-2000, celsius", x = "Year", title = "Temperature Anomalies, Svalbard")+
#   theme_cowplot()+
#   geom_hline(yintercept = 0)+
#   scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

ClimChange_wc %>% filter(site=="cowi") %>%
  ggplot(aes(x=plotdate, y=PrecipDifference))+
  geom_point()+
  geom_smooth(method = "lm")+
  facet_wrap(~Month)+
  labs(y = "Anomaly from 1970-2000, mm", x = "Year", title = "Precip Anomalies, Cowichan")+
  theme_cowplot()+
  geom_hline(yintercept = 0)+
  scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


Indeed, using CEDA data to calculate averages as well as anomolies proves to be a much more logical metric. Cowichan data seems *much* more logical now. 


```{r ceda_avg, echo=F, message=F, warning=F}
# min(montlycru$plotdate)
# max(montlycru$plotdate)

cruavg <- montlycru %>% 
  group_by(sitename, Month) %>%
  summarise(avg_precip = mean(`pre_mm/month`, na.rm = T),
         avg_temp = mean(`tmp_degrees_Celsius`, na.rm = T), 
         N = n())

ClimChange <- montlycru %>%
  full_join(cruavg) %>%
  mutate(TempDifference = tmp_degrees_Celsius - avg_temp) %>%
  mutate(PrecipDifference = `pre_mm/month` - avg_precip) %>%
  separate(sitename, into = c('site', 'b'), sep = "[.]") %>%
  dplyr::select(-b)

# ClimChange %>% filter(site=="sval") %>%
#   ggplot(aes(x=plotdate, y=TempDifference))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   facet_wrap(~Month)+
#   labs(y = "Anomaly from 39 yrs CEDA data, celsius", x = "Year", title = "Temperature Anomalies, Svalbard")+
#   theme_cowplot()+
#   geom_hline(yintercept = 0)+
#   scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

ClimChange %>% filter(site=="cowi") %>%
  ggplot(aes(x=plotdate, y=PrecipDifference))+
  geom_point()+
  geom_smooth(method = "lm")+
  facet_wrap(~Month)+
  labs(y = "Anomaly from 39 yrs CEDA data, mm", x = "Year", title = "Precip Anomalies, Cowichan")+
  theme_cowplot()+
  geom_hline(yintercept = 0)+
  scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Process nitrogen deposition data

**NOTE** Previously N deposition was extracted from "F. Dentener, Global maps of atmospheric nitrogen deposition, 1860, 1993, and 2050. ORNL DAAC  (2006)". However, two things have happened: a) r spatial has a bug (it's known, and I guess they're working on it so this could be fixed), b) a 07 Dec 2020 update added n deposition data to the NutNet Dropbox. Thus, we are proceeding with the file in the dropbox....

Additionally, note that the new data in the NutNet dropbox allows us to better look at *change* in N deposition over time. Since we are evaluating phenological/NDVI change over time as well, this seems like a logical addition. 

```{r ndep, echo=F, include=F}
# ndep <- raster(paste0(localdir, '/N-deposition1993.tif'),
#                crs = '+init=EPSG:4326')
# crs(ndep) <- CRS('+init=EPSG:4326') #https://epsg.io/4326
# 
# ndep_merge <- raster::extract(ndep, sites_nutnet_sp2, fun = max, df = TRUE) %>%
#   mutate(sitename = sites_nutnet_sp2$site_code) %>%
#   separate(sitename, into = c('site', 'b'), sep = "[.]") %>%
#   dplyr::select(-b, -ID)

ndep <- read_csv(paste0(dropboxdir, "/site-n-deposition-07-December-2020.csv")) %>%
  dplyr::select(site_code, year, total_deposition) %>%
  filter(site_code %in% sites_nutnet_sp2$site_code) %>%
  filter(year == 1984 | year == 2016) %>%
  pivot_wider(names_from = year, values_from = total_deposition) %>%
  mutate(ndep_change_2016_1984 = `2016` - `1984`) %>%
  rename(ndep_2016 = `2016`,
         ndep_1984 = `1984`) %>%
  separate(site_code, into = c('site', 'b'), sep = "[.]") %>%
  dplyr::select(-b)

```

## Process site-specific info

Need to connect to dropbox, this will be different for each user
```{r connectDB, message=F, warning=F}
dropbox <- read_csv(paste0(dropboxdir, '/comb-by-plot-clim-soil-diversity-07-December-2020.csv'), 
                    col_types = cols(),
                    na = c("NULL", "NA"))
```

```{r siteinfo, echo=F, message=F, warning=F}
siteclim<- dropbox %>%
  filter(trt == "Control") %>%
  separate(site_code, into = c('site', 'country'), sep='[.]') %>%
  group_by(site) %>%
  summarise(Managed = mean(managed), #sdManaged = sd(managed),
            Burned = mean(burned), #sdBurned=sd(burned),
            Elevation = mean(elevation), #sdElevation=sd(elevation),
            RainMAP = mean(MAP_v2, na.rm=T), #sdRainPET=sd(MAP_v2, na.rm=T),
            TempMAT = mean(MAT_v2), #sdTempMAT = sd(MAT_v2),
            Richness = mean(site_richness),#, sdRichness = sd(site_richness))
            NativeRichness = mean(site_native_richness),
            IntRichness = mean(site_introduced_richness),
            FractionExotic = (IntRichness/(NativeRichness+IntRichness)),
            SoilC = mean(pct_C, na.rm = T),
            SoilN = mean(pct_N, na.rm = T),
            SoilP = mean(ppm_P, na.rm = T), 
            SoilK = mean(ppm_K, na.rm = T)) %>%
  filter(!is.na(Richness))


obsyr_even <- dropbox %>%
  filter((year == (first_nutrient_year - 1)) | experiment_type == "Observational") %>%
  group_by(site_code) %>%  #filter(site_code == "lagoas.br")
  summarise(ObsYrMeanEven = mean(evenness, na.rm = T),
            ObsYrMeanInvSimp = mean(inverse_simpson, na.rm = T)) %>%
    separate(site_code, into = c('site', 'country'), sep='[.]') %>% 
  dplyr::select(-country)

obsyr_even %>%
  ggplot(aes(x = ObsYrMeanEven, y = ObsYrMeanInvSimp)) + 
  geom_point()
  
region <- dropbox %>%
  separate(site_code, into = c('site', 'country'), sep='[.]') %>% 
  dplyr::select(site, continent, habitat, latitude, longitude)

SiteInfo <- ndep_merge %>% 
  left_join(region) %>% 
  inner_join(siteclim) %>%
  left_join(obsyr_even)

# kable(SiteInfo %>%
#   select(site:SoilK, N.deposition1993)) %>%
#   kable_styling("striped", full_width = F)

###
# Site biomass
###
yrprod1<- dropbox %>%
  separate(site_code, into = c('site', 'country'), sep = '[.]') %>%
  filter(trt == "Control") %>% 
  group_by(site, year) %>%
  summarise(ANPP = mean(live_mass, na.rm = T),
            sdANPP = sd(live_mass, na.rm = T)) %>%
  rename(gs = year) %>% filter(!is.na(sdANPP))

yrprod <- yrprod1 %>% group_by(site) %>%
  summarise(YrsBiomass = n()) %>%
  left_join(yrprod1)

# #plot
# yrprod %>%
#   gather(Msmr, Value, -site, -gs) %>%
#   mutate(Msmr = dplyr::recode(Msmr, "ANPP" = "ANPP g/m2", "sdANPP" = "St. Dev ANPP")) %>%
#   ggplot(aes(Value)) +
#   geom_histogram() +
#   facet_wrap(~ Msmr, scales = "free") +
#   theme_cowplot() +
#   labs(x = "", title = "Biomass Production")
# 
```


