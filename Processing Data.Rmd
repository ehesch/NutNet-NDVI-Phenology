---
title: "NutNet Phenology with NDVI"
author: "Ellen Esch"
date: "`r format(Sys.time(), '%d %B %Y')`"
# output:
#   word_document: default
#   pdf_document: default
#   html_document: default
output: 
  github_document:
    toc: true
urlcolor: blue
---

```{r packages, include=F, echo=F, message=F}
library("tidyverse")
library("sf")
library("rnaturalearth")
library("cowplot")
library("lubridate")
library("raster")
library("knitr")
library("kableExtra")
```

# Overview

This document walks through the steps necessary to:
-
-


Please change `process_raw_climate` equal to `TRUE` if you need to download and process raw data. In most cases, this will likely remain as `FALSE`, becuase intermediate (processed) files *are* stored on GitHub. 

Likewise, change `process_raw_ndvi` equal to `TRUE` if you need to process the landsat output. 

You'll need to read in the location of the sites being used in this analysis.
You'll also need to store some files on your local machine, so set a path to your desired location.

```{r filelocations, }
sites <- read_csv("./data/NutNetGreening_2019.10.15_ee.csv", col_types = cols())
localdir <- "/Users/ellen/Desktop/Ellen/Guelph/Project_Andrew phenology/pheno_localdata"
dropboxdir <- "/Users/ellen/Dropbox/NutNet data"

process_raw_climate <- FALSE # TRUE

process_raw_ndvi <- TRUE # TRUE
```

```{r moresetup, echo=F, include = F}
#for some of the analyses need to change the spatial form
sites_nutnet <- sites %>%
  dplyr::select(site_code, "GEE coordinates") %>%
  separate("GEE coordinates", sep = ',', into = c('long', 'lat')) %>%
  filter(!is.na(long)) %>%
  mutate(lat = as.numeric(lat), long = as.numeric(long)) %>%
  filter(site_code != "ethass.au") #this site just isn't greening up....google earth confirms it appears to be mostly rock where the lat/long are pointing to. So this needs pi confirmation to be included
sites_nutnet_sp <- sites_nutnet
coordinates(sites_nutnet_sp) <- ~long + lat

sites_nutnet_sp2<- sites_nutnet_sp %>% st_as_sf()
```

And just for fun, here is a map of the sites in this analysis

```{r sitemap, echo=F}
world <- ne_countries(scale = "medium", returnclass = "sf")
ggplot(data = world, label = site) +
  geom_sf() +
  geom_point(data = sites_nutnet, aes(x = long, y = lat), col = "orange2") +
  theme_bw() 
```

## Process weather & climate data

**Skip to step 5 if not needing to re-create data. Steps 1-4 walk through raw data downloads.**

1) Download montly [precipitation data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.04/data/pre) onto your local machine (large files). You will have to either create an account or log in. Download 4 time periods (you want the files with the 'nc' in the name):

- 1981-1990
- 1991-2000
- 2001-2010
- 2011-2019

```{r precipdata, echo=F}
### create precipitation data for the 4 distinct time periods
if (process_raw_climate == TRUE) {
  cruv <- "pre"
  timeframe <- "1981.1990."
  source("./get-CRU-TS.R")

  timeframe <- "1991.2000."
  source("./get-CRU-TS.R")

  timeframe <- "2001.2010."
  source("./get-CRU-TS.R")

  timeframe <- "2011.2019."
  source("./get-CRU-TS.R")
} else {
  print("not processing raw CLIMATE data")
}
```


2) Repeat with montly [temperature data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.04/data/tmp).


```{r temperaturedata, echo = F}
if (process_raw_climate == TRUE) {
  cruv <- "tmp"
  timeframe <- "1981.1990."
  source("./get-CRU-TS.R")

  timeframe <- "1991.2000."
  source("./get-CRU-TS.R")

  timeframe <- "2001.2010."
  source("./get-CRU-TS.R")

  timeframe <- "2011.2019."
  source("./get-CRU-TS.R")
} else {
  print("not processing raw CLIMATE data")
}

```

3) Write a dataframe with merged monthly temperature and precipitation data

```{r climatedf, echo=F}
if (process_raw_climate == TRUE) {
  eightys <- read_csv("./data/weather/CRU-monthly-tmp1981.1990.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre1981.1990.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  ninetys <- read_csv("./data/weather/CRU-monthly-tmp1991.2000.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre1991.2000.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  thousands <- read_csv("./data/weather/CRU-monthly-tmp2001.2010.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre2001.2010.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  tens <- read_csv("./data/weather/CRU-monthly-tmp2011.2019.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre2011.2019.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  monthlyweather <- bind_rows(eightys, ninetys, thousands, tens) %>%
    rename(sitename = site_code) %>%
    mutate(Month = month(plotdate))

  write.csv(
    monthlyweather,
    "./data/weather/monthlyweather.csv",
    row.names = F
  )
} else {
  print("not processing raw CLIMATE data")
}

```

4) **See point #5**, but in general.... Download [30 year averages from WorldClim](https://www.worldclim.org/data/worldclim21.html) onto your local machine. Download the most detailed spatial level (30 seconds) for:

- average temperature (tavg_30s)
- precipitaiton (preci_30s)

```{r worldclim, echo=F}
if (process_raw_climate == TRUE) {

  #####
  # temperature
  #####

  WC1 <- raster(paste0(localdir, "/wc2.1_30s_tavg_01.tif"))
  wc1 <- raster::extract(WC1, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC2 <- raster(paste0(localdir, "/wc2.1_30s_tavg_02.tif"))
  wc2 <- raster::extract(WC2, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC3 <- raster(paste0(localdir, "/wc2.1_30s_tavg_03.tif"))
  wc3 <- raster::extract(WC3, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC4 <- raster(paste0(localdir, "/wc2.1_30s_tavg_04.tif"))
  wc4 <- raster::extract(WC4, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC5 <- raster(paste0(localdir, "/wc2.1_30s_tavg_05.tif"))
  wc5 <- raster::extract(WC5, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC6 <- raster(paste0(localdir, "/wc2.1_30s_tavg_06.tif"))
  wc6 <- raster::extract(WC6, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC7 <- raster(paste0(localdir, "/wc2.1_30s_tavg_07.tif"))
  wc7 <- raster::extract(WC7, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC8 <- raster(paste0(localdir, "/wc2.1_30s_tavg_08.tif"))
  wc8 <- raster::extract(WC8, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC9 <- raster(paste0(localdir, "/wc2.1_30s_tavg_09.tif"))
  wc9 <- raster::extract(WC9, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC10 <- raster(paste0(localdir, "/wc2.1_30s_tavg_10.tif"))
  wc10 <- raster::extract(WC10, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC11 <- raster(paste0(localdir, "/wc2.1_30s_tavg_11.tif"))
  wc11 <- raster::extract(WC11, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC12 <- raster(paste0(localdir, "/wc2.1_30s_tavg_12.tif"))
  wc12 <- raster::extract(WC12, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)


  AvgClimate <- full_join(wc1, wc2, by = c("ID", "sitename")) %>%
    full_join(wc3, by = c("ID", "sitename")) %>%
    full_join(wc4, by = c("ID", "sitename")) %>%
    full_join(wc5, by = c("ID", "sitename")) %>%
    full_join(wc6, by = c("ID", "sitename")) %>%
    full_join(wc7, by = c("ID", "sitename")) %>%
    full_join(wc8, by = c("ID", "sitename")) %>%
    full_join(wc9, by = c("ID", "sitename")) %>%
    full_join(wc10, by = c("ID", "sitename")) %>%
    full_join(wc11, by = c("ID", "sitename")) %>%
    full_join(wc12, by = c("ID", "sitename")) %>%
    gather(Month, AvgTempLTA, -ID, -sitename) %>%
    mutate(Month = recode(Month,
      "wc2.1_30s_tavg_01" = 1,
      "wc2.1_30s_tavg_02" = 2,
      "wc2.1_30s_tavg_03" = 3,
      "wc2.1_30s_tavg_04" = 4,
      "wc2.1_30s_tavg_05" = 5,
      "wc2.1_30s_tavg_06" = 6,
      "wc2.1_30s_tavg_07" = 7,
      "wc2.1_30s_tavg_08" = 8,
      "wc2.1_30s_tavg_09" = 9,
      "wc2.1_30s_tavg_10" = 10,
      "wc2.1_30s_tavg_11" = 11,
      "wc2.1_30s_tavg_12" = 12
    ))


  ####
  # precip
  #####

  preWC1 <- raster(paste0(localdir, "/wc2.1_30s_prec_01.tif"))
  prewc1 <- raster::extract(preWC1, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC2 <- raster(paste0(localdir, "/wc2.1_30s_prec_02.tif"))
  prewc2 <- raster::extract(preWC2, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC3 <- raster(paste0(localdir, "/wc2.1_30s_prec_03.tif"))
  prewc3 <- raster::extract(preWC3, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC4 <- raster(paste0(localdir, "/wc2.1_30s_prec_04.tif"))
  prewc4 <- raster::extract(preWC4, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC5 <- raster(paste0(localdir, "/wc2.1_30s_prec_05.tif"))
  prewc5 <- raster::extract(preWC5, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC6 <- raster(paste0(localdir, "/wc2.1_30s_prec_06.tif"))
  prewc6 <- raster::extract(preWC6, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC7 <- raster(paste0(localdir, "/wc2.1_30s_prec_07.tif"))
  prewc7 <- raster::extract(preWC7, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC8 <- raster(paste0(localdir, "/wc2.1_30s_prec_08.tif"))
  prewc8 <- raster::extract(preWC8, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC9 <- raster(paste0(localdir, "/wc2.1_30s_prec_09.tif"))
  prewc9 <- raster::extract(preWC9, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC10 <- raster(paste0(localdir, "/wc2.1_30s_prec_10.tif"))
  prewc10 <- raster::extract(preWC10, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC11 <- raster(paste0(localdir, "/wc2.1_30s_prec_11.tif"))
  prewc11 <- raster::extract(preWC11, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC12 <- raster(paste0(localdir, "/wc2.1_30s_prec_12.tif"))
  prewc12 <- raster::extract(preWC12, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)


  AvgClimatePrecip <- full_join(prewc1, prewc2, by = c("ID", "sitename")) %>%
    full_join(prewc3, by = c("ID", "sitename")) %>%
    full_join(prewc4, by = c("ID", "sitename")) %>%
    full_join(prewc5, by = c("ID", "sitename")) %>%
    full_join(prewc6, by = c("ID", "sitename")) %>%
    full_join(prewc7, by = c("ID", "sitename")) %>%
    full_join(prewc8, by = c("ID", "sitename")) %>%
    full_join(prewc9, by = c("ID", "sitename")) %>%
    full_join(prewc10, by = c("ID", "sitename")) %>%
    full_join(prewc11, by = c("ID", "sitename")) %>%
    full_join(prewc12, by = c("ID", "sitename")) %>%
    gather(Month, AvgPrecipLTA, -ID, -sitename) %>%
    mutate(Month = recode(Month,
      "wc2.1_30s_prec_01" = 1,
      "wc2.1_30s_prec_02" = 2,
      "wc2.1_30s_prec_03" = 3,
      "wc2.1_30s_prec_04" = 4,
      "wc2.1_30s_prec_05" = 5,
      "wc2.1_30s_prec_06" = 6,
      "wc2.1_30s_prec_07" = 7,
      "wc2.1_30s_prec_08" = 8,
      "wc2.1_30s_prec_09" = 9,
      "wc2.1_30s_prec_10" = 10,
      "wc2.1_30s_prec_11" = 11,
      "wc2.1_30s_prec_12" = 12
    ))


  longterm_climate <- full_join(AvgClimate, AvgClimatePrecip)

  write.csv(
    longterm_climate,
    "./data/weather/longterm_avgclimate.csv",
    row.names = F
  )
} else {
  print("not processing raw CLIMATE data")
}

```

5) Look at the monthly deviations from the long term average. 

A plot illustrate a problem here (with an easy solution). At Cowichan there are no consistent longitudinal trends, but it is obvious that worldclim (long term average) and ceda (montly) data don't always necessarily align (expected becuase they have differnt methods, resolutions, etc.). This suggests that ceda should be used to calculate averages (1981-2019, 39 years) as well as anomolies. 


```{r weatherdev, echo=F, message=F, warning=F}
worldclim <- read_csv("./data/weather/longterm_avgclimate.csv")
montlycru <- read_csv("./data/weather/monthlyweather.csv")

# ceda precip in mm/month
# world clim is also in mm/month
ClimChange_wc <- montlycru %>%
  full_join(worldclim) %>%
  mutate(TempDifference = tmp_degrees_Celsius - AvgTempLTA) %>%
  mutate(PrecipDifference = `pre_mm/month` - AvgPrecipLTA) %>%
  separate(sitename, into = c("site", "b"), sep = "[.]") %>%
  dplyr::select(-b)

# ClimChange_wc %>% filter(site=="sval") %>%
#   ggplot(aes(x=plotdate, y=TempDifference))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   facet_wrap(~Month)+
#   labs(y = "Anomaly from 1970-2000, celsius", x = "Year", title = "Temperature Anomalies, Svalbard")+
#   theme_cowplot()+
#   geom_hline(yintercept = 0)+
#   scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

ClimChange_wc %>%
  filter(site == "cowi") %>%
  ggplot(aes(x = plotdate, y = PrecipDifference)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~Month) +
  labs(y = "Anomaly from 1970-2000, mm", x = "Year", title = "Precip Anomalies, Cowichan") +
  theme_cowplot() +
  geom_hline(yintercept = 0) +
  scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


Indeed, using CEDA data to calculate averages as well as anomolies proves to be a much more logical metric. Cowichan data seems *much* more logical now. 


```{r ceda_avg, echo=F, message=F, warning=F}
# min(montlycru$plotdate)
# max(montlycru$plotdate)

cruavg <- montlycru %>%
  group_by(sitename, Month) %>%
  summarise(
    avg_precip = mean(`pre_mm/month`, na.rm = T),
    avg_temp = mean(`tmp_degrees_Celsius`, na.rm = T),
    N = n()
  )

ClimChange <- montlycru %>%
  full_join(cruavg) %>%
  mutate(TempDifference = tmp_degrees_Celsius - avg_temp) %>%
  mutate(PrecipDifference = `pre_mm/month` - avg_precip) %>%
  separate(sitename, into = c("site", "b"), sep = "[.]") %>%
  dplyr::select(-b)

# ClimChange %>% filter(site=="sval") %>%
#   ggplot(aes(x=plotdate, y=TempDifference))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   facet_wrap(~Month)+
#   labs(y = "Anomaly from 39 yrs CEDA data, celsius", x = "Year", title = "Temperature Anomalies, Svalbard")+
#   theme_cowplot()+
#   geom_hline(yintercept = 0)+
#   scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

ClimChange %>%
  filter(site == "cowi") %>%
  ggplot(aes(x = plotdate, y = PrecipDifference)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~Month) +
  labs(y = "Anomaly from 39 yrs CEDA data, mm", x = "Year", title = "Precip Anomalies, Cowichan") +
  theme_cowplot() +
  geom_hline(yintercept = 0) +
  scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Process nitrogen deposition data

**NOTE** Previously N deposition was extracted from "F. Dentener, Global maps of atmospheric nitrogen deposition, 1860, 1993, and 2050. ORNL DAAC  (2006)". However, two things have happened: a) r spatial has a bug (it's known, and I guess they're working on it so this could be fixed), b) a 07 Dec 2020 update added n deposition data to the NutNet Dropbox. Thus, we are proceeding with the file in the dropbox....

Additionally, note that the new data in the NutNet dropbox allows us to better look at *change* in N deposition over time. Since we are evaluating phenological/NDVI change over time as well, this seems like a logical addition. 

```{r ndep, echo=F, include=F}
# ndep <- raster(paste0(localdir, '/N-deposition1993.tif'),
#                crs = '+init=EPSG:4326')
# crs(ndep) <- CRS('+init=EPSG:4326') #https://epsg.io/4326
#
# ndep_merge <- raster::extract(ndep, sites_nutnet_sp2, fun = max, df = TRUE) %>%
#   mutate(sitename = sites_nutnet_sp2$site_code) %>%
#   separate(sitename, into = c('site', 'b'), sep = "[.]") %>%
#   dplyr::select(-b, -ID)

ndep <- read_csv(paste0(dropboxdir, "/site-n-deposition-07-December-2020.csv")) %>%
  dplyr::select(site_code, year, total_deposition) %>%
  filter(site_code %in% sites_nutnet_sp2$site_code) %>%
  filter(year == 1984 | year == 2016) %>%
  pivot_wider(names_from = year, values_from = total_deposition) %>%
  mutate(ndep_change_2016_1984 = `2016` - `1984`) %>%
  rename(
    ndep_2016 = `2016`,
    ndep_1984 = `1984`
  ) %>%
  separate(site_code, into = c("site", "b"), sep = "[.]") %>%
  dplyr::select(-b)

```

## Process site-specific info (biomass, richness, etc)

Connect to the NutNet Dropbox, and go from there. 

```{r siteinfo, echo=F, message=F, warning=F}
dropbox <- read_csv(paste0(dropboxdir, "/comb-by-plot-clim-soil-diversity-07-December-2020.csv"),
  col_types = cols(),
  na = c("NULL", "NA")
)

siteclim <- dropbox %>%
  filter(trt == "Control") %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  group_by(site) %>%
  summarise(
    Managed = mean(managed), # sdManaged = sd(managed),
    Burned = mean(burned), # sdBurned=sd(burned),
    Elevation = mean(elevation), # sdElevation=sd(elevation),
    RainMAP = mean(MAP_v2, na.rm = T), # sdRainPET=sd(MAP_v2, na.rm=T),
    TempMAT = mean(MAT_v2), # sdTempMAT = sd(MAT_v2),
    Richness = mean(site_richness), # , sdRichness = sd(site_richness))
    NativeRichness = mean(site_native_richness),
    IntRichness = mean(site_introduced_richness),
    FractionExotic = (IntRichness / (NativeRichness + IntRichness)),
    SoilC = mean(pct_C, na.rm = T),
    SoilN = mean(pct_N, na.rm = T),
    SoilP = mean(ppm_P, na.rm = T),
    SoilK = mean(ppm_K, na.rm = T)
  ) %>%
  filter(!is.na(Richness))


obsyr_even <- dropbox %>%
  filter((year == (first_nutrient_year - 1)) | experiment_type == "Observational") %>%
  group_by(site_code) %>% # filter(site_code == "lagoas.br")
  summarise(
    ObsYrMeanEven = mean(evenness, na.rm = T),
    ObsYrMeanInvSimp = mean(inverse_simpson, na.rm = T)
  ) %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  dplyr::select(-country)

# obsyr_even %>%
#   ggplot(aes(x = ObsYrMeanEven, y = ObsYrMeanInvSimp)) +
#   geom_point()

region <- dropbox %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  dplyr::select(site, continent, habitat, latitude, longitude)

SiteInfo <- ndep %>%
  left_join(region) %>%
  inner_join(siteclim) %>%
  left_join(obsyr_even)

# kable(SiteInfo %>%
#   select(site:SoilK, N.deposition1993)) %>%
#   kable_styling("striped", full_width = F)

###
# Site biomass
###
yrprod1 <- dropbox %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  filter(trt == "Control") %>%
  group_by(site, year) %>%
  summarise(
    ANPP = mean(live_mass, na.rm = T),
    sdANPP = sd(live_mass, na.rm = T)
  ) %>%
  rename(gs = year) %>%
  filter(!is.na(sdANPP))

yrprod <- yrprod1 %>%
  group_by(site) %>%
  summarise(YrsBiomass = n()) %>%
  left_join(yrprod1)

# #plot
# yrprod %>%
#   gather(Msmr, Value, -site, -gs) %>%
#   mutate(Msmr = dplyr::recode(Msmr, "ANPP" = "ANPP g/m2", "sdANPP" = "St. Dev ANPP")) %>%
#   ggplot(aes(Value)) +
#   geom_histogram() +
#   facet_wrap(~ Msmr, scales = "free") +
#   theme_cowplot() +
#   labs(x = "", title = "Biomass Production")
# 
```


## Collect the Landsat NDVI data

For this analyses, I have focused on Landsat Satelites 4, 5, 7, and 8. All have a resolution of 30 m. Prior Landsats were more coarse (LS 1 & 2 = 80 m resolution, LS 3 = 40 m resolution). In [google earth engine](https://code.earthengine.google.com/), I have processed the top of atmosphere values, filtered data by band quality, and included both [tier 1 and tier 2 data](https://www.usgs.gov/core-science-systems/nli/landsat/landsat-collection-1?qt-science_support_page_related_con=1#qt-science_support_page_related_con). From tier 1 data, readings which indicated clear conditions or snow cover at time of imagery were kept. From tier 2 data, NDVI was only used when the band quality indicated snow cover. Tier 2 data were used as a supplement, because the snow covered satelite images were often not included tier 1 data. It is important to have sufficient landsat data coverage during snow-covered periods (if applicable!) so that accurate "troughs" can be identified.

As an FYI, Landsat coverage improved greatly with satelites 7 and 8. As such, some sites have very sparse coverage prior to 1999. However, the  scan-line corrector on landsat 7 failed on June 2003, meaning that some sites again had only spotty coverage until landsat 8's launch in 2003. 

At each NutNet site, I picked an adjacent area to the experimental plot locations. There, I took a 30 m buffer radius (a couple sites are different - see GEE code) around that point to be our "site." Sometimes this meant that multiple paths/rows of landsat were included, which is fine. Sometimes it might mean that the NutNet site is not accurately reflected in our data (if for example a grassland site occurs admist a bunch of trees...those trees might get included by using this method). Essentially, if the NDVI data looks really bad for a specific site, the accuracy and relevance of the location I chose is probably the first thing to check - how accurately does our designated "region of interest" reflect the landscape?

In google earth engine, the following was run. Apologies that it's long, messy, and that I'm not very good at creating loops within loops in JavaScript! There are some button options that pop up in GEE prompting the various extracted data to be saved. And that each site needs to get individually commented out and run. It's not pretty - but it does work well!!!

```GEE code not shown here due to length, but feel free to look at it in the markdown file```
```{r, include=F, echo=F}

# ## GEE code
# //create ROI
# /*
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(18.724477, 68.366069), {label:'abisko_se'}).buffer(30)]); //UPDATED
# var NAME = 'abisko_se';
# 
# var NAME = 'ahth_is'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.014311, 48.465041), {label:'amcamp_us'}).buffer(30)]);//UPDATED 
# var NAME = 'amcamp_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-19.672060, 65.133350), {label:'amlr_is'}).buffer(30)]); 
# var NAME = 'amlr_is'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-78.162, -0.467083333), {label:'anti_ec'}).buffer(30)]); 
# var NAME = 'anti_ec'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-81.218472, 27.170194), {label:'arch_us'}).buffer(30)]);//UPDATED 
# var NAME = 'arch_us' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(101.865677, 33.670918), {label:'azitwo_cn'}).buffer(30)]); //UPDATED
# var NAME = 'azitwo_cn';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(11.880336, 51.391717), {label:'badlau_de'}).buffer(30)]); //UPDATED
# var NAME = 'badlau_de' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-71.152318, -41.006807), {label:'bari_ar'}).buffer(30)]);
# var NAME = 'bari_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-99.652173, 42.245453), {label:'barta_us'}).buffer(30)]);
# var NAME = 'barta_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(11.584053, 49.921323), {label:'bayr_de'}).buffer(30)]);
# var NAME = 'bayr_de';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-105.233063, 39.972852), {label:'bldr_us'}).buffer(30)]);
# var NAME = 'bldr_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-95.087419, 39.596985), {label:'bnbt_us'}).buffer(30)]);
# var NAME = 'bnbt_us' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.966830, 44.278458), {label:'bnch_us'}).buffer(30)]);
# var NAME = 'bnch_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(147.253979, -36.873971), {label:'bogong_au'}).buffer(30)]);
# var NAME = 'bogong_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.958134, 44.277013), {label:'bttr_us'}).buffer(30)]);
# var NAME = 'bttr_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(151.606325, -26.892224), {label:'bunya_au'}).buffer(30)]);
# var NAME = 'bunya_au' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(151.139265, -27.735334), {label:'burrawan_au'}).buffer(30)]);
# var NAME = 'burrawan_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-8.992624, 53.072023), {label:'burren_ie'}).buffer(30)]);
# var NAME = 'burren_ie' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(82.708800, 42.884700), {label:'bynb_cn'}).buffer(30)]);
# var NAME = 'bynb_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-93.384990, 41.784526), {label:'cbgb_us'}).buffer(30)]);
# var NAME = 'cbgb_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-93.211968, 45.427225), {label:'cdcr_us'}).buffer(30)]);
# var NAME = 'cdcr_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-101.643424, 41.206284), {label:'cdpt_us'}).buffer(30)]);
# var NAME = 'cdpt_us';
# 
# var NAME = 'cereep_fr'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-58.264246, -36.275950), {label:'chilcas_ar'}).buffer(30)]);
# var NAME = 'chilcas_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-8.791102, 38.829049), {label:'comp_pt'}).buffer(30)]);
# var NAME = 'comp_pt';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.635063, 48.808390), {label:'cowi_ca'}).buffer(30)]); //UPDATED
# var NAME = 'cowi_ca';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(144.791878, -37.806923), {label:'derr_au'}).buffer(30)]);
# var NAME = 'derr_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.855257, 40.695596), {label:'doane_us'}).buffer(30)]);
# var NAME = 'doane_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(29.267586, -28.961042), {label:'drake_za'}).buffer(30)]);
# var NAME = 'drake_za'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.05025, 36.984861), {label:'elkh_us'}).buffer(30)]);
# var NAME = 'elkh_us';
# */
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-117.090227, 32.892409),{label:'elliot_us'}).buffer(30)]); 
# var NAME = 'elliot_us';
# /*
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(26.353695, 58.257967), {label:'elva_ee'}).buffer(30)]);
# var NAME = 'elva_ee';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(138.474286, -23.756611), {label:'ethamc_au'}).buffer(30)]);
# var NAME = 'ethamc_au'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(138.398970, -23.638610), {label:'ethass_au'}).buffer(30)]);
# var NAME = 'ethass_au'
# 
# var NAME = 'fnly_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(7.827216, 48.020054), {label:'free_de'}).buffer(30)]);
# var NAME = 'free_de'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(8.540363, 47.114728), {label:'frue_ch'}).buffer(30)]);
# var NAME = 'frue_ch';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(10.718500, 43.797100), {label:'gall_it'}).buffer(30)]);
# var NAME = 'gall_it'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(90.233300, 31.383330), {label:'gcnban_cn'}).buffer(30)]);
# var NAME = 'gcnban_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(91.066770, 30.496210), {label:'gcndan_cn'}).buffer(30)]);
# var NAME = 'gcndan_cn'
# 
# var NAME = 'gcnhai_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(102.577863, 32.828816), {label:'gcnhon_cn'}).buffer(30)]);
# var NAME = 'gcnhon_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(119.951580, 49.333000), {label:'gcnhul_cn'}).buffer(30)]);
# var NAME = 'gcnhul_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(92.009700, 31.643700), {label:'gcnnaq_cn'}).buffer(30)]);
# var NAME = 'gcnnaq_cn'
# 
# var NAME = 'gcnsui_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(106.966670, 41.416670), {label:'gcnura_cn'}).buffer(30)]);
# var NAME = 'gcnura_cn'
# 
# var NAME = 'gcnxil_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(106.929611, 37.309860), {label:'gcnyan_cn'}).buffer(30)]);
# var NAME = 'gcnyan_cn'
# 
# var NAME = 'gcnyou_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(30.292178, -29.284046), {label:'gilb_za'}).buffer(30)]);
# var NAME = 'gilb_za'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.035557, 46.869083), {label:'glac_us'}).buffer(30)]); 
# var NAME = 'glac_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.1396933, 41.339757), {label:'glcr_us'}).buffer(30)]); 
# var NAME = 'glcr_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-86.702686, 36.872137), {label:'hall_us'}).buffer(30)]); 
# var NAME = 'hall_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-119.497670, 42.723745), {label:'hart_us'}).buffer(30)]);
# var NAME = 'hart_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.551652, 36.386708), {label:'hast_us'}).buffer(30)]); //UPDATED
# var NAME = 'hast_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-0.645026, 51.413098), {label:'hero_uk'}).buffer(30)]);
# var NAME = 'hero_uk';
# 
# var NAME = 'hnvr_us'
# 
# var NAME = 'hogone_us'
# 
# var NAME = 'hogtwo_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.061648, 39.011633), {label:'hopl_us'}).buffer(30)]);
# var NAME = 'hopl_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(105.967778, 47.666528), {label:'hustai_mn'}).buffer(30)]);
# var NAME = 'hustai_mn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.241545, 37.405222), {label:'jasp_us'}).buffer(30)]);
# var NAME = "jasp_us"
# 
# var NAME = 'jena_de';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-106.787094, 32.530054), {label:'jorn_us'}).buffer(30)]);
# var NAME = 'jorn_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(18.323098, 68.392169), {label:'kark_se'}).buffer(30)]);
# var NAME = 'kark_se';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-85.391144, 42.409125), {label:'kbs_us'}).buffer(30)]);
# var NAME = 'kbs_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(78.008754, 32.319508), {label:'kibber_in'}).buffer(30)]);
# var NAME = "kibber_in"
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(130.951768, -16.110229), {label:'kidman_au'}).buffer(30)]);
# var NAME = 'kidman_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(20.87352408,  69.05678545), {label:'kilp_fi'}).buffer(30)]);
# var NAME = 'kilp_fi';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(143.851472, -36.324810), {label:'kiny_au'}).buffer(30)]);
# var NAME = 'kiny_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(23.794941, 58.710601), {label:'kirik_ee'}).buffer(30)]);
# var NAME = "kirik_ee";
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-79.537881, 44.024616), {label:'koffler_ca'}).buffer(30)]);
# var NAME = 'koffler_ca';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.582323, 39.070328), {label:'konz_us'}).buffer(30)]);
# var NAME = 'konz_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-51.798613, -20.983784), {label:'lagoas_br'}).buffer(30)]);
# var NAME = 'lagoas_br';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-95.182989, 43.384792), {label:'lake_us'}).buffer(30)]);
# var NAME = 'lake_us';
# 
# var NAME = 'lakta_se';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-2.627710, 53.985674), {label:'lancaster_uk'}).buffer(30)]);
# var NAME = 'lancaster_uk';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-124.048400, 46.614900), {label:'lead_us'}).buffer(30)]);
# var NAME = 'lead_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.128676, 44.205263), {label:'look_us'}).buffer(30)]);
# var NAME = 'look_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-101.897343, 33.600589), {label:'lubb_us'}).buffer(30)]);
# var NAME = 'lubb_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-57.424151, -37.714602), {label:'marc_ar'}).buffer(30)]);
# var NAME = 'marc_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-77.042078, 39.527459), {label:'mcdan_us'}).buffer(30)]);
# var NAME = 'mcdan_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.407466, 38.864058), {label:'mcla_us'}).buffer(30)]);
# var NAME = 'mcla_us';
# 
# var NAME = 'meto_us'''
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(143.321660, -22.483803), {label:'mitch_au'}).buffer(30)]);
# var NAME = 'mitch_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-114.000646,  46.664443), {label:'msla_us'}).buffer(30)]);
# var NAME = 'msla_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.453980, 46.870938), {label:'msum_us'}).buffer(30)]);
# var NAME = 'msum_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(117.611742, -31.781354), {label:'mtca_au'}).buffer(30)]);
# var NAME = 'mtca_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(137.661096, 35.231685), {label:'neba_jp'}).buffer(30)]);
# var NAME = 'neba_jp'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(146.006711, -36.895778), {label:'nilla_au'}).buffer(30)]);
# var NAME = 'nilla_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(5.749875, 52.059696), {label:'nioo_nl'}).buffer(30)]);
# var NAME = 'nioo_nl';
# 
# var NAME = 'niwo_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(106.886887, 47.769495), {label:'ovor_mn'}).buffer(30)]);
# var NAME = 'ovor_mn';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-64.434831, -36.586107), {label:'pampa_ar'}).buffer(30)]);
# var NAME = 'pampa_ar';
# 
# var NAME = 'pape_de';
# 
# var NAME = 'pich_ec'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(116.972561, -32.496899), {label:'ping_au'}).buffer(30)]);
# var NAME = 'ping_au'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(152.923522, -27.530424), {label:'pinj_au'}).buffer(30)]);
# var NAME = 'pinj_au';
# 
# var NAME = 'podo_ec'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-70.407124, -51.914837), {label:'potrok_ar'}).buffer(30)]);
# var NAME = 'potrok_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-90.169865, 30.517510), {label:'ramsay_us'}).buffer(30)]);
# var NAME = 'ramsay_us';
# 
# var NAME = 'rook_uk';
# 
# var NAME = "saana_fi';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-120.239269, 39.430489), {label:'sage_us'}).buffer(30)]);
# var NAME = 'sage_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-99.166935, 39.085843), {label:'saline_us'}).buffer(30)]);
# var NAME = 'saline_us';
# 
# var NAME = 'sava_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-120.021470, 34.696911), {label:'sedg_us'}).buffer(30)]);
# var NAME = 'sedg_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(34.857533, -2.429138), {label:'sereng_tz'}).buffer(30)]);
# var NAME = 'sereng_tz';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-106.691175, 34.360198), {label:'sevi_us'}).buffer(30)]);
# var NAME = 'sevi_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-104.767174, 40.816332), {label:'sgs_us'}).buffer(30)]);
# var NAME = 'sgs_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-112.196699, 44.244893), {label:'shps_us'}).buffer(30)]);
# var NAME = 'shps_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.283696, 39.235510), {label:'sier_us'}).buffer(30)]);
# var NAME = 'sier_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(74.742883, 14.419046), {label:'sirsi_in'}).buffer(30)]);
# var NAME = 'sirsi_in';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.624726, 48.206135), {label:'smith_us'}).buffer(30)]);
# var NAME = 'smith_us';
# 
# var NAME = 'spin_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-64.170575, -42.654068), {label:'spv_ar'}).buffer(30)]);
# var NAME = 'spv_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(30.716644, -29.810615), {label:'summ_za'}).buffer(30)]);
# var NAME = 'summ_za';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point( 16.44713167, 78.69017773 ), {label:'sval_no'}).buffer(30)]);
# var NAME = 'sval_no';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-97.349443, 31.044188), {label:'temple_us'}).buffer(30)]);
# var NAME = 'temple_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-17.090214, 65.901294), {label:'thth_is'}).buffer(30)]);
# var NAME = 'thth_is';
# 
# var NAME = 'tmlr_is';
# 
# var NAME = 'trel_us';
# 
# var NAME = 'tyso_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.740563, 36.867430), {label:'ucsc_us'}).buffer(30)]);
# var NAME = 'ucsc_us';
# 
# var NAME = "ufrec.us";
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(30.412772, -29.670833), {label:'ukul_za'}).buffer(30)]);
# var NAME = 'ukul_za';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-79.019186, 36.007745), {label:'unc_us'}).buffer(30)]);
# var NAME = 'unc_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-81.317171, 43.192809), {label:'uwo_ca'}).buffer(30)]);
# var NAME = 'uwo_ca';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(10.371966, 46.632543), {label:'valm_ch'}).buffer(30)]);
# var NAME = 'valm_ch';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-101.609734, 21.783367), {label:'vaqu_mx'}).buffer(30)]);
# var NAME = 'vaqu_mx';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(29.089939, 70.308117), {label:'varexp_no'}).buffer(30)]);
# var NAME = 'varexp_no';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(29.539800, 70.327970), {label:'vargrass_no'}).buffer(30)]);
# var NAME = 'vargrass_no';
# 
# var NAME = 'varheath_no';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(18.274396, 68.416979), {label:'vass_se'}).buffer(30)]);
# var NAME = 'vass_se';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-111.579722, 33.583333), {label:'wapatki_us'}).buffer(30)]);
# var NAME = 'wapatki_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(116.673782, 43.551476), {label:'xilin_cn'}).buffer(30)]);
# var NAME = 'xilin_cn';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(150.738804, -33.614034), {label:'yarra_au'}).buffer(30)]);
# var NAME = 'yarra_au';
# 
# */
# 
# 
# //LS 8
# var addNDVI_8 = function(image){
#   return image
#   .addBands(image.normalizedDifference(['B5','B4'])
#   .rename('NDVI'))
#   .float();
# };
# 
# //LS 754
# var addNDVI_754 = function(image){
#   return image
#   .addBands(image.normalizedDifference(['B4','B3'])
#   .rename('NDVI'))
#   .float();
# };
# 
# 
# //abisko
# var site_LS8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA')
#   .map(addNDVI_8)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
# 
# var site_LS7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
# 
# var site_LS5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
#   
# var site_LS4 = ee.ImageCollection('LANDSAT/LT04/C01/T1_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
#   
# var site_LS4_t2 = ee.ImageCollection('LANDSAT/LT04/C01/T2_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);  
# 
# var site_LS5_t2 = ee.ImageCollection('LANDSAT/LT05/C01/T2_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
#   
# var site_LS7_t2 = ee.ImageCollection('LANDSAT/LE07/C01/T2_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);  
# 
# var site_LS8_t2 = ee.ImageCollection('LANDSAT/LC08/C01/T2_TOA')
#   .map(addNDVI_8)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
# 
# 
# var site_8 = ee.FeatureCollection(site_LS8.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# var site_7 = ee.FeatureCollection(site_LS7.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# var site_5 = ee.FeatureCollection(site_LS5.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# var site_4 = ee.FeatureCollection(site_LS4.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# //
# var site_4_t2 = ee.FeatureCollection(site_LS4_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# var site_5_t2 = ee.FeatureCollection(site_LS5_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# var site_7_t2 = ee.FeatureCollection(site_LS7_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# var site_8_t2 = ee.FeatureCollection(site_LS8_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# Export.table.toDrive({
#   collection: site_8,
#   description: NAME.concat('_ls8_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_7,
#   description: NAME.concat('_ls7_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_5,
#   description: NAME.concat('_ls5_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_4,
#   description: NAME.concat('_ls4_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_8_t2,
#   description: NAME.concat('_ls8_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_7_t2,
#   description: NAME.concat('_ls7_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_5_t2,
#   description: NAME.concat('_ls5_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_4_t2,
#   description: NAME.concat('_ls4_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });

```

## Process the NDVI data

There are seemingly infiniate ways to process NDVI to pick out phenological dates. Quickly, we fitted cubic splines to each growing season at each site. We found that a single spline fit to each site (incompassing all growing years) was too sensitive to years with large spans of time without NDVI values. Then, we picked out the maximum NDVI from the fitted spline in each growing season. And the first date where NDVI >= 50 of the average range of NDVI (plus the avg. min) (as a green-up threshold). Originally I had used a fancy combination of derivatives (Buitenwerf et al), but they turned out to be very sensitive to missing values (problematic in snow-covered regions). 


Behind the scenes, some fancy (and not-so-fancy) data processing happened: 
1) dates were treated as radians to accomodate southern hemisphere and mediterranean sites with potential phenological dates spanning across years within the same growing season (for instance, a site with green-up in Dec 2018, NDVI max in March 2019, and brown down in May 2019; this is all classified as occuring during the 2019 growing season.)
2) At certain sites there are periods of the year where NDVI data are not available, either becuase of persistant seasonal clouds, algorithm confusion between clouds and snow (aka water refelctance), or poor radiometric calibration on the landsat satelites. Conversely, there may often be time periods in which the 10-day average landsat coverage of a site is way more frequent (for example, periods when both landsat 7 and 8 orbit, etc).  
3) NDVI values which were < 0 were reassigned a value of 0. This helped the fitted splines behave much better. 
4) Regarding weighting points within the splines, each growing season's maximum NDVI value was weighted at 1, and all other values at 0.5 in order to most accurately capture the peak. 

Also, some sites are strange - need to contact PIs eventually.
* lake_us, lakta_se, marc_ar, mitch_au, tyso_us, valm_ch, xilin_cn, bldr_us all needed larger radius buffers

Going thru the setps in more detail:

1) Create functions to convert linear (julian) calendar days into radian (circular) data. (becuase southern hemisphere/mediterranean sites which green up on, say julian day 365 on average...but if one year greens up on julian day 1, this is a 1 day difference NOT a -365 day difference)

```{r radianfxn echo=F}
circmean_degrees <- function(x) {
  x_radians <- x * pi / 180
  sinr <- sum(sin(x_radians))
  cosr <- sum(cos(x_radians))
  circmean_radians <- atan2(sinr, cosr)
  circmean_degree <- circmean_radians / pi * 180
  circmean_degree
}

circ_degrees <- function(x) {
  x_radians <- x * pi / 180
  sinr <- (sin(x_radians))
  cosr <- (cos(x_radians))
  circmean_radians <- atan2(sinr, cosr)
  circmean_degree <- circmean_radians / pi * 180
  circmean_degree
}
```

2) Filter the Landsat data to include only pixels identified as being "clear" or "snow" covered. **NOTE** This should probably be double checked, I started off very conservatively when deciding to include tier 2 data. Documentation could have been much more through, so it would be good to see if there are some additional data which should be included/excluded. 

Also we set the minimum NDVI to 0 to assist in the spline fitting. 

```{r readinphenodata, eval = T, echo = F, message = F, warning = F}

if (process_raw_ndvi == TRUE) {
  pheno_path <- paste0(localdir, "/landsat") # path to the data
  pheno_files <- dir(pheno_path, pattern = "*.csv") # get file names
  pheno_files_all <- tibble(filename = pheno_files) %>%
    mutate(file_contents = map(
      filename,
      ~ read_csv(file.path(pheno_path, .),
        skip = 2, col_type = cols(), col_names = FALSE
      )
    )) %>%
    unnest() %>%
    rename(row = X1, id = X2, longitude = X3, latitude = X4, NDVI = X5, BQA = X6, geo = X7) %>%
    filter(!is.na(NDVI), !is.na(BQA)) %>%
    separate(filename, sep = "_", into = c("site", "country", "ls", "tier")) %>%
    separate(id, sep = "_", into = c("landsat", "path", "date")) %>%
    mutate(tier = case_when(
      tier == "t1.csv" ~ 1,
      tier == "t2.csv" ~ 2
    )) %>%
    dplyr::select(-row, -geo) %>%
    mutate(Date = ymd(date))

  # landsat_codes2 <- read_csv(paste0(localdir, '/landsat_interpretation.csv'), col_types = cols())
  landsat_codes <- read_csv(paste0(localdir, "/landsat_band_interpretation.csv"), col_types = cols())

  pheno_data <- pheno_files_all %>%
    full_join(landsat_codes) %>%
    filter(action == "keep") %>% # remove=='keep') %>%
    filter(!is.na(NDVI), !is.na(BQA)) %>%
    group_by(site, Date) %>%
    summarise(NDVI = mean(NDVI)) %>%
    mutate(NDVI = if_else(NDVI < 0, 0, NDVI)) %>%
    arrange(site, Date) %>%
    filter(site != "ethass") #never greening up, middle of rock 
} else {
  print("not processing raw NDVI data")
}
```

3) 
* Convert linear calendar years into circular data. 
* Then need to identify the average trough date
* A "growing season" or "phenological year" is subsequently identified as the 365 days following the "trough date" plus an GSType 90 day buffer on either end. So a total of 90 + 365 + 90 days. I know this is not totally right since leap years are confusing. 
* And then take the Landsat readings and duplicate the readings that fall within the "shoulder season". So that we have the total, long data set of all NDVI readings
* We also assign a green-up critera, which is the 50% of the average range of NDVI at each site ((avg max - avg min * 50) + avg min). This green-up threshold is constant at all years within a site. 

```{r getShoulderSeasonsTroughs, eval = T, echo = F, message = F, warning = F}
if (process_raw_ndvi == TRUE) {

  # first need to fit a single cubic spline to all data, and then get the average date which has the minimum NDVI
  min_ndvi_name <- levels(as.factor(pheno_data$site)) %>%
    tibble() %>%
    rename(site = ".") %>%
    mutate(sitenum = seq(1:nrow(.)))

  ndviemptydataframe <- tibble(
    x = numeric(),
    ndvi = numeric(),
    sitenum = numeric(0)
  )

  min_ndvi <- ndviemptydataframe
  for (i in c(1:nrow(min_ndvi_name))) {
    b <- i #30 = elliott, 33 = ethass
    plot.filter <- (min_ndvi_name %>% full_join(pheno_data, by = "site")) %>% filter(sitenum == b)
    spline <- smooth.spline(x = plot.filter$Date, y = plot.filter$NDVI)
    ndvi <- as_tibble(predict(spline, as.numeric(ymd(min(plot.filter$Date))):as.numeric(ymd(max(plot.filter$Date))), deriv = 0)) %>%
      rename(ndvi = y) %>%
      mutate(
        Date = as_date(x),
        sitenum = b
      )
    min_ndvi <- bind_rows(min_ndvi, ndvi)
    
    # ggplotly(min_ndvi %>% 
    #   ggplot(aes(x = Date, y = ndvi)) +
    #   geom_point())
    
  }

  min_ndvifitted <- full_join(min_ndvi, min_ndvi_name)

  siteinfo <- min_ndvifitted %>%
    mutate(Year = year(Date)) %>%
    group_by(site, Year) %>%
    summarise(ndvi_min = min(ndvi), ndvi_max = max(ndvi)) %>%
    rename(ndvi = ndvi_min) %>%
    mutate(DaysInYear = ifelse(leap_year(Year) == "TRUE", 366, 365)) %>%
    left_join((min_ndvifitted %>%
      mutate(Year = year(Date)))) %>%
    mutate(Julian = yday(Date)) %>%
    mutate(DegreeJulian = Julian / DaysInYear * 360) %>%
    group_by(site) %>%
    summarise(
      TroughDateRadian = circmean_degrees(DegreeJulian),
      avg_ndvi_max = mean(ndvi_max)
    ) %>%
    mutate(
      TroughDate = TroughDateRadian * 365 / 360,
      MinDate = (as_date(TroughDate)),
      Start = yday(MinDate),
      End = yday(MinDate) - 1
    ) %>%
    dplyr::select(-TroughDate, -MinDate, -avg_ndvi_max) %>%
    full_join(min_ndvi_name)

  pheno_data_all_1 <- full_join(pheno_data, siteinfo)

  # assign growing season
  GSmain <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (2019 - 1981)), year = rep(1982:2019, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 364,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalstart, normalend, year) %>%
    mutate(ID = paste(site, year, sep = "_")) %>%
    dplyr::select(-site, -year) %>%
    gather(var, Date, -ID) %>%
    group_by(ID) %>%
    distinct() %>%
    complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
    dplyr::select(-var) %>%
    separate(ID, into = c("site", "GrowingSeason"), sep = "_") %>%
    mutate(
      GSType = "mainGS",
      GrowingSeason = as.numeric(GrowingSeason),
      weights = 999
    )

  # add 90 days pre/post growing season
  GSearly <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (2019 - 1981)), year = rep(1982:2019, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 364,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalstart, earlystart, year) %>%
    mutate(ID = paste(site, year, sep = "_")) %>%
    dplyr::select(-site, -year) %>%
    gather(var, Date, -ID) %>%
    group_by(ID) %>%
    distinct() %>%
    complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
    dplyr::select(-var) %>%
    separate(ID, into = c("site", "GrowingSeason"), sep = "_") %>%
    mutate(
      GSType = "earlyGS",
      GrowingSeason = as.numeric(GrowingSeason),
      weights = 0.5
    ) %>%
    arrange(site, GrowingSeason, Date)

  GSlate <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (2019 - 1981)), year = rep(1982:2019, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 365,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalend, lateend, year) %>%
    gather(var, Date, -site, -year) %>%
    arrange(site, year, Date) %>%
    mutate(ID = paste(site, year, sep = "_")) %>%
    dplyr::select(-site, -year) %>%
    group_by(ID) %>%
    distinct() %>%
    complete(Date = seq.Date((min(Date)), max(Date), by = "day")) %>%
    dplyr::select(-var) %>%
    separate(ID, into = c("site", "GrowingSeason"), sep = "_") %>%
    mutate(
      GSType = "lateGS",
      GrowingSeason = as.numeric(GrowingSeason),
      weights = 0.5
    ) %>%
    arrange(site, GrowingSeason, Date)

  GScutoffs <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (2019 - 1981)), year = rep(1982:2019, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 364,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalstart, normalend, year) %>%
    rename(GrowingSeason = year)

  pheno_early <- inner_join(GSearly, pheno_data_all_1)
  pheno_late <- GSlate %>% inner_join(pheno_data_all_1)
  pheno_normal <- GSmain %>% right_join(pheno_data_all_1)

  # so this is the final data, with duped early/late growing season
  pheno_data_all_2 <- bind_rows(pheno_early, pheno_late) %>%
    bind_rows(pheno_normal) %>%
    left_join(GScutoffs) %>%
    arrange(site, GrowingSeason, Date)

  siteinfo2 <- filter(pheno_data_all_2, GSType == "mainGS") %>%
    group_by(site, GrowingSeason) %>%
    summarise(
      MIN = min(NDVI),
      MAX = max(NDVI)
    ) %>%
    group_by(site) %>%
    summarise(
      avg_ndvi_min = mean(MIN),
      avg_ndvi_max = mean(MAX)
    ) %>%
    mutate(Max50 = (.5 * (avg_ndvi_max - avg_ndvi_min) + avg_ndvi_min)) %>%
    full_join(siteinfo)

  # to weight by max NDVI
  pheno_data_all <- pheno_normal %>%
    group_by(site, GrowingSeason) %>%
    summarise(
      N = n(),
      MAXNDVI = max(NDVI),
      MINNDVI = min(NDVI)
    ) %>%
    full_join(pheno_data_all_2) %>%
    mutate(weights = ifelse((NDVI == MAXNDVI & weights == 999), 1, .5))
  
  write.csv(pheno_data_all, "pheno_data_all.csv")
  
} else {
  print("not processing raw NDVI data")
}

pheno_data_all <- read_csv("./pheno_data_all.csv")

kable(pheno_data_all %>% group_by(site) %>%
  summarise(
    `Start Date` = min(Date),
    `End Date` = max(Date),
    `N images (w/ shoulders)` = n(),
    `Site number` = mean(sitenum),
    `Julian GS start date` = mean(Start)
  ) %>%
  full_join(siteinfo2) %>%
  rename(`NDVI Green-up threshold` = Max50) %>%
  dplyr::select(`Site number`, site, `Start Date`, `End Date`, `Julian GS start date`, `NDVI Green-up threshold`, `N images (w/ shoulders)`)) %>%
  kable_styling("striped", full_width = F)

```


4) 
* Then we fit cubic splines to each site-year combo. These splines are weighted (max NDVI in each year = weight 1; all others = weight 0.5), and have a smoothing parameter of 0.5 (spar = 0.5).
* Each site-year combo must have atleast 10 NDVI readings (measurementscutoff) for us to determine phenological dates. 
* Some extra critera are added to determining the phenological dates.

  + Green-up:
    - The first derivative needed to indicate that the NDVI was increasing (first deriv > 0)
    - The NDVI needed to be above 0 (a low threshold, very easy!)
    - Green-up date was the FIRST DAY where NDVI was above the green-up threshold + 0.02 NDVI (I forget what all the problems were if it was simply just the cutoff, but they were there)
    - The green-up date must occur within the "main" growing season (not within the shoulder seasons).

  + Maximum:
    - Must occur after green-up date but within the "main" growing season (no shoulder seasons)
    - If there was no green-up date (due to sparse data, no rain, fire, whatever), we still forced a maximum NDVI date

  + Senescense:
    - The first derivative must be negative (indicating decling function/NDVI)
    - Must occur after the maximum NDVI date
    - NDVI must be less than the green-threshold (< Max50) but ABOVE the threshold - 0.05 (ndvi > (Max50 - .05))). Had to do this so that it wouldn't pick a senescence date that had a crazy low NDVI in the event of sparse data. 

* Then all the dates were "redialed" as departure from the average start/end at each site to get change over time

```{r splineseachGS, eval = T, echo = F, message = F, warning = F}
if (process_raw_ndvi == TRUE) {

  ##################
# with shoulder data; EACH SITE-YEAR COMBO NEEDS ITS OWN SPLINE. 
##################
emptydataframe <- tibble(
  x = numeric(),
  ndvi = numeric(),
  Date = as_date(character()),
  first.deriv = numeric(),
  Julian = numeric(),
  sitenum = numeric(0),
  site = character(),
  GrowingSeason = numeric(),
  Type = character(),
  gs = numeric(),
  END = character()
)

site_summary <- emptydataframe
for (i in c(1:nrow(siteinfo2))) {
  a <- i
  plot.filter <-  pheno_data_all %>% filter(sitenum == a) 
  gs_summary <- emptydataframe
  for (i in c((year(min(plot.filter$Date)) + 1):year(max(plot.filter$Date)) )) { #since no GSType shoulder in the first year, so gotta exlude. 
    gs <- i
    lowndvi <- (0)
    measurementscutoff <- 10 #min number of observations to have in gs -->> 7 = way too lax, 10 = better! 
    gs.filter <- plot.filter %>% filter(GrowingSeason == gs) 
    
    spline <- if (nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff ) {
      smooth.spline(x = gs.filter$Date, y = gs.filter$NDVI, spar = .5, w=gs.filter$weights) 
    } else {NA} 
    
    ndvi <-  if (nrow(filter(gs.filter, GSType=='mainGS')) >= measurementscutoff) {
      as_tibble(predict(spline, as.numeric(ymd(min(gs.filter$Date))):as.numeric(ymd(max(gs.filter$Date))), deriv = 0)) %>%
        rename(ndvi = y) %>%
        mutate(Date = as_date(x)) 
    } else {NA}
    
    first.deriv <- if (nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff) {
      as_tibble(predict(spline, as.numeric(ymd(min(gs.filter$Date))):as.numeric(ymd(max(gs.filter$Date))), deriv = 1)) %>%
        rename(first.deriv = y) %>%
        mutate(norm.first.deriv = ((1--1) / (max(first.deriv) - min(first.deriv)) * (first.deriv - max(first.deriv)) + 1))
    } else {NA}
    
    smoothed.data <- if(nrow(filter(gs.filter, GSType == 'mainGS'))>= measurementscutoff) {
      full_join(ndvi, first.deriv, by = "x") %>%
        mutate(Date = as_date(x)) %>% #as.Date
        mutate(change.ndvi = (ndvi - lag(ndvi, n = 1L))) %>%
        mutate(GrowingSeason = gs) %>%    
        mutate(Julian = yday(Date)) %>%
        mutate(sitenum = a) %>%
        left_join(siteinfo2, by = 'sitenum')
    } else {emptydataframe}
    
    GreenUpDate <- if (nrow(smoothed.data %>% 
                            filter(first.deriv > 0, 
                                   ndvi > lowndvi, 
                                   ndvi < (Max50 + 0.05), #for elliot  + 0.03; for lagoas 0.05
                                   Date > max(gs.filter$normalstart ), 
                                   Date < max(gs.filter$normalend - 90))) > 0) { #ethass, start must be 3 mo before end or
      (smoothed.data %>% filter(first.deriv > 0, 
                                ndvi > Max50, 
                                ndvi < (Max50 + .05), 
                                Date < max(gs.filter$Date[gs.filter$GSType == 'mainGS']),
                                Date > max(gs.filter$normalstart),
                                Date < max(gs.filter$normalend - 90)) %>% 
         .[which.min(.$Date),]) 
    } else { emptydataframe }
    if (nrow(GreenUpDate) > 0) { GreenUpDate$Type <- "green-up date" } else { GreenUpDate$Type <- character(0) }
    if (nrow(GreenUpDate) > 0) { GreenUpDate$gs <- gs } else { GreenUpDate$gs <- numeric(0) }
    if (nrow(GreenUpDate) > 0) { GreenUpDate$sitenum <- a } else { GreenUpDate$sitenum <- numeric(0) }
    
    max <- if (nrow(GreenUpDate) >0 ) {
      smoothed.data %>% filter(Date > GreenUpDate$Date, 
                               Date <= max(gs.filter$Date[gs.filter$GSType=='mainGS']) ) %>% 
        .[which.max(.$ndvi), ]
    } else {
      (filter(smoothed.data, 
              Date > (max(gs.filter$normalstart) + 60),
              Date < (max(gs.filter$normalend) - 60),
              change.ndvi > 0 )) %>%
        .[which.max(.$ndvi),]  }
    if (nrow(max) > 0) { max$Type <- "max" } else { max$Type <- character(0) }
    if (nrow(max) > 0) { max$gs <- gs } else { max$gs <- character(0) }
    if (nrow(max) > 0) { max$sitenum <- a } else { max$sitenum <- numeric(0) }
    
    SenescenseDate <- if (nrow(GreenUpDate) > 0) {
      smoothed.data %>% 
        filter(first.deriv < 0, 
               Date > max$Date,
               Date < max(gs.filter$normalend),
               ndvi < (Max50 + 0.0), #changed due to elliot 2017 (+0.05)...but jorn says keep it 0, 0 make sense
               ndvi > (Max50 - .05),
               Date < max(gs.filter$normalend - 30)) %>% #due to marc 2018...senesence can't really be in the next year...
        .[which.min(.$Date), ] #jorn made me change this form min to max??
    } else ({emptydataframe})
    if (nrow(SenescenseDate) > 0) { SenescenseDate$Type <- "senescence date" } else { SenescenseDate$Type <- character(0) }
    if (nrow(SenescenseDate) > 0) { SenescenseDate$gs <- gs } else { SenescenseDate$gs <- numeric(0) }
    if (nrow(SenescenseDate) > 0) { SenescenseDate$sitenum <- a } else { SenescenseDate$sitenum <- numeric(0) }
    
       # ggplotly(smoothed.data %>%
       #        ggplot() +
       #        geom_hline(yintercept = (smoothed.data[[1, 'Max50']] + 0.05)) +
       #        geom_point(aes(x = Date, y = ndvi))+
       #        geom_point(aes(x = Date, y = ndvi), col = 'blue', data = filter(smoothed.data,
       #                                                         first.deriv > 0,
       #                                                         ndvi > lowndvi,
       #                                                         ndvi < (smoothed.data[[1, 'Max50']] + 0.05)))+#,
       #                                                         # Date > ymd("1996-10-21"),
       #                                                         # Date < ymd("1997-10-20") - 90)) +
       #          geom_point(aes(x = Date, y = NDVI), data = gs.filter, pch = 2)+
       #          geom_point(aes(x = Date, y = ndvi), data = max, col = "green", size = 5))
       # ggplotly(smoothed.data %>%
       #    ggplot() +
       #    geom_hline(yintercept = (0.134)) +
       #    geom_point(aes(x = Date, y = ndvi))+
       #    geom_point(aes(x = Date, y = ndvi), col = 'blue', data = filter(smoothed.data,
       #                                                     first.deriv < 0,
       #                                                     Date > max$Date,
       #                                                     ndvi < (0.134 + 0.05),
       #                                                     ndvi > (0.134 - 0.05),
       #                                                     Date < ymd("2001-02-18") - 30)) +
       #      geom_point(aes(x = Date, y = NDVI), data = gs.filter, pch = 2)+
       #      geom_point(aes(x = Date, y = ndvi), data = SenescenseDate, col = "green", size = 5))

    pheno.dates <- bind_rows(GreenUpDate, max, SenescenseDate)
  
    gs_summary <- bind_rows(pheno.dates, gs_summary)
  }
  site_summary <- bind_rows(gs_summary, site_summary)
}


NutNetPhenoALL <- site_summary %>%
  dplyr::select(ndvi, Date, Julian, sitenum, Type, gs) %>%
  full_join(siteinfo2 %>% dplyr::select(-Start, -End)) 

NutNetPhenoFILTER1 <- NutNetPhenoALL %>% #if either sos or eos is missing, don't include those years in the average dates of anything
  dplyr::select(Julian, gs, site, Type) %>%
  pivot_wider(names_from = Type, values_from = Julian, id_cols = c(gs, site)) %>%
  filter(!is.na(`green-up date`), !is.na(`senescence date`)) %>%
  dplyr::select(gs, site)

NutNetPhenoFILTER2 <- NutNetPhenoFILTER1 %>% group_by(site) %>%
  summarise(N = n()) %>%
  filter(N >= 3) %>% #must have 3 yrs data to use
  left_join(NutNetPhenoFILTER1)

NutNetPheno <- NutNetPhenoALL %>% 
  right_join(NutNetPhenoFILTER2)

# NutNetPheno %>% filter(site == "pinj")
# 
# NutNetPheno %>% filter(gs >= 2018) %>% 
#   ggplot(aes(x = Date, y = ndvi, col = Type)) +
#   geom_point() + theme_cowplot()

#then need to redial dates into radians, to get change
AVG_nnp<-
  NutNetPheno %>% 
  mutate(DaysInYear = ifelse(leap_year(gs) == 'TRUE', 366, 365),
         DegreeJulian = (Julian / DaysInYear * 360)) %>% 
  group_by(site, Type) %>%
  summarise(DateRadianAVG = circmean_degrees(DegreeJulian)) %>%
  mutate(DateDateAVG = DateRadianAVG * 365 / 360,
         DDAVG = as_date(DateDateAVG),
         JulAGV = yday(DDAVG)) %>%
  dplyr::select(site, Type, JulAGV) %>% 
  spread(Type, JulAGV) 

redial<- AVG_nnp %>%
  full_join( tibble(site = rep(AVG_nnp$site, each = (2019 - 1981)), 
                    year = rep(1982:2019, (nrow(AVG_nnp)))),  
             by = 'site') %>% 
  rename(maxJul = max,
         sosJul = `green-up date`,
         eosJul = `senescence date`) %>%
  full_join(dplyr::select(siteinfo2, site, Start), by = 'site')  %>% 
  mutate(startyrGU = ifelse(Start > 182 & sosJul > 182, year - 1, (year)),
         maxyr = ifelse(Start > 182 & maxJul > Start, year - 1, ifelse(Start < 182 & maxJul < Start, year + 1,  year)),
         eosyr = ifelse(Start > 182 & eosJul > Start, year - 1, ifelse(Start < 182 & eosJul < Start, year + 1,  year)),
         SOS = paste(startyrGU, month(as_date(sosJul)), day(as_date(sosJul)), sep = '-'),
         MAX = paste(maxyr, month(as_date(maxJul)), day(as_date(maxJul)), sep = '-'),
         EOS = paste(eosyr, month(as_date(eosJul)), day(as_date(eosJul)), sep = '-')) %>% 
  filter(!is.na(year)) %>% 
  mutate(SOS=as_date(SOS), 
         MAX = as_date(MAX), 
         EOS = as_date(EOS))  %>% 
  dplyr::select(site, year, SOS, MAX, EOS) %>%
  gather(Type, TypicalDate, -site, -year) %>%
  rename(gs = year) %>%
  mutate(Type= ifelse(Type=='SOS', 'green-up date', ifelse(Type=='MAX', 'max', 'senescence date'))) %>%
  right_join(
    NutNetPheno %>% 
      dplyr::select(site, gs, Type, Date) %>% 
      spread(Type, Date)  %>% 
      mutate(GU = ifelse(is.na(`senescence date`), NA, `green-up date`)) %>% 
      mutate(GU = as_date(GU)) %>% 
      dplyr::select(-`green-up date`) %>% 
      rename("green-up date" = GU) %>% 
      mutate(MAX = ifelse(is.na(`senescence date`), NA, max)) %>% 
      mutate(MAX = as_date(MAX)) %>% 
      dplyr::select(-max) %>%
      rename("max" = MAX) %>% 
      gather(Type, Date, -site, -gs) %>%
      filter(!is.na(Date))  ) %>%  
  mutate(DIFFERENCE = Date - TypicalDate)  


# write full data set
NutNetPheno_long <- redial %>% 
  full_join(NutNetPheno) %>%
  left_join(siteinfo2) %>%
  dplyr::select(-Julian, -TroughDateRadian, -avg_ndvi_min) %>% #, -avg_ndvi_max) %>% 
  rename(DaysFromAvg = DIFFERENCE) %>%
  arrange(site, gs, Type) %>%
  dplyr::select(-Max50, -Start, -End) %>%
  mutate(Change_ndvi_max = ifelse(Type =="max", (ndvi - avg_ndvi_max), NA)) %>%
  dplyr::select(-avg_ndvi_max)

NutNetPheno_long %>% 
  filter(site == "glac", Type == "green-up date") %>%
  ggplot(aes(x = gs, y = as.numeric(DaysFromAvg))) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(x = "Growing season", y = "Change from average, \n Green-up Date", title = "Glac") +
  theme_cowplot()

NutNetPheno_long %>% filter(Type == "green-up date") %>% arrange(-DaysFromAvg)
NutNetPheno_long %>% filter(Type == "green-up date") %>% arrange(DaysFromAvg)


NutNetPheno_long %>%
  filter(Type == "green-up date") %>% 
  ggplot() +
  geom_histogram(aes(x = DaysFromAvg))

write.csv(NutNetPheno_long, "NutNetPheno_long.csv")

} else {
  print("not processing raw NDVI data")
}

NutNetPheno_long <- read_csv("./NutNetPheno_long.csv")

```


