---
title: "NutNet Phenology with NDVI"
author: "Ellen Esch"
date: "`r format(Sys.time(), '%d %B %Y')`"
# output:
#   word_document: default
#   pdf_document: default
#   html_document: default
output: 
  # github_document:
    pdf_document:
    toc: true
always_allow_html: true
urlcolor: blue
---

```{r packages, include=F, echo=F, message=F}
library("sf")
library("scales")
library("lme4")
library("car")
library("rnaturalearth")
library("cowplot")
library("raster")
library("knitr")
library("kableExtra")
library("plotly")
library("tidyverse")
library("lubridate")
```

# Overview

This document walks through the steps necessary to process and aggregate long-term phenological and productivity trends across many NutNet sites. This analysis leverages satellite-derived data, and those raw data are not collected within this script (but find directions relevant to that Google Earth Engine step here). 

While most files are stored on GitHub, you will need to set set the location to your dropbox folder which contains the raw NutNet data. 

In all cases, intermediate (processed) files *are* stored on GitHub. However, you may change the suite of statements equal to `TRUE` if you wish to process any raw data again.

```{r filelocations, }
dropboxdir <- "/Users/ellen/Dropbox/NutNet data"

process_raw_climate <- FALSE # TRUE
localdir <- "/Users/ellen/Desktop/Ellen/Guelph/Project_NutNet phenology/pheno_localdata" #set this if processing_raw_climate == TRUE

process_raw_ndvi <- FALSE #TRUE

merge_climate_ndvi <- FALSE #TRUE

create_final_df <- FALSE #if this is TRUE, you will need to connect to dropbox
```

```{r moresetup, echo=F, include = F}
sites <- read_csv("./data/NutNetGreening_2019.10.15_ee.csv", col_types = cols())

#for some of the analyses need to change the spatial form
sites_nutnet <- sites %>%
  dplyr::select(site_code, "GEE coordinates") %>%
  separate("GEE coordinates", sep = ',', into = c('long', 'lat')) %>%
  filter(!is.na(long)) %>%
  mutate(lat = as.numeric(lat), long = as.numeric(long)) %>%
  filter(site_code != "ethass.au") #this site just isn't greening up....google earth confirms it appears to be mostly rock where the lat/long are pointing to. So this needs pi confirmation to be included
sites_nutnet_sp <- sites_nutnet
coordinates(sites_nutnet_sp) <- ~long + lat

sites_nutnet_sp2<- sites_nutnet_sp %>% st_as_sf()
```


# Process weather & climate data

* Download monthly [precipitation data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.04/data/pre) onto your local machine (large files). You will have to either create an account or log in. Download 4 time periods (1981-1990, 1991-2000, 2001-2010, 2011-2019; you want the files with the 'nc' in the name)

```{r precipdata, echo=F}
### create precipitation data for the 4 distinct time periods
if (process_raw_climate == TRUE) {
  cruv <- "pre"
  timeframe <- "1981.1990."
  source("./get-CRU-TS.R")

  timeframe <- "1991.2000."
  source("./get-CRU-TS.R")

  timeframe <- "2001.2010."
  source("./get-CRU-TS.R")

  timeframe <- "2011.2019."
  source("./get-CRU-TS.R")
} else {
  print("not processing raw CLIMATE data")
}
```


* Repeat with monthly [temperature data](http://data.ceda.ac.uk/badc/cru/data/cru_ts/cru_ts_4.04/data/tmp).


```{r temperaturedata, echo = F}
if (process_raw_climate == TRUE) {
  cruv <- "tmp"
  timeframe <- "1981.1990."
  source("./get-CRU-TS.R")

  timeframe <- "1991.2000."
  source("./get-CRU-TS.R")

  timeframe <- "2001.2010."
  source("./get-CRU-TS.R")

  timeframe <- "2011.2019."
  source("./get-CRU-TS.R")
} else {
  print("not processing raw CLIMATE data")
}

```

* Write a data frame with merged monthly temperature and precipitation data

```{r climatedf, echo=F}
if (process_raw_climate == TRUE) {
  eightys <- read_csv("./data/weather/CRU-monthly-tmp1981.1990.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre1981.1990.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  ninetys <- read_csv("./data/weather/CRU-monthly-tmp1991.2000.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre1991.2000.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  thousands <- read_csv("./data/weather/CRU-monthly-tmp2001.2010.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre2001.2010.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  tens <- read_csv("./data/weather/CRU-monthly-tmp2011.2019.csv", col_types = cols()) %>%
    full_join(read_csv("./data/weather/CRU-monthly-pre2011.2019.csv", col_types = cols())) %>%
    dplyr::select(-date, -month, -year)

  monthlyweather <- bind_rows(eightys, ninetys, thousands, tens) %>%
    rename(sitename = site_code) %>%
    mutate(Month = month(plotdate))

  write.csv(
    monthlyweather,
    "./data/weather/monthlyweather.csv",
    row.names = F
  )
} else {
  print("not processing raw CLIMATE data")
}

```

* **See next point**, but in general.... Download [30 year averages from WorldClim](https://www.worldclim.org/data/worldclim21.html) onto your local machine. Download the most detailed spatial level (30 seconds) for:

- average temperature (tavg_30s)
- precipitation (preci_30s)

```{r worldclim, echo=F}
if (process_raw_climate == TRUE) {

  #####
  # temperature
  #####

  WC1 <- raster(paste0(localdir, "/wc2.1_30s_tavg_01.tif"))
  wc1 <- raster::extract(WC1, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC2 <- raster(paste0(localdir, "/wc2.1_30s_tavg_02.tif"))
  wc2 <- raster::extract(WC2, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC3 <- raster(paste0(localdir, "/wc2.1_30s_tavg_03.tif"))
  wc3 <- raster::extract(WC3, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC4 <- raster(paste0(localdir, "/wc2.1_30s_tavg_04.tif"))
  wc4 <- raster::extract(WC4, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC5 <- raster(paste0(localdir, "/wc2.1_30s_tavg_05.tif"))
  wc5 <- raster::extract(WC5, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC6 <- raster(paste0(localdir, "/wc2.1_30s_tavg_06.tif"))
  wc6 <- raster::extract(WC6, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC7 <- raster(paste0(localdir, "/wc2.1_30s_tavg_07.tif"))
  wc7 <- raster::extract(WC7, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC8 <- raster(paste0(localdir, "/wc2.1_30s_tavg_08.tif"))
  wc8 <- raster::extract(WC8, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC9 <- raster(paste0(localdir, "/wc2.1_30s_tavg_09.tif"))
  wc9 <- raster::extract(WC9, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC10 <- raster(paste0(localdir, "/wc2.1_30s_tavg_10.tif"))
  wc10 <- raster::extract(WC10, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC11 <- raster(paste0(localdir, "/wc2.1_30s_tavg_11.tif"))
  wc11 <- raster::extract(WC11, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  WC12 <- raster(paste0(localdir, "/wc2.1_30s_tavg_12.tif"))
  wc12 <- raster::extract(WC12, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)


  AvgClimate <- full_join(wc1, wc2, by = c("ID", "sitename")) %>%
    full_join(wc3, by = c("ID", "sitename")) %>%
    full_join(wc4, by = c("ID", "sitename")) %>%
    full_join(wc5, by = c("ID", "sitename")) %>%
    full_join(wc6, by = c("ID", "sitename")) %>%
    full_join(wc7, by = c("ID", "sitename")) %>%
    full_join(wc8, by = c("ID", "sitename")) %>%
    full_join(wc9, by = c("ID", "sitename")) %>%
    full_join(wc10, by = c("ID", "sitename")) %>%
    full_join(wc11, by = c("ID", "sitename")) %>%
    full_join(wc12, by = c("ID", "sitename")) %>%
    gather(Month, AvgTempLTA, -ID, -sitename) %>%
    mutate(Month = recode(Month,
      "wc2.1_30s_tavg_01" = 1,
      "wc2.1_30s_tavg_02" = 2,
      "wc2.1_30s_tavg_03" = 3,
      "wc2.1_30s_tavg_04" = 4,
      "wc2.1_30s_tavg_05" = 5,
      "wc2.1_30s_tavg_06" = 6,
      "wc2.1_30s_tavg_07" = 7,
      "wc2.1_30s_tavg_08" = 8,
      "wc2.1_30s_tavg_09" = 9,
      "wc2.1_30s_tavg_10" = 10,
      "wc2.1_30s_tavg_11" = 11,
      "wc2.1_30s_tavg_12" = 12
    ))


  ####
  # precip
  #####

  preWC1 <- raster(paste0(localdir, "/wc2.1_30s_prec_01.tif"))
  prewc1 <- raster::extract(preWC1, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC2 <- raster(paste0(localdir, "/wc2.1_30s_prec_02.tif"))
  prewc2 <- raster::extract(preWC2, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC3 <- raster(paste0(localdir, "/wc2.1_30s_prec_03.tif"))
  prewc3 <- raster::extract(preWC3, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC4 <- raster(paste0(localdir, "/wc2.1_30s_prec_04.tif"))
  prewc4 <- raster::extract(preWC4, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC5 <- raster(paste0(localdir, "/wc2.1_30s_prec_05.tif"))
  prewc5 <- raster::extract(preWC5, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC6 <- raster(paste0(localdir, "/wc2.1_30s_prec_06.tif"))
  prewc6 <- raster::extract(preWC6, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC7 <- raster(paste0(localdir, "/wc2.1_30s_prec_07.tif"))
  prewc7 <- raster::extract(preWC7, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC8 <- raster(paste0(localdir, "/wc2.1_30s_prec_08.tif"))
  prewc8 <- raster::extract(preWC8, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC9 <- raster(paste0(localdir, "/wc2.1_30s_prec_09.tif"))
  prewc9 <- raster::extract(preWC9, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC10 <- raster(paste0(localdir, "/wc2.1_30s_prec_10.tif"))
  prewc10 <- raster::extract(preWC10, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC11 <- raster(paste0(localdir, "/wc2.1_30s_prec_11.tif"))
  prewc11 <- raster::extract(preWC11, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)

  preWC12 <- raster(paste0(localdir, "/wc2.1_30s_prec_12.tif"))
  prewc12 <- raster::extract(preWC12, sites_nutnet_sp2, fun = max, df = TRUE) %>%
    mutate(sitename = sites_nutnet_sp2$site_code)


  AvgClimatePrecip <- full_join(prewc1, prewc2, by = c("ID", "sitename")) %>%
    full_join(prewc3, by = c("ID", "sitename")) %>%
    full_join(prewc4, by = c("ID", "sitename")) %>%
    full_join(prewc5, by = c("ID", "sitename")) %>%
    full_join(prewc6, by = c("ID", "sitename")) %>%
    full_join(prewc7, by = c("ID", "sitename")) %>%
    full_join(prewc8, by = c("ID", "sitename")) %>%
    full_join(prewc9, by = c("ID", "sitename")) %>%
    full_join(prewc10, by = c("ID", "sitename")) %>%
    full_join(prewc11, by = c("ID", "sitename")) %>%
    full_join(prewc12, by = c("ID", "sitename")) %>%
    gather(Month, AvgPrecipLTA, -ID, -sitename) %>%
    mutate(Month = recode(Month,
      "wc2.1_30s_prec_01" = 1,
      "wc2.1_30s_prec_02" = 2,
      "wc2.1_30s_prec_03" = 3,
      "wc2.1_30s_prec_04" = 4,
      "wc2.1_30s_prec_05" = 5,
      "wc2.1_30s_prec_06" = 6,
      "wc2.1_30s_prec_07" = 7,
      "wc2.1_30s_prec_08" = 8,
      "wc2.1_30s_prec_09" = 9,
      "wc2.1_30s_prec_10" = 10,
      "wc2.1_30s_prec_11" = 11,
      "wc2.1_30s_prec_12" = 12
    ))


  longterm_climate <- full_join(AvgClimate, AvgClimatePrecip)

  write.csv(
    longterm_climate,
    "./data/weather/longterm_avgclimate.csv",
    row.names = F
  )
} else {
  print("not processing raw CLIMATE data")
}

```

* Look at the monthly deviations from the long term average. 

This first plot illustrates a problem (fortunately with an easy solution). At Cowichan notice there are no consistent longitudinal trends in precipitation, but it is obvious that WorldClim (long term average) and CEDA (monthly) data don't always necessarily align (expected because they have different methods, resolutions, etc.). This suggests that CEDA should be used to calculate long-term averages (1981-2019, 39 years) as well as anomalies. 


```{r weatherdev, echo=F, message=F, warning=F}
worldclim <- read_csv("./data/weather/longterm_avgclimate.csv")
montlycru <- read_csv("./data/weather/monthlyweather.csv")

# CEDA precip in mm/month
# world clim is also in mm/month
ClimChange_wc <- montlycru %>%
  full_join(worldclim) %>%
  mutate(TempDifference = tmp_degrees_Celsius - AvgTempLTA) %>%
  mutate(PrecipDifference = `pre_mm/month` - AvgPrecipLTA) %>%
  separate(sitename, into = c("site", "b"), sep = "[.]") %>%
  dplyr::select(-b)

# ClimChange_wc %>% filter(site=="sval") %>%
#   ggplot(aes(x=plotdate, y=TempDifference))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   facet_wrap(~Month)+
#   labs(y = "Anomaly from 1970-2000, celsius", x = "Year", title = "Temperature Anomalies, Svalbard")+
#   theme_cowplot()+
#   geom_hline(yintercept = 0)+
#   scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

ClimChange_wc %>%
  filter(site == "cowi") %>%
  ggplot(aes(x = plotdate, y = PrecipDifference)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~Month) +
  labs(y = "Anomaly from 1970-2000, mm", x = "Year", title = "Precip Anomalies, Cowichan") +
  theme_cowplot() +
  geom_hline(yintercept = 0) +
  scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


Indeed, using CEDA data to calculate averages as well as anomalies proves to be a much more logical metric. Cowichan data seems *much* more logical now. 

```{r ceda_avg, echo=F, message=F, warning=F}
# min(montlycru$plotdate)
# max(montlycru$plotdate)

cruavg <- montlycru %>%
  group_by(sitename, Month) %>%
  summarise(
    avg_precip = mean(`pre_mm/month`, na.rm = T),
    avg_temp = mean(`tmp_degrees_Celsius`, na.rm = T),
    N = n()
  )

ClimChange <- montlycru %>%
  full_join(cruavg) %>%
  mutate(TempDifference = tmp_degrees_Celsius - avg_temp) %>%
  mutate(PrecipDifference = `pre_mm/month` - avg_precip) %>%
  separate(sitename, into = c("site", "b"), sep = "[.]") %>%
  dplyr::select(-b)

# ClimChange %>% filter(site=="sval") %>%
#   ggplot(aes(x=plotdate, y=TempDifference))+
#   geom_point()+
#   geom_smooth(method = "lm")+
#   facet_wrap(~Month)+
#   labs(y = "Anomaly from 39 yrs CEDA data, celsius", x = "Year", title = "Temperature Anomalies, Svalbard")+
#   theme_cowplot()+
#   geom_hline(yintercept = 0)+
#   scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))

ClimChange %>%
  filter(site == "cowi") %>%
  ggplot(aes(x = plotdate, y = PrecipDifference)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~Month) +
  labs(y = "Anomaly from 39 yrs CEDA data, mm", x = "Year", title = "Precip Anomalies, Cowichan") +
  theme_cowplot() +
  geom_hline(yintercept = 0) +
  scale_x_date(limits = c(ymd("1984-01-01"), ymd("2020-01-01")), date_breaks = "10 years", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Process site-specific info (from dropbox)

## Nitrogen deposition data

**NOTE** Previously N deposition was extracted from "F. Dentener, Global maps of atmospheric nitrogen deposition, 1860, 1993, and 2050. ORNL DAAC  (2006)". However, two things have happened: a) r spatial has a bug (it's known, and I guess they're working on it so this could be fixed), b) a 07 Dec 2020 update added n deposition data to the NutNet Dropbox. Thus, we are proceeding with the file in the dropbox....

Additionally, note that the new data in the NutNet dropbox allows us to better look at *change* in N deposition over time. Since we are evaluating phenological/NDVI change over time as well, this seems like a logical addition. 

```{r ndep, echo=F, include=F}
if (create_final_df == TRUE) {

# ndep <- raster(paste0(localdir, '/N-deposition1993.tif'),
#                crs = '+init=EPSG:4326')
# crs(ndep) <- CRS('+init=EPSG:4326') #https://epsg.io/4326
#
# ndep_merge <- raster::extract(ndep, sites_nutnet_sp2, fun = max, df = TRUE) %>%
#   mutate(sitename = sites_nutnet_sp2$site_code) %>%
#   separate(sitename, into = c('site', 'b'), sep = "[.]") %>%
#   dplyr::select(-b, -ID)

ndep <- read_csv(paste0(dropboxdir, "/site-n-deposition-07-December-2020.csv")) %>%
  dplyr::select(site_code, year, total_deposition) %>%
  filter(site_code %in% sites_nutnet_sp2$site_code) %>%
  filter(year == 1984 | year == 2016) %>%
  pivot_wider(names_from = year, values_from = total_deposition) %>%
  mutate(ndep_change_2016_1984 = `2016` - `1984`) %>%
  rename(
    ndep_2016 = `2016`,
    ndep_1984 = `1984`
  ) %>%
  separate(site_code, into = c("site", "b"), sep = "[.]") %>%
  dplyr::select(-b)

} else {
  print("not creating final data sets")
}

```

## Biomass, richness, etc.

Extract important site-specific information the NutNet master data files (species richness, elevation, management, soil nutrients, etc). 

Additionally, process observational year data on species evenness.

To connect on-the-ground biomass measurements with NDVI, also extract the average biomass (ANPP) for all control plots for all years available. 

```{r siteinfo, echo=F, message=F, warning=F}
if (create_final_df == TRUE) {

  dropbox <- read_csv(paste0(dropboxdir, "/comb-by-plot-clim-soil-diversity-06-May-2021.csv"),
  col_types = cols(),
  na = c("NULL", "NA")
)

siteclim <- dropbox %>%
  filter(trt == "Control") %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  group_by(site) %>%
  summarise(
    Managed = mean(managed), # sdManaged = sd(managed),
    Burned = mean(burned), # sdBurned=sd(burned),
    Elevation = mean(elevation), # sdElevation=sd(elevation),
    RainMAP = mean(MAP_v2, na.rm = T), # sdRainPET=sd(MAP_v2, na.rm=T),
    TempMAT = mean(MAT_v2), # sdTempMAT = sd(MAT_v2),
    Richness = mean(site_richness), # , sdRichness = sd(site_richness))
    NativeRichness = mean(site_native_richness),
    IntRichness = mean(site_introduced_richness),
    FractionExotic = (IntRichness / (NativeRichness + IntRichness)),
    SoilC = mean(pct_C, na.rm = T),
    SoilN = mean(pct_N, na.rm = T),
    SoilP = mean(ppm_P, na.rm = T),
    SoilK = mean(ppm_K, na.rm = T)
  )


obsyr_even <- dropbox %>%
  filter((year == (first_nutrient_year - 1)) | experiment_type == "Observational") %>%
  group_by(site_code) %>%
  summarise(
    ObsYrMeanEven = mean(evenness, na.rm = T),
    ObsYrMeanInvSimp = mean(inverse_simpson, na.rm = T)
  ) %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  dplyr::select(-country)

# obsyr_even %>%
#   ggplot(aes(x = ObsYrMeanEven, y = ObsYrMeanInvSimp)) +
#   geom_point()

region <- dropbox %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  dplyr::select(site, continent, habitat, latitude, longitude) %>%
  unique()

SiteInfo <- ndep %>%
  left_join(region) %>%
  inner_join(siteclim) %>%
  left_join(obsyr_even)

# kable(SiteInfo %>%
#   select(site:SoilK, N.deposition1993)) %>%
#   kable_styling("striped", full_width = F)

###
# Site biomass
###
sitenpp1 <- dropbox %>%
  separate(site_code, into = c("site", "country"), sep = "[.]") %>%
  filter(trt == "Control") %>%
  group_by(site, year) %>%
  summarise(
    ANPP_control = mean(live_mass, na.rm = T),
    sdANPP_control = sd(live_mass, na.rm = T)
  ) %>%
  rename(gs = year) %>%
  filter(!is.na(sdANPP_control))

sitenpp <- sitenpp1 %>%
  group_by(site) %>%
  summarise(YrsBiomass = n()) %>%
  left_join(sitenpp1)

#plot
sitenpp %>%
  gather(Msmr, Value, -site, -gs) %>%
  mutate(Msmr = dplyr::recode(Msmr, "ANPP" = "ANPP g/m2", "sdANPP" = "St. Dev ANPP")) %>%
  ggplot(aes(Value)) +
  geom_histogram() +
  facet_wrap(~ Msmr, scales = "free") +
  theme_cowplot() +
  labs(x = "", title = "Biomass Production")

} else {
  print("not creating final data sets")
}
```


# Process the Landsat NDVI data

For this analyses, I have focused on Landsat Satellites 4, 5, 7, and 8. All have a resolution of 30 m. Prior Landsats were more coarse (LS 1 & 2 = 80 m resolution, LS 3 = 40 m resolution). In [google earth engine](https://code.earthengine.google.com/), I have processed the top of atmosphere values, filtered data by band quality, and included both [tier 1 and tier 2 data](https://www.usgs.gov/core-science-systems/nli/landsat/landsat-collection-1?qt-science_support_page_related_con=1#qt-science_support_page_related_con). From tier 1 data, readings which indicated clear conditions or snow cover at time of imagery were kept. From tier 2 data, NDVI was only used when the band quality indicated snow cover. Tier 2 data were used as a supplement, because the snow covered satellite images were often not included tier 1 data. It is important to have sufficient Landsat data coverage during snow-covered periods (if applicable!) so that accurate "troughs" can be identified.

As an FYI, Landsat coverage improved greatly with satellites 7 and 8. As such, some sites have very sparse coverage prior to 1999. However, the  scan-line corrector on Landsat 7 failed on June 2003, meaning that some sites again had only spotty coverage until Landsat 8's launch in 2003. 

At each NutNet site, I picked an adjacent area to the experimental plot locations. There, I took a 30 m buffer radius (a couple sites are different - see GEE code) around that point to be our "site." Sometimes this meant that multiple paths/rows of Landsat were included, which is fine. Sometimes it might mean that the NutNet site is not accurately reflected in our data (if for example a grassland site occurs amidst a bunch of trees...those trees might get included by using this method). Essentially, if the NDVI data looks really bad for a specific site, the accuracy and relevance of the location I chose is probably the first thing to check - how accurately does our designated "region of interest" reflect the landscape?

## Collect raw data from Google Earth Engine
In goggle earth engine, the following was run. Apologies that it's long, messy, and that I'm not very good at creating loops within loops in JavaScript! There are some button options that pop up in GEE prompting the various extracted data to be saved. And that each site needs to get individually commented out and run. It's not pretty - but it does work well!!!

**NOTE** the lat/longs were "found" by using the `/Dropbox/NutNet data/sites-02-August-2019.csv`. NOT all of the sites have data in the `comb-by-plot-clim-soil-diversity-07-December-2020.csv` file (and so will subsequently be excluded). 

If lat/long of certain sites need to be updated, do so in the excel file below. Make sure to note which sites have been updated, or else Google Earth Engine will make your life (slightly) miserable. If you know which sites you need to run again, it's much better.

```{r echo=T}
process_site_lat_long <- FALSE
```


```{r makeshaefile, echo = F}
process_site_latlong <- FALSE

if (process_site_latlong == TRUE)
{
library(tidyverse)
library(sf)
library(leaflet)
sites <- read_csv('./data/site_check_2020.9.23.csv') %>%
  filter(!is.na(long), !is.na(lat)) %>%
  select(-X1) %>%
  st_as_sf(coords = c("long", "lat"), crs = 4326) %>% 
  st_transform(3857) %>% #http://epsg.io/3857 uom = meter
  st_buffer(dist = 30) %>%
  st_transform(4326) #for leaflet

leaflet() %>%
  addTiles() %>%
  addPolygons(data = sites)

st_write(sites, "./data/site_shapefiles/sites.shp", append = FALSE)
} else {print("Not updating site lat/longs")}

```

```GEE code not shown here due to length, but feel free to look at it in the markdown file```




```{r geecode, eval=F, include=F, echo=F}

# ## GEE code
# //create ROI
# /*
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(18.724477, 68.366069), {label:'abisko_se'}).buffer(30)]); //UPDATED
# var NAME = 'abisko_se';
# 
# var NAME = 'ahth_is'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.014311, 48.465041), {label:'amcamp_us'}).buffer(30)]);//UPDATED 
# var NAME = 'amcamp_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-19.672060, 65.133350), {label:'amlr_is'}).buffer(30)]); 
# var NAME = 'amlr_is'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-78.162, -0.467083333), {label:'anti_ec'}).buffer(30)]); 
# var NAME = 'anti_ec'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-81.218472, 27.170194), {label:'arch_us'}).buffer(30)]);//UPDATED 
# var NAME = 'arch_us' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(101.865677, 33.670918), {label:'azitwo_cn'}).buffer(30)]); //UPDATED
# var NAME = 'azitwo_cn';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(11.880336, 51.391717), {label:'badlau_de'}).buffer(30)]); //UPDATED
# var NAME = 'badlau_de' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-71.152318, -41.006807), {label:'bari_ar'}).buffer(30)]);
# var NAME = 'bari_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-99.652173, 42.245453), {label:'barta_us'}).buffer(30)]);
# var NAME = 'barta_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(11.584053, 49.921323), {label:'bayr_de'}).buffer(30)]);
# var NAME = 'bayr_de';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-105.233063, 39.972852), {label:'bldr_us'}).buffer(30)]);
# var NAME = 'bldr_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-95.087419, 39.596985), {label:'bnbt_us'}).buffer(30)]);
# var NAME = 'bnbt_us' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.966830, 44.278458), {label:'bnch_us'}).buffer(30)]);
# var NAME = 'bnch_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(147.253979, -36.873971), {label:'bogong_au'}).buffer(30)]);
# var NAME = 'bogong_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.958134, 44.277013), {label:'bttr_us'}).buffer(30)]);
# var NAME = 'bttr_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(151.606325, -26.892224), {label:'bunya_au'}).buffer(30)]);
# var NAME = 'bunya_au' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(151.139265, -27.735334), {label:'burrawan_au'}).buffer(30)]);
# var NAME = 'burrawan_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-8.992624, 53.072023), {label:'burren_ie'}).buffer(30)]);
# var NAME = 'burren_ie' 
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(82.708800, 42.884700), {label:'bynb_cn'}).buffer(30)]);
# var NAME = 'bynb_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-93.384990, 41.784526), {label:'cbgb_us'}).buffer(30)]);
# var NAME = 'cbgb_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-93.211968, 45.427225), {label:'cdcr_us'}).buffer(30)]);
# var NAME = 'cdcr_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-101.643424, 41.206284), {label:'cdpt_us'}).buffer(30)]);
# var NAME = 'cdpt_us';
# 
# var NAME = 'cereep_fr'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-58.264246, -36.275950), {label:'chilcas_ar'}).buffer(30)]);
# var NAME = 'chilcas_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-8.791102, 38.829049), {label:'comp_pt'}).buffer(30)]);
# var NAME = 'comp_pt';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.635063, 48.808390), {label:'cowi_ca'}).buffer(30)]); //UPDATED
# var NAME = 'cowi_ca';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(144.791878, -37.806923), {label:'derr_au'}).buffer(30)]);
# var NAME = 'derr_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.855257, 40.695596), {label:'doane_us'}).buffer(30)]);
# var NAME = 'doane_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(29.267586, -28.961042), {label:'drake_za'}).buffer(30)]);
# var NAME = 'drake_za'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.05025, 36.984861), {label:'elkh_us'}).buffer(30)]);
# var NAME = 'elkh_us';
# */
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-117.090227, 32.892409),{label:'elliot_us'}).buffer(30)]); 
# var NAME = 'elliot_us';
# /*
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(26.353695, 58.257967), {label:'elva_ee'}).buffer(30)]);
# var NAME = 'elva_ee';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(138.474286, -23.756611), {label:'ethamc_au'}).buffer(30)]);
# var NAME = 'ethamc_au'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(138.398970, -23.638610), {label:'ethass_au'}).buffer(30)]);
# var NAME = 'ethass_au'
# 
# var NAME = 'fnly_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(7.827216, 48.020054), {label:'free_de'}).buffer(30)]);
# var NAME = 'free_de'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(8.540363, 47.114728), {label:'frue_ch'}).buffer(30)]);
# var NAME = 'frue_ch';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(10.718500, 43.797100), {label:'gall_it'}).buffer(30)]);
# var NAME = 'gall_it'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(90.233300, 31.383330), {label:'gcnban_cn'}).buffer(30)]);
# var NAME = 'gcnban_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(91.066770, 30.496210), {label:'gcndan_cn'}).buffer(30)]);
# var NAME = 'gcndan_cn'
# 
# var NAME = 'gcnhai_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(102.577863, 32.828816), {label:'gcnhon_cn'}).buffer(30)]);
# var NAME = 'gcnhon_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(119.951580, 49.333000), {label:'gcnhul_cn'}).buffer(30)]);
# var NAME = 'gcnhul_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(92.009700, 31.643700), {label:'gcnnaq_cn'}).buffer(30)]);
# var NAME = 'gcnnaq_cn'
# 
# var NAME = 'gcnsui_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(106.966670, 41.416670), {label:'gcnura_cn'}).buffer(30)]);
# var NAME = 'gcnura_cn'
# 
# var NAME = 'gcnxil_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(106.929611, 37.309860), {label:'gcnyan_cn'}).buffer(30)]);
# var NAME = 'gcnyan_cn'
# 
# var NAME = 'gcnyou_cn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(30.292178, -29.284046), {label:'gilb_za'}).buffer(30)]);
# var NAME = 'gilb_za'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.035557, 46.869083), {label:'glac_us'}).buffer(30)]); 
# var NAME = 'glac_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.1396933, 41.339757), {label:'glcr_us'}).buffer(30)]); 
# var NAME = 'glcr_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-86.702686, 36.872137), {label:'hall_us'}).buffer(30)]); 
# var NAME = 'hall_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-119.497670, 42.723745), {label:'hart_us'}).buffer(30)]);
# var NAME = 'hart_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.551652, 36.386708), {label:'hast_us'}).buffer(30)]); //UPDATED
# var NAME = 'hast_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-0.645026, 51.413098), {label:'hero_uk'}).buffer(30)]);
# var NAME = 'hero_uk';
# 
# var NAME = 'hnvr_us'
# 
# var NAME = 'hogone_us'
# 
# var NAME = 'hogtwo_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-123.061648, 39.011633), {label:'hopl_us'}).buffer(30)]);
# var NAME = 'hopl_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(105.967778, 47.666528), {label:'hustai_mn'}).buffer(30)]);
# var NAME = 'hustai_mn'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.241545, 37.405222), {label:'jasp_us'}).buffer(30)]);
# var NAME = "jasp_us"
# 
# var NAME = 'jena_de';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-106.787094, 32.530054), {label:'jorn_us'}).buffer(30)]);
# var NAME = 'jorn_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(18.323098, 68.392169), {label:'kark_se'}).buffer(30)]);
# var NAME = 'kark_se';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-85.391144, 42.409125), {label:'kbs_us'}).buffer(30)]);
# var NAME = 'kbs_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(78.008754, 32.319508), {label:'kibber_in'}).buffer(30)]);
# var NAME = "kibber_in"
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(130.951768, -16.110229), {label:'kidman_au'}).buffer(30)]);
# var NAME = 'kidman_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(20.87352408,  69.05678545), {label:'kilp_fi'}).buffer(30)]);
# var NAME = 'kilp_fi';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(143.851472, -36.324810), {label:'kiny_au'}).buffer(30)]);
# var NAME = 'kiny_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(23.794941, 58.710601), {label:'kirik_ee'}).buffer(30)]);
# var NAME = "kirik_ee";
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-79.537881, 44.024616), {label:'koffler_ca'}).buffer(30)]);
# var NAME = 'koffler_ca';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.582323, 39.070328), {label:'konz_us'}).buffer(30)]);
# var NAME = 'konz_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-51.798613, -20.983784), {label:'lagoas_br'}).buffer(30)]);
# var NAME = 'lagoas_br';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-95.182989, 43.384792), {label:'lake_us'}).buffer(30)]);
# var NAME = 'lake_us';
# 
# var NAME = 'lakta_se';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-2.627710, 53.985674), {label:'lancaster_uk'}).buffer(30)]);
# var NAME = 'lancaster_uk';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-124.048400, 46.614900), {label:'lead_us'}).buffer(30)]);
# var NAME = 'lead_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.128676, 44.205263), {label:'look_us'}).buffer(30)]);
# var NAME = 'look_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-101.897343, 33.600589), {label:'lubb_us'}).buffer(30)]);
# var NAME = 'lubb_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-57.424151, -37.714602), {label:'marc_ar'}).buffer(30)]);
# var NAME = 'marc_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-77.042078, 39.527459), {label:'mcdan_us'}).buffer(30)]);
# var NAME = 'mcdan_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.407466, 38.864058), {label:'mcla_us'}).buffer(30)]);
# var NAME = 'mcla_us';
# 
# var NAME = 'meto_us'''
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(143.321660, -22.483803), {label:'mitch_au'}).buffer(30)]);
# var NAME = 'mitch_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-114.000646,  46.664443), {label:'msla_us'}).buffer(30)]);
# var NAME = 'msla_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-96.453980, 46.870938), {label:'msum_us'}).buffer(30)]);
# var NAME = 'msum_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(117.611742, -31.781354), {label:'mtca_au'}).buffer(30)]);
# var NAME = 'mtca_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(137.661096, 35.231685), {label:'neba_jp'}).buffer(30)]);
# var NAME = 'neba_jp'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(146.006711, -36.895778), {label:'nilla_au'}).buffer(30)]);
# var NAME = 'nilla_au';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(5.749875, 52.059696), {label:'nioo_nl'}).buffer(30)]);
# var NAME = 'nioo_nl';
# 
# var NAME = 'niwo_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(106.886887, 47.769495), {label:'ovor_mn'}).buffer(30)]);
# var NAME = 'ovor_mn';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-64.434831, -36.586107), {label:'pampa_ar'}).buffer(30)]);
# var NAME = 'pampa_ar';
# 
# var NAME = 'pape_de';
# 
# var NAME = 'pich_ec'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(116.972561, -32.496899), {label:'ping_au'}).buffer(30)]);
# var NAME = 'ping_au'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(152.923522, -27.530424), {label:'pinj_au'}).buffer(30)]);
# var NAME = 'pinj_au';
# 
# var NAME = 'podo_ec'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-70.407124, -51.914837), {label:'potrok_ar'}).buffer(30)]);
# var NAME = 'potrok_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-90.169865, 30.517510), {label:'ramsay_us'}).buffer(30)]);
# var NAME = 'ramsay_us';
# 
# var NAME = 'rook_uk';
# 
# var NAME = "saana_fi';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-120.239269, 39.430489), {label:'sage_us'}).buffer(30)]);
# var NAME = 'sage_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-99.166935, 39.085843), {label:'saline_us'}).buffer(30)]);
# var NAME = 'saline_us';
# 
# var NAME = 'sava_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-120.021470, 34.696911), {label:'sedg_us'}).buffer(30)]);
# var NAME = 'sedg_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(34.857533, -2.429138), {label:'sereng_tz'}).buffer(30)]);
# var NAME = 'sereng_tz';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-106.691175, 34.360198), {label:'sevi_us'}).buffer(30)]);
# var NAME = 'sevi_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-104.767174, 40.816332), {label:'sgs_us'}).buffer(30)]);
# var NAME = 'sgs_us'
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-112.196699, 44.244893), {label:'shps_us'}).buffer(30)]);
# var NAME = 'shps_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.283696, 39.235510), {label:'sier_us'}).buffer(30)]);
# var NAME = 'sier_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(74.742883, 14.419046), {label:'sirsi_in'}).buffer(30)]);
# var NAME = 'sirsi_in';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-122.624726, 48.206135), {label:'smith_us'}).buffer(30)]);
# var NAME = 'smith_us';
# 
# var NAME = 'spin_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-64.170575, -42.654068), {label:'spv_ar'}).buffer(30)]);
# var NAME = 'spv_ar';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(30.716644, -29.810615), {label:'summ_za'}).buffer(30)]);
# var NAME = 'summ_za';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point( 16.44713167, 78.69017773 ), {label:'sval_no'}).buffer(30)]);
# var NAME = 'sval_no';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-97.349443, 31.044188), {label:'temple_us'}).buffer(30)]);
# var NAME = 'temple_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-17.090214, 65.901294), {label:'thth_is'}).buffer(30)]);
# var NAME = 'thth_is';
# 
# var NAME = 'tmlr_is';
# 
# var NAME = 'trel_us';
# 
# var NAME = 'tyso_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-121.740563, 36.867430), {label:'ucsc_us'}).buffer(30)]);
# var NAME = 'ucsc_us';
# 
# var NAME = "ufrec.us";
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(30.412772, -29.670833), {label:'ukul_za'}).buffer(30)]);
# var NAME = 'ukul_za';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-79.019186, 36.007745), {label:'unc_us'}).buffer(30)]);
# var NAME = 'unc_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-81.317171, 43.192809), {label:'uwo_ca'}).buffer(30)]);
# var NAME = 'uwo_ca';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(10.371966, 46.632543), {label:'valm_ch'}).buffer(30)]);
# var NAME = 'valm_ch';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-101.609734, 21.783367), {label:'vaqu_mx'}).buffer(30)]);
# var NAME = 'vaqu_mx';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(29.089939, 70.308117), {label:'varexp_no'}).buffer(30)]);
# var NAME = 'varexp_no';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(29.539800, 70.327970), {label:'vargrass_no'}).buffer(30)]);
# var NAME = 'vargrass_no';
# 
# var NAME = 'varheath_no';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(18.274396, 68.416979), {label:'vass_se'}).buffer(30)]);
# var NAME = 'vass_se';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(-111.579722, 33.583333), {label:'wapatki_us'}).buffer(30)]);
# var NAME = 'wapatki_us';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(116.673782, 43.551476), {label:'xilin_cn'}).buffer(30)]);
# var NAME = 'xilin_cn';
# 
# var site = ee.FeatureCollection([ee.Feature(ee.Geometry.Point(150.738804, -33.614034), {label:'yarra_au'}).buffer(30)]);
# var NAME = 'yarra_au';
# 
# */
# 
# 
# //LS 8
# var addNDVI_8 = function(image){
#   return image
#   .addBands(image.normalizedDifference(['B5','B4'])
#   .rename('NDVI'))
#   .float();
# };
# 
# //LS 754
# var addNDVI_754 = function(image){
#   return image
#   .addBands(image.normalizedDifference(['B4','B3'])
#   .rename('NDVI'))
#   .float();
# };
# 
# 
# //abisko
# var site_LS8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_TOA')
#   .map(addNDVI_8)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
# 
# var site_LS7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
# 
# var site_LS5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
#   
# var site_LS4 = ee.ImageCollection('LANDSAT/LT04/C01/T1_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
#   
# var site_LS4_t2 = ee.ImageCollection('LANDSAT/LT04/C01/T2_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);  
# 
# var site_LS5_t2 = ee.ImageCollection('LANDSAT/LT05/C01/T2_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
#   
# var site_LS7_t2 = ee.ImageCollection('LANDSAT/LE07/C01/T2_TOA')
#   .map(addNDVI_754)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);  
# 
# var site_LS8_t2 = ee.ImageCollection('LANDSAT/LC08/C01/T2_TOA')
#   .map(addNDVI_8)
#   .select('NDVI','BQA')
#   .getRegion(site, 30);
# 
# 
# var site_8 = ee.FeatureCollection(site_LS8.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# var site_7 = ee.FeatureCollection(site_LS7.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# var site_5 = ee.FeatureCollection(site_LS5.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# var site_4 = ee.FeatureCollection(site_LS4.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# //
# var site_4_t2 = ee.FeatureCollection(site_LS4_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# var site_5_t2 = ee.FeatureCollection(site_LS5_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# var site_7_t2 = ee.FeatureCollection(site_LS7_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# var site_8_t2 = ee.FeatureCollection(site_LS8_t2.map(function(list) {
#     var list = ee.List(list)
#     var dict = {
#         col1: list.get(0),
#         col2: list.get(1),
#         col3: list.get(2),
#         col4: list.get(4),
#         col5: list.get(5),
#     };
#     return ee.Feature(null, dict);
# }));
# 
# Export.table.toDrive({
#   collection: site_8,
#   description: NAME.concat('_ls8_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_7,
#   description: NAME.concat('_ls7_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_5,
#   description: NAME.concat('_ls5_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_4,
#   description: NAME.concat('_ls4_t1'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_8_t2,
#   description: NAME.concat('_ls8_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_7_t2,
#   description: NAME.concat('_ls7_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_5_t2,
#   description: NAME.concat('_ls5_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });
# 
# Export.table.toDrive({
#   collection: site_4_t2,
#   description: NAME.concat('_ls4_t2'), //job name
#   folder: 'annoying manual',
#   fileFormat: 'CSV'
# });

```

## Identify phenological dates

There are seemingly infinite ways to process NDVI to pick out phenological dates. Quickly, we fitted cubic splines to each growing season at each site. We found that a single spline fit to each site (encompassing all growing years) was too sensitive to years with large spans of time without NDVI values. Then, we picked out the maximum NDVI from the fitted spline in each growing season. And the first date where NDVI >= 50 of the average range of NDVI (plus the avg. min) (as a green-up threshold). Originally I had used a fancy combination of derivatives (Buitenwerf et al), but they turned out to be very sensitive to missing values (problematic in snow-covered regions). 


Behind the scenes, some fancy (and not-so-fancy) data processing happened: 
1) dates were treated as radians to accommodate southern hemisphere and Mediterranean sites with potential phenological dates spanning across years within the same growing season (for instance, a site with green-up in Dec 2018, NDVI max in March 2019, and brown down in May 2019; this is all classified as occurring during the 2019 growing season.)
2) At certain sites there are periods of the year where NDVI data are not available, either because of persistent seasonal clouds, algorithm confusion between clouds and snow (aka water reflectance), or poor radiometric calibration on the Landsat satellites. Conversely, there may often be time periods in which the 10-day average Landsat coverage of a site is way more frequent (for example, periods when both Landsat 7 and 8 orbit, etc).  
3) NDVI values which were < 0 were reassigned a value of 0. This helped the fitted splines behave much better. 
4) Regarding weighting points within the splines, each growing season's maximum NDVI value was weighted at 1, and all other values at 0.5 in order to most accurately capture the peak. 

Also, some sites are strange - need to contact PIs eventually.
* lake_us, lakta_se, marc_ar, mitch_au, tyso_us, valm_ch, xilin_cn, bldr_us all needed larger radius buffers

Going through the steps in more detail:

* Create functions to convert linear (Julian) calendar days into radian (circular) data. (because southern hemisphere/Mediterranean sites which green up on, say Julian day 365 on average...but if one year greens up on Julian day 1, this is a 1 day difference NOT a -365 day difference)

```{r radianfxn, message=F, warning=F, echo=F}
circmean_degrees <- function(x) {
  x_radians <- x * pi / 180
  sinr <- sum(sin(x_radians))
  cosr <- sum(cos(x_radians))
  circmean_radians <- atan2(sinr, cosr)
  circmean_degree <- circmean_radians / pi * 180
  circmean_degree
}

circ_degrees <- function(x) {
  x_radians <- x * pi / 180
  sinr <- (sin(x_radians))
  cosr <- (cos(x_radians))
  circmean_radians <- atan2(sinr, cosr)
  circmean_degree <- circmean_radians / pi * 180
  circmean_degree
}
```

* Filter the Landsat data to include only pixels identified as being "clear" or "snow" covered. **NOTE** This should probably be double checked, I started off very conservatively when deciding to include tier 2 data. Documentation could have been much more through, so it would be good to see if there are some additional data which should be included/excluded. Also we set the minimum NDVI to 0 to assist in the spline fitting. 

```{r readinphenodata, eval = T, echo = F, message = F, warning = F}

if (process_raw_ndvi == TRUE) {
  pheno_path <- paste0(localdir, "/landsat") # path to the data
  pheno_files <- dir(pheno_path, pattern = "*.csv") # get file names
  pheno_files_all <- tibble(filename = pheno_files) %>%
    mutate(file_contents = map(
      filename,
      ~ read_csv(file.path(pheno_path, .),
        skip = 2, col_type = cols(filename = col_character(),
                                  X1 = col_character(),
                                  X2 = col_character(),
                                  X3 = col_double(),
                                  X4 = col_double(),
                                  X5 = col_double(),
                                  X6 = col_double(),
                                  X7 = col_logical()), 
        col_names = FALSE
      )
    )) %>%
    unnest() %>%
    rename(row = X1, id = X2, longitude = X3, latitude = X4, NDVI = X5, BQA = X6, geo = X7) %>%
    filter(!is.na(NDVI), !is.na(BQA)) %>%
    separate(filename, sep = "_", into = c("site", "country", "ls", "tier")) %>%
    separate(id, sep = "_", into = c("landsat", "path", "date")) %>%
    mutate(tier = case_when(
      tier == "t1.csv" ~ 1,
      tier == "t2.csv" ~ 2
    )) %>%
    dplyr::select(-row, -geo) %>%
    mutate(Date = ymd(date))

  # landsat_codes2 <- read_csv(paste0(localdir, '/landsat_interpretation.csv'), col_types = cols())
  landsat_codes <- read_csv(paste0(localdir, "/landsat_band_interpretation.csv"), col_types = cols())

  pheno_data <- pheno_files_all %>%
    full_join(landsat_codes) %>%
    filter(action == "keep") %>% # remove=='keep') %>%
    filter(!is.na(NDVI), !is.na(BQA)) %>%
    mutate(NDVI = case_when(ls == "ls8" ~ (NDVI - 0.0306) / 0.9824,
                           TRUE ~ NDVI)) %>% #roy et al 2016; https://www.sciencedirect.com/science/article/pii/S0034425715302455
 
    group_by(site, Date) %>%
    summarise(NDVI = mean(NDVI)) %>%
    mutate(NDVI = if_else(NDVI < 0, 0, NDVI)) %>%
    arrange(site, Date) %>%
    filter(site != "ethass") #never greening up, middle of rock 
} else {
  print("not processing raw NDVI data")
}
```

* Identify the average trough (lowest NDVI) date. From that, a "growing season" or "phenological year" is subsequently identified as the 365 days following the "trough date" plus an 90 day buffer on either end. So a total of 90 + 365 + 90 days. I know this is not totally right since leap years are confusing. Within these phenological years we create a long data set of all NDVI readings (readings must be duplicated if they fall in the "shoulder season"). 

```{r getShoulderSeasonsTroughs, eval = T, echo = F, message = F, warning = F}
if (process_raw_ndvi == TRUE) {

  END_YR <- 2021
  # first need to fit a single cubic spline to all data, and then get the average date which has the minimum NDVI
  min_ndvi_name <- levels(as.factor(pheno_data$site)) %>%
    tibble() %>%
    rename(site = ".") %>%
    mutate(sitenum = seq(1:nrow(.)))

  ndviemptydataframe <- tibble(
    x = numeric(),
    ndvi = numeric(),
    sitenum = numeric(0)
  )

  min_ndvi <- ndviemptydataframe
  for (i in c(1:nrow(min_ndvi_name))) {
    b <- i #30 = elliott, 33 = ethass
    plot.filter <- (min_ndvi_name %>% full_join(pheno_data, by = "site")) %>% filter(sitenum == b)
    spline <- smooth.spline(x = plot.filter$Date, y = plot.filter$NDVI)
    ndvi <- as_tibble(predict(spline, as.numeric(ymd(min(plot.filter$Date))):as.numeric(ymd(max(plot.filter$Date))), deriv = 0)) %>%
      rename(ndvi = y) %>%
      mutate(
        Date = as_date(x),
        sitenum = b
      )
    min_ndvi <- bind_rows(min_ndvi, ndvi)
    
    # ggplotly(min_ndvi %>% 
    #   ggplot(aes(x = Date, y = ndvi)) +
    #   geom_point())
    
  }

  min_ndvifitted <- full_join(min_ndvi, min_ndvi_name)

  siteinfo <- min_ndvifitted %>%
    mutate(Year = year(Date)) %>%
    group_by(site, Year) %>%
    summarise(ndvi_min = min(ndvi), ndvi_max = max(ndvi)) %>%
    rename(ndvi = ndvi_min) %>%
    mutate(DaysInYear = ifelse(leap_year(Year) == "TRUE", 366, 365)) %>%
    left_join((min_ndvifitted %>%
      mutate(Year = year(Date)))) %>%
    mutate(Julian = yday(Date)) %>%
    mutate(DegreeJulian = Julian / DaysInYear * 360) %>%
    group_by(site) %>%
    summarise(
      TroughDateRadian = circmean_degrees(DegreeJulian),
      avg_ndvi_max = mean(ndvi_max)
    ) %>%
    mutate(
      TroughDate = TroughDateRadian * 365 / 360,
      MinDate = (as_date(TroughDate)),
      Start = yday(MinDate),
      End = yday(MinDate) - 1
    ) %>%
    dplyr::select(-TroughDate, -MinDate, -avg_ndvi_max) %>%
    full_join(min_ndvi_name)

  pheno_data_all_1 <- full_join(pheno_data, siteinfo)

  # assign growing season
  GSmain <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (END_YR - 1981)), year = rep(1982:END_YR, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 364,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalstart, normalend, year) %>%
    mutate(ID = paste(site, year, sep = "_")) %>%
    dplyr::select(-site, -year) %>%
    gather(var, Date, -ID) %>%
    group_by(ID) %>%
    distinct() %>%
    complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
    dplyr::select(-var) %>%
    separate(ID, into = c("site", "GrowingSeason"), sep = "_") %>%
    mutate(
      GSType = "mainGS",
      GrowingSeason = as.numeric(GrowingSeason),
      weights = 999
    )

  # add 90 days pre/post growing season
  GSearly <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (END_YR - 1981)), year = rep(1982:END_YR, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 364,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalstart, earlystart, year) %>%
    mutate(ID = paste(site, year, sep = "_")) %>%
    dplyr::select(-site, -year) %>%
    gather(var, Date, -ID) %>%
    group_by(ID) %>%
    distinct() %>%
    complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
    dplyr::select(-var) %>%
    separate(ID, into = c("site", "GrowingSeason"), sep = "_") %>%
    mutate(
      GSType = "earlyGS",
      GrowingSeason = as.numeric(GrowingSeason),
      weights = 0.5
    ) %>%
    arrange(site, GrowingSeason, Date)

  GSlate <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (END_YR - 1981)), year = rep(1982:END_YR, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 365,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalend, lateend, year) %>%
    gather(var, Date, -site, -year) %>%
    arrange(site, year, Date) %>%
    mutate(ID = paste(site, year, sep = "_")) %>%
    dplyr::select(-site, -year) %>%
    group_by(ID) %>%
    distinct() %>%
    complete(Date = seq.Date((min(Date)), max(Date), by = "day")) %>%
    dplyr::select(-var) %>%
    separate(ID, into = c("site", "GrowingSeason"), sep = "_") %>%
    mutate(
      GSType = "lateGS",
      GrowingSeason = as.numeric(GrowingSeason),
      weights = 0.5
    ) %>%
    arrange(site, GrowingSeason, Date)

  GScutoffs <- siteinfo %>%
    full_join(tibble(site = rep(siteinfo$site, each = (END_YR - 1981)), year = rep(1982:END_YR, (nrow(siteinfo))))) %>%
    mutate(
      startyr = ifelse(Start > 182, year - 1, (year)),
      normalstart = paste(startyr, month(as_date(Start)), day(as_date(Start)), sep = "-"),
      normalstart = ymd(normalstart),
      normalend = normalstart + 364,
      earlystart = normalstart - 90,
      lateend = normalend + 90
    ) %>%
    dplyr::select(site, normalstart, normalend, year) %>%
    rename(GrowingSeason = year)

  pheno_early <- inner_join(GSearly, pheno_data_all_1)
  pheno_late <- GSlate %>% inner_join(pheno_data_all_1)
  pheno_normal <- GSmain %>% right_join(pheno_data_all_1)

  # so this is the final data, with duped early/late growing season
  pheno_data_all_2 <- bind_rows(pheno_early, pheno_late) %>%
    bind_rows(pheno_normal) %>%
    left_join(GScutoffs) %>%
    arrange(site, GrowingSeason, Date)

  siteinfo2 <- filter(pheno_data_all_2, GSType == "mainGS") %>%
    group_by(site, GrowingSeason) %>%
    summarise(
      MIN = min(NDVI),
      MAX = max(NDVI)
    ) %>%
    group_by(site) %>%
    summarise(
      avg_ndvi_min = mean(MIN),
      avg_ndvi_max = mean(MAX)
    ) %>%
    mutate(Max50 = (.5 * (avg_ndvi_max - avg_ndvi_min) + avg_ndvi_min)) %>%
    full_join(siteinfo)

  # to weight by max NDVI
  pheno_data_all <- pheno_normal %>%
    group_by(site, GrowingSeason) %>%
    summarise(
      N = n(),
      MAXNDVI = max(NDVI),
      MINNDVI = min(NDVI)
    ) %>%
    full_join(pheno_data_all_2) %>%
    mutate(weights = ifelse((NDVI == MAXNDVI & weights == 999), 1, .5))
  
  write.csv(pheno_data_all, "pheno_data_all.csv")

  kable(pheno_data_all %>% group_by(site) %>%
  summarise(
    `Start Date` = min(Date),
    `End Date` = max(Date),
    `N images (w/ shoulders)` = n(),
    `Site number` = mean(sitenum),
    `Julian GS start date` = mean(Start)
  ) %>%
  full_join(siteinfo2) %>%
  rename(`NDVI Green-up threshold` = Max50) %>%
  dplyr::select(`Site number`, site, `Start Date`, `End Date`, `Julian GS start date`, `NDVI Green-up threshold`, `N images (w/ shoulders)`)) %>%
  kable_styling("striped", full_width = F)
  
} else {
  print("not processing raw NDVI data")
}

pheno_data_all <- read_csv("./pheno_data_all.csv", col_type = cols())

```


* Then we fit cubic splines to each site-year combo. These splines are weighted (max NDVI in each year = weight 1; all others = weight 0.5), and have a smoothing parameter of 0.5 (spar = 0.5). The green-up criteria assigned is basically the 50% of the average range of NDVI at each site ((avg max - avg min * 50) + avg min) plus a little buffer. This means that green-up threshold is constant at all years within a site (it's a huge advantage over using derivative -based inflection points when the number of ndvi readings can be highly variable between years. also it's a bit nonsense to say that, hypothetically, green-up has been getting earlier over time and yet the NDVI at green-up date is not consistent.). 

  + Each site-year combo must have at least 10 NDVI readings (`measurementscutoff`) for us to determine phenological dates. 
   
  + Extra Green-up criteria:
    - The first derivative needed to indicate that the NDVI was increasing (first derivative > 0)
    - The NDVI needed to be above 0 (a low threshold, very easy!)
    - Green-up date was the FIRST DAY where NDVI was above the green-up threshold + 0.02 NDVI (I forget what all the problems were if it was simply just the cutoff, but they were there)
    - The green-up date must occur within the "main" growing season (not within the shoulder seasons).

  + Maximum:
    - Must occur after green-up date but within the "main" growing season (no shoulder seasons)
    - If there was no green-up date (due to sparse data, no rain, fire, whatever), we still forced a maximum NDVI date

  + Senescence:
    - The first derivative must be negative (indicating declining function/NDVI)
    - Must occur after the maximum NDVI date
    - NDVI must be less than the green-threshold (< Max50) but ABOVE the threshold - 0.05 (NDVI > (Max50 - .05))). Had to do this so that it wouldn't pick a senescence date that had a crazy low NDVI in the event of sparse data. 

Of course, all the dates were "redialed" as departure from the average start/end at each site to get change over time

```{r splineseachGS, echo = F, message = F, warning = F}
if (process_raw_ndvi == TRUE) {

  ##################
# with shoulder data; EACH SITE-YEAR COMBO NEEDS ITS OWN SPLINE. 
##################
emptydataframe <- tibble(
  x = numeric(),
  ndvi = numeric(),
  Date = as_date(character()),
  first.deriv = numeric(),
  Julian = numeric(),
  sitenum = numeric(0),
  site = character(),
  GrowingSeason = numeric(),
  Type = character(),
  gs = numeric(),
  END = character()
)

site_summary <- emptydataframe
for (i in c(1:nrow(siteinfo2))) {
  a <- i
  plot.filter <-  pheno_data_all %>% filter(sitenum == a) 
  gs_summary <- emptydataframe
  for (i in c((year(min(plot.filter$Date)) + 1):year(max(plot.filter$Date)) )) { #since no GSType shoulder in the first year, so gotta exlude. 
    gs <- i
    lowndvi <- (0)
    measurementscutoff <- 10 #min number of observations to have in gs -->> 7 = way too lax, 10 = better! 
    gs.filter <- plot.filter %>% filter(GrowingSeason == gs) 
    
    spline <- if (nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff ) {
      smooth.spline(x = gs.filter$Date, y = gs.filter$NDVI, spar = .5, w=gs.filter$weights) 
    } else {NA} 
    
    ndvi <-  if (nrow(filter(gs.filter, GSType=='mainGS')) >= measurementscutoff) {
      as_tibble(predict(spline, as.numeric(ymd(min(gs.filter$Date))):as.numeric(ymd(max(gs.filter$Date))), deriv = 0)) %>%
        rename(ndvi = y) %>%
        mutate(Date = as_date(x)) 
    } else {NA}
    
    first.deriv <- if (nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff) {
      as_tibble(predict(spline, as.numeric(ymd(min(gs.filter$Date))):as.numeric(ymd(max(gs.filter$Date))), deriv = 1)) %>%
        rename(first.deriv = y) %>%
        mutate(norm.first.deriv = ((1--1) / (max(first.deriv) - min(first.deriv)) * (first.deriv - max(first.deriv)) + 1))
    } else {NA}
    
    smoothed.data <- if(nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff) {
      full_join(ndvi, first.deriv, by = "x") %>%
        mutate(Date = as_date(x)) %>% #as.Date
        mutate(change.ndvi = (ndvi - lag(ndvi, n = 1L))) %>%
        mutate(GrowingSeason = gs) %>%    
        mutate(Julian = yday(Date)) %>%
        mutate(sitenum = a) %>%
        left_join(siteinfo2, by = 'sitenum')
    } else {emptydataframe %>% mutate(Max50 = numeric(),
                                      change.ndvi = numeric())}
    
    GreenUpDate <- if (nrow(smoothed.data %>% 
                            filter(first.deriv > 0, 
                                   ndvi > lowndvi, 
                                   ndvi < (Max50 + 0.05), #for elliot  + 0.03; for lagoas 0.05
                                   Date > max(gs.filter$normalstart ), 
                                   Date < max(gs.filter$normalend - 90))) > 0) { #ethass, start must be 3 mo before end or
      (smoothed.data %>% filter(first.deriv > 0, 
                                ndvi > Max50, 
                                ndvi < (Max50 + .05), 
                                Date < max(gs.filter$Date[gs.filter$GSType == 'mainGS']),
                                Date > max(gs.filter$normalstart),
                                Date < max(gs.filter$normalend - 90)) %>% 
         .[which.min(.$Date),]) 
    } else { emptydataframe }
    if (nrow(GreenUpDate) > 0) { GreenUpDate$Type <- "green-up date" } else { GreenUpDate$Type <- character(0) }
    if (nrow(GreenUpDate) > 0) { GreenUpDate$gs <- gs } else { GreenUpDate$gs <- numeric(0) }
    if (nrow(GreenUpDate) > 0) { GreenUpDate$sitenum <- a } else { GreenUpDate$sitenum <- numeric(0) }
    
    max <- if (nrow(GreenUpDate) > 0 ) {
      smoothed.data %>% filter(Date > GreenUpDate$Date, 
                               Date <= max(gs.filter$Date[gs.filter$GSType=='mainGS']) ) %>% 
        .[which.max(.$ndvi), ]
    } else {
      (filter((smoothed.data),# %>% mutate(change.ndvi = numeric())), 
              Date > (max(gs.filter$normalstart) + 60),
              Date < (max(gs.filter$normalend) - 60),
              change.ndvi > 0 )) %>%
        .[which.max(.$ndvi),]  }
    if (nrow(max) > 0) { max$Type <- "max" } else { max$Type <- character(0) }
    if (nrow(max) > 0) { max$gs <- gs } else { max$gs <- numeric(0) }
    if (nrow(max) > 0) { max$sitenum <- a } else { max$sitenum <- numeric(0) }
    
    SenescenseDate <- if (nrow(GreenUpDate) > 0) {
      smoothed.data %>% 
        filter(first.deriv < 0, 
               Date > max$Date,
               Date < max(gs.filter$normalend),
               ndvi < (Max50 + 0.0), #changed due to elliot 2017 (+0.05)...but jorn says keep it 0, 0 make sense
               ndvi > (Max50 - .05),
               Date < max(gs.filter$normalend - 30)) %>% #due to marc 2018...senesence can't really be in the next year...
        .[which.min(.$Date), ] #jorn made me change this form min to max??
    } else ({emptydataframe})
    if (nrow(SenescenseDate) > 0) { SenescenseDate$Type <- "senescence date" } else { SenescenseDate$Type <- character(0) }
    if (nrow(SenescenseDate) > 0) { SenescenseDate$gs <- gs } else { SenescenseDate$gs <- numeric(0) }
    if (nrow(SenescenseDate) > 0) { SenescenseDate$sitenum <- a } else { SenescenseDate$sitenum <- numeric(0) }
    
       # ggplotly(smoothed.data %>%
       #        ggplot() +
       #        geom_hline(yintercept = (smoothed.data[[1, 'Max50']] + 0.05)) +
       #        geom_point(aes(x = Date, y = ndvi))+
       #        geom_point(aes(x = Date, y = ndvi), col = 'blue', data = filter(smoothed.data,
       #                                                         first.deriv > 0,
       #                                                         ndvi > lowndvi,
       #                                                         ndvi < (smoothed.data[[1, 'Max50']] + 0.05)))+#,
       #                                                         # Date > ymd("1996-10-21"),
       #                                                         # Date < ymd("1997-10-20") - 90)) +
       #          geom_point(aes(x = Date, y = NDVI), data = gs.filter, pch = 2)+
       #          geom_point(aes(x = Date, y = ndvi), data = max, col = "green", size = 5))
       # ggplotly(smoothed.data %>%
       #    ggplot() +
       #    geom_hline(yintercept = (0.134)) +
       #    geom_point(aes(x = Date, y = ndvi))+
       #    geom_point(aes(x = Date, y = ndvi), col = 'blue', data = filter(smoothed.data,
       #                                                     first.deriv < 0,
       #                                                     Date > max$Date,
       #                                                     ndvi < (0.134 + 0.05),
       #                                                     ndvi > (0.134 - 0.05),
       #                                                     Date < ymd("2001-02-18") - 30)) +
       #      geom_point(aes(x = Date, y = NDVI), data = gs.filter, pch = 2)+
       #      geom_point(aes(x = Date, y = ndvi), data = SenescenseDate, col = "green", size = 5))

    pheno.dates <- bind_rows(GreenUpDate, max, SenescenseDate)
  
    gs_summary <- bind_rows(pheno.dates, gs_summary)
  }
  site_summary <- bind_rows(gs_summary, site_summary)
}


NutNetPhenoALL <- site_summary %>%
  dplyr::select(ndvi, Date, Julian, sitenum, Type, gs) %>%
  full_join(siteinfo2 %>% dplyr::select(-Start, -End)) 

NutNetPhenoFILTER1 <- NutNetPhenoALL %>% #if either sos or eos is missing, don't include those years in the average dates of anything
  dplyr::select(Julian, gs, site, Type) %>%
  pivot_wider(names_from = Type, values_from = Julian, id_cols = c(gs, site)) %>%
  filter(!is.na(`green-up date`), !is.na(`senescence date`)) %>%
  dplyr::select(gs, site)

NutNetPhenoFILTER2 <- NutNetPhenoFILTER1 %>% group_by(site) %>%
  summarise(N = n()) %>%
  filter(N >= 3) %>% #must have 3 yrs data to use
  left_join(NutNetPhenoFILTER1)

NutNetPheno <- NutNetPhenoALL %>% 
  right_join(NutNetPhenoFILTER2)

# NutNetPheno %>% filter(site == "pinj")
# 
# NutNetPheno %>% filter(gs >= 2018) %>% 
#   ggplot(aes(x = Date, y = ndvi, col = Type)) +
#   geom_point() + theme_cowplot()

#then need to redial dates into radians, to get change
AVG_nnp<-
  NutNetPheno %>% 
  mutate(DaysInYear = ifelse(leap_year(gs) == 'TRUE', 366, 365),
         DegreeJulian = (Julian / DaysInYear * 360)) %>% 
  group_by(site, Type) %>%
  summarise(DateRadianAVG = circmean_degrees(DegreeJulian)) %>%
  mutate(DateDateAVG = DateRadianAVG * 365 / 360,
         DDAVG = as_date(DateDateAVG),
         JulAGV = yday(DDAVG)) %>%
  dplyr::select(site, Type, JulAGV) %>% 
  spread(Type, JulAGV) 

redial<- AVG_nnp %>%
  full_join( tibble(site = rep(AVG_nnp$site, each = (END_YR - 1981)), 
                    year = rep(1982:END_YR, (nrow(AVG_nnp)))),  
             by = 'site') %>% 
  rename(maxJul = max,
         sosJul = `green-up date`,
         eosJul = `senescence date`) %>%
  full_join(dplyr::select(siteinfo2, site, Start), by = 'site')  %>% 
  mutate(startyrGU = ifelse(Start > 182 & sosJul > 182, year - 1, (year)),
         maxyr = ifelse(Start > 182 & maxJul > Start, year - 1, ifelse(Start < 182 & maxJul < Start, year + 1,  year)),
         eosyr = ifelse(Start > 182 & eosJul > Start, year - 1, ifelse(Start < 182 & eosJul < Start, year + 1,  year)),
         SOS = paste(startyrGU, month(as_date(sosJul)), day(as_date(sosJul)), sep = '-'),
         MAX = paste(maxyr, month(as_date(maxJul)), day(as_date(maxJul)), sep = '-'),
         EOS = paste(eosyr, month(as_date(eosJul)), day(as_date(eosJul)), sep = '-')) %>% 
  filter(!is.na(year)) %>% 
  mutate(SOS=as_date(SOS), 
         MAX = as_date(MAX), 
         EOS = as_date(EOS))  %>% 
  dplyr::select(site, year, SOS, MAX, EOS) %>%
  gather(Type, TypicalDate, -site, -year) %>%
  rename(gs = year) %>%
  mutate(Type= ifelse(Type=='SOS', 'green-up date', ifelse(Type=='MAX', 'max', 'senescence date'))) %>%
  right_join(
    NutNetPheno %>% 
      dplyr::select(site, gs, Type, Date) %>% 
      spread(Type, Date)  %>% 
      mutate(GU = ifelse(is.na(`senescence date`), NA, `green-up date`)) %>% 
      mutate(GU = as_date(GU)) %>% 
      dplyr::select(-`green-up date`) %>% 
      rename("green-up date" = GU) %>% 
      mutate(MAX = ifelse(is.na(`senescence date`), NA, max)) %>% 
      mutate(MAX = as_date(MAX)) %>% 
      dplyr::select(-max) %>%
      rename("max" = MAX) %>% 
      gather(Type, Date, -site, -gs) %>%
      filter(!is.na(Date))  ) %>%  
  mutate(DIFFERENCE = Date - TypicalDate)  


# write full data set
NutNetPheno_long <- redial %>% 
  full_join(NutNetPheno) %>%
  left_join(siteinfo2) %>%
  dplyr::select(-Julian, -TroughDateRadian, -avg_ndvi_min) %>% #, -avg_ndvi_max) %>% 
  rename(DaysFromAvg = DIFFERENCE) %>%
  arrange(site, gs, Type) %>%
  dplyr::select(-Max50, -Start, -End) %>%
  mutate(Change_ndvi_max = ifelse(Type =="max", (ndvi - avg_ndvi_max), NA)) #%>%
  # dplyr::select(-avg_ndvi_max)

NutNetPheno_long %>% 
  filter(site == "glac", Type == "green-up date") %>%
  ggplot(aes(x = gs, y = as.numeric(DaysFromAvg))) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(x = "Growing season", y = "Change from average, \n Green-up Date", title = "Glac") +
  theme_cowplot()

NutNetPheno_long %>% filter(Type == "green-up date") %>% arrange(-DaysFromAvg)
NutNetPheno_long %>% filter(Type == "green-up date") %>% arrange(DaysFromAvg)


NutNetPheno_long %>%
  filter(Type == "green-up date") %>% 
  ggplot() +
  geom_histogram(aes(x = DaysFromAvg))

write_csv(NutNetPheno_long, "NutNetPheno_long.csv")

} else {
  print("not processing raw NDVI data")
}

NutNetPheno_long <- read_csv("./NutNetPheno_long.csv", col_type = cols())

NutNetPheno_wide <- NutNetPheno_long %>%
  dplyr::select(site, gs, Type, DaysFromAvg) %>%
  spread(Type, DaysFromAvg) %>%
  rename(ChangeSOS = `green-up date`, ChangeMAX = max, ChangeEOS = `senescence date`) %>% #, ChangeGSL = gsl) %>%
  full_join(NutNetPheno_long %>%
              dplyr::select(site, gs, Type, ndvi) %>%
              spread(Type, ndvi) %>%
              rename(SOSndvi = `green-up date`, MAXndvi = max, EOSndvi = `senescence date`)) %>%
  full_join(NutNetPheno_long %>%
              dplyr::select(site, gs, Type, Date) %>%
              spread(Type, Date) %>%
              mutate(GSLength = `senescence date` - `green-up date`) %>%
              rename(SOSdate = `green-up date`, MAXdate = max, EOSdate = `senescence date`)) %>%
  full_join(NutNetPheno_long %>%
              dplyr::select(site, sitenum, gs, Type, TypicalDate) %>%
              filter(Type!= "gsl") %>%
              pivot_wider(names_from = Type, values_from = TypicalDate) %>%
              rename(SOStypicaldate = `green-up date`, MAXtypicaldate = max, EOStypicaldate = `senescence date`) %>%
              mutate(TypicalGSL = EOStypicaldate - SOStypicaldate)) %>%
  full_join(NutNetPheno_long %>%
              group_by(site) %>%
              summarise("TypicalMaxNDVI" = mean(avg_ndvi_max))) %>%
  mutate(ChangeGSL = GSLength - TypicalGSL) %>%
  filter(!is.na(GSLength)) %>%
  full_join(NutNetPheno_long %>%
              filter(Type == "max") %>%
              dplyr::select(site, gs, Change_ndvi_max))

```

* For plotting purposes, the individual year-site combo splines were run again, and all the fitted NDVI values extracted. Here is a figure showing all processed NDVI data (the black points are the actual NDVI readings, and the black line is the fitted NDVI spline from which phenological dates were extracted).

```{r fittedndvi, eval = T, echo = F, message = F, warning = F}

if (process_raw_ndvi == TRUE) {

#######
#return all fitted NDVI values
#####
full_ndvi_edf <- tibble(
  x = numeric(),
  ndvi = numeric(),
  Date = as_date(character()),
  sitenum = numeric(0), 
  gs = numeric())

ndvi_site_summary_edf <- full_ndvi_edf

for (i in c(1:nrow(siteinfo2))) {
  a <- i
  plot.filter <- pheno_data_all %>% filter(sitenum == a) 
  ndvi_gs_summary_edf <- full_ndvi_edf
  for (i in c(year(min(plot.filter$Date)):year(max(plot.filter$Date)) )) {
    gs <- i
    lowndvi <- (0)
    gs.filter <- plot.filter %>% filter(GrowingSeason == gs) %>%
      select(GrowingSeason, site, GSType, NDVI, Date, weights)
    
    spline <- if (nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff) {
      smooth.spline(x = gs.filter$Date, y = gs.filter$NDVI, spar = .5, w = gs.filter$weights) 
    } else {NA}
    
    fullndvi_fitting <- if (nrow(filter(gs.filter, GSType == 'mainGS')) >= measurementscutoff) {
      as_tibble(predict(spline, as.numeric(ymd(min(gs.filter$Date))):as.numeric(ymd(max(gs.filter$Date))), deriv = 0)) %>% 
        rename(ndvi = y) %>% 
        mutate(Date = as_date(x))
    } else {full_ndvi_edf}
    
    if (nrow(fullndvi_fitting)>0 ) { fullndvi_fitting$sitenum <- a } else { NA}
    if (nrow(fullndvi_fitting)>0) { fullndvi_fitting$gs <- gs } else { NA }
    ndvi_gs_summary_edf <- bind_rows(fullndvi_fitting, ndvi_gs_summary_edf)
  }
  ndvi_site_summary_edf <- bind_rows(ndvi_gs_summary_edf, ndvi_site_summary_edf)
}

allfittedndvi <- ndvi_site_summary_edf %>% 
  left_join(siteinfo2 %>%
              select(site, sitenum))

write.csv(allfittedndvi, "allfittedndvi.csv")

} else {
  print("not processing raw NDVI data")
}

allfittedndvi <- read_csv("./allfittedndvi.csv", col_types = cols())

#make a test figure
test = "elliot" #comp
grow = 1980
#plot of various locations
(allfittedndvi %>% 
  filter(site == test, gs >=grow) %>% 
  ggplot(aes(x = Date, y = ndvi)) +
  geom_point(size = .1) +
  geom_point(data = filter(pheno_data_all, site == test, GrowingSeason >= grow), aes(y = NDVI), pch = 21) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.major.x = element_line(colour = 'grey')) +
  geom_point(data = filter(NutNetPheno_long, site == test, gs >= grow), aes(col = Type), size = 2) +
  theme_cowplot()+
  labs(title = test)) 

```


# Determine sites to use in analysis

Given that we are looking at long-term trends, it doesn't make much sense to include sites which only have a minimal amount of years were we could extract meaningful phenological dates. Reasons that sites might not have enough years include: satellites had poor coverage of location (esp true in earlier Landsats), too much cloud cover (and thus couldn't fit a nice trend/too few images during growing season), site never "greened-up" threshold during a given year (important esp for Mediterranean sites, and probably other reasons.

```{r filtershorttermsites, echo=F, message=F, warning=F}
if (create_final_df == TRUE) {

  longtermsites <- NutNetPheno_wide %>%
  filter(!is.na(GSLength)) %>%
  group_by(site) %>%
  summarise(N = n()) %>%
  filter(N>=5) %>%
  select(-N) %>%
  filter(site %in% SiteInfo$site) %>%
  inner_join(NutNetPheno_long %>%
               group_by(site) %>%
               summarise(sitenum = mean(sitenum))) %>%
  mutate(sitenum2 = row_number())
write_csv(longtermsites, './longtermsites.csv')

levels(as.factor(longtermsites$site))


} else {
  print("not creating final data sets")
}
```

# Merge climate data with phenology data

## Temporal trends in climate anomalies
* The goal is to recognize that some growing seasons see bigger temperature and precipitation anomalies, does that influence site phenology/productivity? To look at the temperature/precipitation anomalies in each growing season:
  + We can look weather windows BEFORE/during the typical green-up month (here looking at the 2 month window before the typical green-up date)
  + And also do weather windows around the typical date of NDVI max (here looking at a 5 month window around the typical maximum NDVI date (basically the month of max ndvi + the 2 months before and after from that date; this is essentially a growing season)

This plot simply shows sites which have seen a significant trend in temperatures and precipitation over time within a 2 month window preceding the site-specific typical green-up date. 

```{r temptrends, eval = T, echo = F, message = F, warning = F}

if (merge_climate_ndvi == TRUE) {

###
# Must make full df with all possible climate change dates (1984 - END_YR growing seasons)
####
  END_YR <- 2021
CLIMYRS <- tibble(gs = rep(c(1984:END_YR), nrow(longtermsites)),
       site = rep(longtermsites$site, each = (END_YR-1983)))

CLIMYRS_ALL <- NutNetPheno_wide %>%
  group_by(site) %>% summarise(gs = min(gs)) %>%
  left_join(NutNetPheno_wide %>% select(site, gs, SOStypicaldate, MAXtypicaldate, EOStypicaldate)) %>%
  mutate(typicalGSL = EOStypicaldate - SOStypicaldate) %>%
  dplyr::select(-EOStypicaldate) %>%
  mutate(SOStypicalDAY = day(SOStypicaldate),
         SOStypicalMONTH = month(SOStypicaldate),
         MAXtypicalDAY = day(MAXtypicaldate),
         MAXtypicalMONTH = month(MAXtypicaldate),
         SOStypicalYEAR = ifelse(year(SOStypicaldate)==gs, "GS", 
                                 ifelse(year(SOStypicaldate) == (gs-1), "GS-1", NA)),
         MAXtypicalYEAR = ifelse(year(MAXtypicaldate)==gs, "GS",
                                 ifelse(year(MAXtypicaldate) == (gs - 1), "GS-1", NA))) %>%
  dplyr::select(-gs, -SOStypicaldate, -MAXtypicaldate) %>%
  full_join(CLIMYRS) %>%
  mutate(TYPICALSOS = ymd(paste(ifelse(SOStypicalYEAR == "GS", gs, 
                                       ifelse(SOStypicalYEAR == "GS-1", (gs - 1), NA)), SOStypicalMONTH, SOStypicalDAY, sep = "-")),
         TYPICALMAX = ymd(paste(ifelse(MAXtypicalYEAR == "GS", gs, 
                                      ifelse(MAXtypicalYEAR == "GS-1", (gs - 1), NA)), MAXtypicalMONTH, MAXtypicalDAY, sep = "-"))) %>%
  select(site, gs, TYPICALSOS, TYPICALMAX, typicalGSL) 

#####
# Window around SOS date, preceding only
#####
#MONTH
clim_month_soswindow <- CLIMYRS_ALL %>% 
  select(site, gs, TYPICALSOS) %>%
  mutate(SOS_typical = month(TYPICALSOS),
         SOS_typical_sub1  = month(TYPICALSOS - 30),
         SOS_typical_sub2  = month(TYPICALSOS - 60),
         SOS_typical_sub3  = month(TYPICALSOS - 90),
         SOS_typical_sub4  = month(TYPICALSOS - 120)) %>%
  gather(-site, -gs, -TYPICALSOS, key = "TYPE", value = "MONTH")
#YEAR
clim_year_soswindow <- CLIMYRS_ALL %>% 
  select(site, gs, TYPICALSOS) %>%
  mutate(SOS_typical = year(TYPICALSOS),
         SOS_typical_sub1 = year(TYPICALSOS - 30),
         SOS_typical_sub2 = year(TYPICALSOS - 60),
         SOS_typical_sub3 = year(TYPICALSOS - 90),
         SOS_typical_sub4 = year(TYPICALSOS - 120))  %>%
  gather(-site, -gs, -TYPICALSOS, key = "TYPE", value = "YEAR")


ee_precip_soswindow <- full_join(clim_month_soswindow, clim_year_soswindow) %>%
  left_join(ClimChange %>%
            mutate(MONTH = month(plotdate),
                   YEAR = year(plotdate))) %>%
  select(site, gs, TYPE, PrecipDifference) %>%
  pivot_wider(names_from = TYPE, values_from = PrecipDifference) %>%
  rowwise() %>%
  mutate(anom_1moPrecip = SOS_typical,
         anom_2moPrecip = (anom_1moPrecip + SOS_typical_sub1),
         anom_3moPrecip = anom_2moPrecip + SOS_typical_sub2,
         anom_4moPrecip = anom_3moPrecip + SOS_typical_sub3,
         anom_5moPrecip = anom_4moPrecip + SOS_typical_sub4) %>%
  rename(SOSPrecip_typical = SOS_typical,
         SOSPrecip_typical_sub1 = SOS_typical_sub1,
         SOSPrecip_typical_sub2 = SOS_typical_sub2,
         SOSPrecip_typical_sub3 = SOS_typical_sub3,
         SOSPrecip_typical_sub4 = SOS_typical_sub4)

ee_clim_soswindow <- full_join(clim_month_soswindow, clim_year_soswindow) %>%
  left_join(ClimChange %>%
            mutate(MONTH = month(plotdate),
                   YEAR = year(plotdate))) %>%
  select(site, gs, TYPE, TempDifference) %>%
  pivot_wider(names_from = TYPE, values_from = TempDifference) %>%
  rowwise() %>%
  mutate(anom_1moTemp = SOS_typical,
         anom_2moTemp = (anom_1moTemp + SOS_typical_sub1),
         anom_3moTemp = anom_2moTemp + SOS_typical_sub2,
         anom_4moTemp = anom_3moTemp + SOS_typical_sub3,
         anom_5moTemp = anom_4moTemp + SOS_typical_sub4) %>%
  full_join(NutNetPheno_wide) %>%
  full_join(ee_precip_soswindow)


#####
# Window around MAX date
#####
#MONTH
clim_month_maxwindow <- CLIMYRS_ALL %>% 
  select(site, gs, TYPICALMAX) %>%
  mutate(MAX_typical = month(TYPICALMAX),
         MAX_typical_sub1  = month(TYPICALMAX - 30),
         MAX_typical_plus1  = month(TYPICALMAX + 30),
         MAX_typical_sub2  = month(TYPICALMAX - 60),
         MAX_typical_plus2  = month(TYPICALMAX + 60),
         MAX_typical_sub3  = month(TYPICALMAX - 90),
         MAX_typical_plus3  = month(TYPICALMAX + 90),
         MAX_typical_sub4  = month(TYPICALMAX - 120),
         MAX_typical_plus4  = month(TYPICALMAX + 120)) %>%
  gather(-site, -gs, -TYPICALMAX, key = "TYPE", value = "MONTH")
#YEAR
clim_year_maxwindow <- CLIMYRS_ALL %>% 
  select(site, gs, TYPICALMAX) %>% 
  mutate(MAX_typical = year(TYPICALMAX),
         MAX_typical_sub1 = year(TYPICALMAX - 30),
         MAX_typical_plus1 = year(TYPICALMAX + 30),
         MAX_typical_sub2 = year(TYPICALMAX - 60),
         MAX_typical_plus2 = year(TYPICALMAX + 60),
         MAX_typical_sub3 = year(TYPICALMAX - 90),
         MAX_typical_plus3 = year(TYPICALMAX + 90),
         MAX_typical_sub4 = year(TYPICALMAX - 120),
         MAX_typical_plus4 = year(TYPICALMAX + 120))  %>%
  gather(-site, -gs, -TYPICALMAX, key = "TYPE", value = "YEAR")

ee_precip_maxwindow <- full_join(clim_month_maxwindow, clim_year_maxwindow) %>%
  left_join(ClimChange %>%
            mutate(MONTH = month(plotdate),
                   YEAR = year(plotdate))) %>%
  select(site, gs, TYPE, PrecipDifference) %>%
  pivot_wider(names_from = TYPE, values_from = PrecipDifference) %>%
  rowwise() %>%
  mutate(anom_1moPrecip = MAX_typical, 
         anom_3moPrecip = (MAX_typical + MAX_typical_sub1 + MAX_typical_plus1),
         anom_5moPrecip = anom_3moPrecip + MAX_typical_sub2 + MAX_typical_plus2,
         anom_7moPrecip = anom_5moPrecip + MAX_typical_sub3 + MAX_typical_plus3,
         anom_9moPrecip = anom_7moPrecip + MAX_typical_sub4 + MAX_typical_plus4) %>%
  rename(MAXPrecip_typical = MAX_typical,
         MAXPrecip_typical = MAX_typical,
         MAXPrecip_typical_sub1 = MAX_typical_sub1,
         MAXPrecip_typical_plus1 = MAX_typical_plus1,
         MAXPrecip_typical_sub2 = MAX_typical_sub2,
         MAXPrecip_typical_plus2 = MAX_typical_plus2,
         MAXPrecip_typical_sub3 = MAX_typical_sub3,
         MAXPrecip_typical_plus3 = MAX_typical_plus3,
         MAXPrecip_typical_sub4 = MAX_typical_sub4,
         MAXPrecip_typical_plus4 = MAX_typical_plus4)


ee_clim_maxwindow <- full_join(clim_month_maxwindow, clim_year_maxwindow) %>%
  left_join(ClimChange %>%
            mutate(MONTH = month(plotdate),
                   YEAR = year(plotdate))) %>%
  select(site, gs, TYPE, TempDifference) %>%
  pivot_wider(names_from = TYPE, values_from = TempDifference) %>%
  rowwise() %>%
  mutate(anom_1moTemp = MAX_typical, 
         anom_3moTemp = (MAX_typical + MAX_typical_sub1 + MAX_typical_plus1),
         anom_5moTemp = anom_3moTemp + MAX_typical_sub2 + MAX_typical_plus2,
         anom_7moTemp = anom_5moTemp + MAX_typical_sub3 + MAX_typical_plus3,
         anom_9moTemp = anom_7moTemp + MAX_typical_sub4 + MAX_typical_plus4) %>%
  full_join(ee_precip_maxwindow) %>%
  full_join(NutNetPheno_wide)  #%>%
  # mutate(TypicalGSL = EOStypicaldate - SOStypicaldate,
         # ChangeGSL = GSLength - TypicalGSL)



###
# Programatically look at temp changes
###
climdataframe <- tibble(
  site = numeric(), 
  sos1moTempF = numeric(), sos1moTempP = numeric(),
  sos2moTempF = numeric(), sos2moTempP = numeric(), sos2moTempSLOPE = numeric(),
  sos3moTempF = numeric(), sos3moTempP = numeric(),
  sos4moTempF = numeric(), sos4moTempP = numeric(),
  max1moTempF = numeric(), max1moTempP = numeric(),
  max3moTempF = numeric(), max3moTempP = numeric(),
  max5moTempF = numeric(), max5moTempP = numeric(), max5moTempSLOPE = numeric(),
  max7moTempF = numeric(), max7moTempP = numeric(),
  max9moTempF = numeric(), max9moTempP = numeric(),
    
  sos1moPrecipF = numeric(), sos1moPrecipP = numeric(),
  sos2moPrecipF = numeric(), sos2moPrecipP = numeric(), sos2moPrecipSLOPE = numeric(),
  sos3moPrecipF = numeric(), sos3moPrecipP = numeric(),
  sos4moPrecipF = numeric(), sos4moPrecipP = numeric(),
  max1moPrecipF = numeric(), max1moPrecipP = numeric(),
  max3moPrecipF = numeric(), max3moPrecipP = numeric(),
  max5moPrecipF = numeric(), max5moPrecipP = numeric(), max5moPrecipSLOPE = numeric(),
  max7moPrecipF = numeric(), max7moPrecipP = numeric(),
  max9moPrecipF = numeric(), max9moPrecipP = numeric())

climate_anom <- ee_clim_maxwindow %>%
  select(site, gs, anom_1moTemp, anom_3moTemp, anom_5moTemp, anom_7moTemp, anom_9moTemp, 
         anom_1moPrecip, anom_3moPrecip, anom_5moPrecip, anom_7moPrecip, anom_9moPrecip) %>%
  gather(-site, -gs, key = Type, value = Anomaly) %>%
  mutate(Type = paste0(Type, "_max")) %>%
  bind_rows( ee_clim_soswindow %>%
               select(site, gs, anom_1moTemp, anom_2moTemp, anom_3moTemp, anom_4moTemp, anom_5moTemp,
                      anom_1moPrecip, anom_2moPrecip, anom_3moPrecip, anom_4moPrecip, anom_5moPrecip) %>%
               gather(-site, -gs, key = Type, value = Anomaly) %>%
               mutate(Type = paste0(Type, "_sos"))) %>%
  full_join(longtermsites) %>% select(-sitenum)

clim_statssite_summary <- climdataframe
for (i in c(1:nrow(longtermsites))) {
  a <- i
  sitefilter <- climate_anom %>% filter(sitenum2 == a)
  
  max1moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_1moTemp_max')) > 2) {
    (lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_1moTemp_max')))
  } else {max1moTemp_mod <- statsdataframe}
  max1moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_1moTemp_max')) > 2) {Anova(max1moTemp_mod)$`F value`[1]} else {NA}
  max1moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_1moTemp_max')) > 2) {Anova(max1moTemp_mod)$`Pr(>F)`[1]} else {NA}
  
  max3moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_3moTemp_max')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_3moTemp_max')))
  } else {max3moTemp_mod <- statsdataframe}
  max3moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_3moTemp_max')) > 2) {max3moTemp_mod$`F value`[1]} else {NA}
  max3moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_3moTemp_max')) > 2) {max3moTemp_mod$`Pr(>F)`[1]} else {NA}

  max5moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_5moTemp_max')) > 2) {
    (lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_5moTemp_max')))
  } else {max5moTemp_mod <- statsdataframe}
  max5moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_5moTemp_max')) > 2) {Anova(max5moTemp_mod)$`F value`[1]} else {NA}
  max5moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_5moTemp_max')) > 2) {Anova(max5moTemp_mod)$`Pr(>F)`[1]} else {NA}
  max5moTempSLOPE <- if (nrow(filter(sitefilter, Type == "anom_5moTemp_max")) > 2) { max5moTemp_mod$coefficients[2] } else {NA}

  max7moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_7moTemp_max')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_7moTemp_max')))
  } else {max7moTemp_mod <- statsdataframe}
  max7moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_7moTemp_max')) > 2) {max7moTemp_mod$`F value`[1]} else {NA}
  max7moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_7moTemp_max')) > 2) {max7moTemp_mod$`Pr(>F)`[1]} else {NA}

  max9moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_9moTemp_max')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_9moTemp_max')))
  } else {max9moTemp_mod <- statsdataframe}
  max9moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_9moTemp_max')) > 2) {max9moTemp_mod$`F value`[1]} else {NA}
  max9moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_9moTemp_max')) > 2) {max9moTemp_mod$`Pr(>F)`[1]} else {NA}

  sos1moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_1moTemp_sos')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_1moTemp_sos')))
  } else {sos1moTemp_mod <- statsdataframe}
  sos1moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_1moTemp_sos')) > 2) {sos1moTemp_mod$`F value`[1]} else {NA}
  sos1moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_1moTemp_sos')) > 2) {sos1moTemp_mod$`Pr(>F)`[1]} else {NA}
        
  sos2moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_2moTemp_sos')) > 2) {
    (lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_2moTemp_sos')))
  } else {sos2moTemp_mod <- statsdataframe}
  sos2moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_2moTemp_sos')) > 2) {Anova(sos2moTemp_mod)$`F value`[1]} else {NA}
  sos2moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_2moTemp_sos')) > 2) {Anova(sos2moTemp_mod)$`Pr(>F)`[1]} else {NA}
  sos2moTempSLOPE <- if (nrow(filter(sitefilter, Type == "anom_2moTemp_sos")) > 2) { sos2moTemp_mod$coefficients[2] } else {NA}
        
  sos3moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_3moTemp_sos')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_3moTemp_sos')))
  } else {sos3moTemp_mod <- statsdataframe}
  sos3moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_3moTemp_sos')) > 2) {sos3moTemp_mod$`F value`[1]} else {NA}
  sos3moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_3moTemp_sos')) > 2) {sos3moTemp_mod$`Pr(>F)`[1]} else {NA}
        
  sos4moTemp_mod<-if (nrow(filter(sitefilter, Type == 'anom_4moTemp_sos')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_4moTemp_sos')))
  } else {sos4moTemp_mod <- statsdataframe}
  sos4moTempF <-   if (nrow(filter(sitefilter, Type == 'anom_4moTemp_sos')) > 2) {sos4moTemp_mod$`F value`[1]} else {NA}
  sos4moTempP <-   if (nrow(filter(sitefilter, Type == 'anom_4moTemp_sos')) > 2) {sos4moTemp_mod$`Pr(>F)`[1]} else {NA}
  
  
  #do precip
  
  max1moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_1moPrecip_max')) > 2) {
    (lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_1moPrecip_max')))
  } else {max1moPrecip_mod <- statsdataframe}
  max1moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_1moPrecip_max')) > 2) {Anova(max1moPrecip_mod)$`F value`[1]} else {NA}
  max1moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_1moPrecip_max')) > 2) {Anova(max1moPrecip_mod)$`Pr(>F)`[1]} else {NA}
  
  max3moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_3moPrecip_max')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_3moPrecip_max')))
  } else {max3moPrecip_mod <- statsdataframe}
  max3moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_3moPrecip_max')) > 2) {max3moPrecip_mod$`F value`[1]} else {NA}
  max3moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_3moPrecip_max')) > 2) {max3moPrecip_mod$`Pr(>F)`[1]} else {NA}

  max5moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_5moPrecip_max')) > 2) {
    (lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_5moPrecip_max')))
  } else {max5moPrecip_mod <- statsdataframe}
  max5moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_5moPrecip_max')) > 2) {Anova(max5moPrecip_mod)$`F value`[1]} else {NA}
  max5moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_5moPrecip_max')) > 2) {Anova(max5moPrecip_mod)$`Pr(>F)`[1]} else {NA}
  max5moPrecipSLOPE <- if (nrow(filter(sitefilter, Type == "anom_5moPrecip_max")) > 2) { max5moPrecip_mod$coefficients[2] } else {NA}

  max7moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_7moPrecip_max')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_7moPrecip_max')))
  } else {max7moPrecip_mod <- statsdataframe}
  max7moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_7moPrecip_max')) > 2) {max7moPrecip_mod$`F value`[1]} else {NA}
  max7moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_7moPrecip_max')) > 2) {max7moPrecip_mod$`Pr(>F)`[1]} else {NA}

  max9moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_9moPrecip_max')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_9moPrecip_max')))
  } else {max9moPrecip_mod <- statsdataframe}
  max9moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_9moPrecip_max')) > 2) {max9moPrecip_mod$`F value`[1]} else {NA}
  max9moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_9moPrecip_max')) > 2) {max9moPrecip_mod$`Pr(>F)`[1]} else {NA}

  sos1moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_1moPrecip_sos')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_1moPrecip_sos')))
  } else {sos1moPrecip_mod <- statsdataframe}
  sos1moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_1moPrecip_sos')) > 2) {sos1moPrecip_mod$`F value`[1]} else {NA}
  sos1moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_1moPrecip_sos')) > 2) {sos1moPrecip_mod$`Pr(>F)`[1]} else {NA}
        
  sos2moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_2moPrecip_sos')) > 2) {
    (lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_2moPrecip_sos')))
  } else {sos2moPrecip_mod <- statsdataframe}
  sos2moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_2moPrecip_sos')) > 2) {Anova(sos2moPrecip_mod)$`F value`[1]} else {NA}
  sos2moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_2moPrecip_sos')) > 2) {Anova(sos2moPrecip_mod)$`Pr(>F)`[1]} else {NA}
  sos2moPrecipSLOPE <- if (nrow(filter(sitefilter, Type == "anom_2moPrecip_sos")) > 2) { sos2moPrecip_mod$coefficients[2] } else {NA}
        
  sos3moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_3moPrecip_sos')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_3moPrecip_sos')))
  } else {sos3moPrecip_mod <- statsdataframe}
  sos3moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_3moPrecip_sos')) > 2) {sos3moPrecip_mod$`F value`[1]} else {NA}
  sos3moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_3moPrecip_sos')) > 2) {sos3moPrecip_mod$`Pr(>F)`[1]} else {NA}
        
  sos4moPrecip_mod<-if (nrow(filter(sitefilter, Type == 'anom_4moPrecip_sos')) > 2) {
    Anova(lm(Anomaly ~ gs, data = filter(sitefilter, Type == 'anom_4moPrecip_sos')))
  } else {sos4moPrecip_mod <- statsdataframe}
  sos4moPrecipF <-   if (nrow(filter(sitefilter, Type == 'anom_4moPrecip_sos')) > 2) {sos4moPrecip_mod$`F value`[1]} else {NA}
  sos4moPrecipP <-   if (nrow(filter(sitefilter, Type == 'anom_4moPrecip_sos')) > 2) {sos4moPrecip_mod$`Pr(>F)`[1]} else {NA}
  
        
  CLIMstats <- tibble(a, 
                  sos1moTempF, sos1moTempP,
                  sos2moTempF, sos2moTempP, sos2moTempSLOPE, 
                  sos3moTempF, sos3moTempP,
                  sos4moTempF, sos4moTempP,
                  max1moTempF, max1moTempP,
                  max3moTempF, max3moTempP,
                  max5moTempF, max5moTempP, max5moTempSLOPE, 
                  max7moTempF, max7moTempP,
                  max9moTempF, max9moTempP,
                  
                  sos1moPrecipF, sos1moPrecipP,
                  sos2moPrecipF, sos2moPrecipP, sos2moPrecipSLOPE, 
                  sos3moPrecipF, sos3moPrecipP,
                  sos4moPrecipF, sos4moPrecipP,
                  max1moPrecipF, max1moPrecipP,
                  max3moPrecipF, max3moPrecipP,
                  max5moPrecipF, max5moPrecipP, max5moPrecipSLOPE, 
                  max7moPrecipF, max7moPrecipP,
                  max9moPrecipF, max9moPrecipP)
  clim_statssite_summary <- rbind (CLIMstats, clim_statssite_summary)
}
# 
# overall_cc_mod <- ClimChange %>% 
#   split(.$site) %>% 
#   map(~lm(tmp_degrees_Celsius ~ plotdate, data = .))
# 
# overall_cc_site <- overall_cc_mod %>%
#   map(summary) 
# 
# overall_cc_p <- overall_cc_mod %>%
#   map(summary) %>%
#   map_dbl(~.$coefficients[2,4]) %>% 
#   as_tibble() %>%
#   rename(pvalue_overalltemp = value) %>%
#   mutate(site = names(overall_cc_mod))
# 
# overall_cc_slope <- overall_cc %>%
#   map(summary) %>%
#   map_dbl(~.$coefficients[2,1]) %>% 
#   as_tibble() %>%
#   rename(Value = value) %>%
#   mutate(site = names(overall_cc_mod),
#          )
# 
# overall_cc <- full_join(overall_cc_p, overall_cc_slope)
# 
# # ClimChange %>% 
# #   filter(site == "sereng") %>%
# #   ggplot(aes(x = plotdate, y = tmp_degrees_Celsius)) +
# #   geom_point()

climate_anom_df <- (dplyr::select(clim_statssite_summary, a,  sos1moTempF, sos2moTempF,sos3moTempF, sos4moTempF, 
                                    sos1moPrecipF, sos2moPrecipF,sos3moPrecipF, sos4moPrecipF, 
                                 max1moTempF, max3moTempF, max5moTempF, max7moTempF,max9moTempF,
                                    max1moPrecipF, max3moPrecipF, max5moPrecipF, max7moPrecipF,max9moPrecipF) %>% 
                     gather(Type, Value, -a) %>% 
                     mutate(Stat = 'F value')) %>%
  bind_rows((dplyr::select(clim_statssite_summary, a, sos1moTempP, sos2moTempP,sos3moTempP, sos4moTempP, 
                           max1moTempP, max3moTempP, max5moTempP, max7moTempP,max9moTempP,
                           sos1moPrecipP, sos2moPrecipP,sos3moPrecipP, sos4moPrecipP, 
                           max1moPrecipP, max3moPrecipP, max5moPrecipP, max7moPrecipP,max9moPrecipP) %>% 
               gather(Type, Value, -a) %>% 
               mutate(Stat = 'P value'))) %>%
  bind_rows(select(clim_statssite_summary, a, max5moTempSLOPE, sos2moTempSLOPE, 
                   max5moPrecipSLOPE, sos2moPrecipSLOPE) %>%
              gather(Type, Value, -a) %>%
              mutate(Stat = "Slope")) %>%
  rename(sitenum2 = a) %>%
  mutate(Type = recode(Type, "sos1moTempF" = "anom_1moTemp_sos", "sos1moTempP" = "anom_1moTemp_sos",
                       "sos2moTempF" = "anom_2moTemp_sos", "sos2moTempP" = "anom_2moTemp_sos", "sos2moTempSLOPE" = "anom_2moTemp_sos",
                       "sos3moTempF" = "anom_3moTemp_sos", "sos3moTempP" = "anom_3moTemp_sos",
                       "sos4moTempF" = "anom_4moTemp_sos", "sos4moTempP" = "anom_4moTemp_sos",
                       "max1moTempF" = "anom_1moTemp_max", "max1moTempP" = "anom_1moTemp_max",
                       "max3moTempF" = "anom_3moTemp_max", "max3moTempP" = "anom_3moTemp_max",
                       "max5moTempF" = "anom_5moTemp_max", "max5moTempP" = "anom_5moTemp_max", "max5moTempSLOPE" = "anom_5moTemp_max",
                       "max7moTempF" = "anom_7moTemp_max", "max7moTempP" = "anom_7moTemp_max",
                       "max9moTempF" = "anom_9moTemp_max", "max9moTempP" = "anom_9moTemp_max",
                       
                       "sos1moPrecipF" = "anom_1moPrecip_sos", "sos1moPrecipP" = "anom_1moPrecip_sos",
                       "sos2moPrecipF" = "anom_2moPrecip_sos", "sos2moPrecipP" = "anom_2moPrecip_sos", "sos2moPrecipSLOPE" = "anom_2moPrecip_sos",
                       "sos3moPrecipF" = "anom_3moPrecip_sos", "sos3moPrecipP" = "anom_3moPrecip_sos",
                       "sos4moPrecipF" = "anom_4moPrecip_sos", "sos4moPrecipP" = "anom_4moPrecip_sos",
                       "max1moPrecipF" = "anom_1moPrecip_max", "max1moPrecipP" = "anom_1moPrecip_max",
                       "max3moPrecipF" = "anom_3moPrecip_max", "max3moPrecipP" = "anom_3moPrecip_max",
                       "max5moPrecipF" = "anom_5moPrecip_max", "max5moPrecipP" = "anom_5moPrecip_max", "max5moPrecipSLOPE" = "anom_5moPrecip_max",
                       "max7moPrecipF" = "anom_7moPrecip_max", "max7moPrecipP" = "anom_7moPrecip_max",
                       "max9moPrecipF" = "anom_9moPrecip_max", "max9moPrecipP" = "anom_9moPrecip_max")) %>%
  full_join((longtermsites %>% select(-sitenum)))  %>% 
  dplyr::select(-sitenum2) %>%
  full_join(climate_anom) %>%
  mutate(includeCHANGE = ifelse(Stat == 'P value' & Value <= 0.05, 'sig', 'ns')) %>%
  filter(Stat != "F value")


write_csv(climate_anom_df, 'climate_anom_df.csv')

} else 
{ print("Not merging weather and ndvi")}

climate_anom_df <- read_csv("./climate_anom_df.csv", col_types = cols())

## make some example figs
climate_anom_df %>%
  filter(includeCHANGE == 'sig', Type == 'anom_2moTemp_sos', Stat == "P value") %>%
  ggplot(aes(x = gs, y = Anomaly)) +
  geom_hline(yintercept = 0, lty = 2)+
  geom_point() +
  geom_smooth(method='lm', fill=NA, col='black')+
  facet_wrap(~site) +
  theme_cowplot() +
  labs(x='Growing Season', y='SOS temp anom \n(2-mo window around typical SOS)')

climate_anom_df %>%
  filter(includeCHANGE == 'sig', Type == 'anom_2moPrecip_sos', Stat == "P value") %>%
  ggplot(aes(x = gs, y = Anomaly)) +
  geom_hline(yintercept = 0, lty = 2)+
  geom_point() +
  geom_smooth(method='lm', fill=NA, col='black')+
  facet_wrap(~site) +
  theme_cowplot() +
  labs(x='Growing Season', y='SOS precip anom \n(2-mo window around typical SOS)')


```

## Process temporal trends in phenological dates

* Similarly, we want to see if sites have seen significant changes in phenological dates over time. 

```{r phenotrendsLOOP, eval = T, echo = F, message = F, warning = F}
if (merge_climate_ndvi == TRUE) {

#all P values
statsdataframe <- tibble(
  site = numeric(), 
  sosF = numeric(), sosP = numeric(), sosSLOPE = numeric(),
  eosF = numeric(), eosP = numeric(), eosSLOPE = numeric(),
  gslF = numeric(), gslP = numeric(), gslSLOPE = numeric(), 
  maxndviF = numeric(), maxndviP = numeric(), maxNDVISLOPE = numeric())

changedf <- NutNetPheno_wide %>% dplyr::select(site, gs, ChangeSOS, ChangeEOS, ChangeGSL, Change_ndvi_max) %>% 
  gather(Type, Change, -gs, -site) %>% 
  left_join(longtermsites)

statssite_summary <- statsdataframe

for (i in c(1:nrow(longtermsites))) {
  a <- i
  sitefilter <- changedf %>% filter(sitenum2 == a)
  
  sosmod<-if (nrow(filter(sitefilter, Type == 'ChangeSOS')) > 2) {
    lm(Change ~ gs, data = filter(sitefilter, Type == 'ChangeSOS'))
  } else {sosmod <- statsdataframe}
   sosF <-   if (nrow(filter(sitefilter, Type == 'ChangeSOS')) > 2) {Anova(sosmod)$`F value`[1]} else {NA}
   sosP <-   if (nrow(filter(sitefilter, Type == 'ChangeSOS')) > 2) {Anova(sosmod)$`Pr(>F)`[1]} else {NA}
  sosSLOPE <- if (nrow(filter(sitefilter, Type == "ChangeSOS")) > 2) { sosmod$coefficients[2] } else {NA}

  eosmod<-if (nrow(filter(sitefilter, Type == 'ChangeEOS')) > 2) {
    (lm(Change ~ gs, data = filter(sitefilter, Type == 'ChangeEOS')))
  } else {eosmod <- statsdataframe}
  eosF <-   if (nrow(filter(sitefilter, Type == 'ChangeEOS'))>2) {Anova(eosmod)$`F value`[1]} else {NA}
  eosP <-   if (nrow(filter(sitefilter, Type == 'ChangeEOS')) > 2) {Anova(eosmod)$`Pr(>F)`[1]} else {NA}
  eosSLOPE <- if (nrow(filter(sitefilter, Type == "ChangeEOS")) > 2) { (eosmod)$coefficients[2] } else {NA}
  
  gslmod<-if (nrow(filter(sitefilter, Type == 'ChangeGSL')) > 2) {
    (lm(Change ~ gs, data = filter(sitefilter, Type == 'ChangeGSL')))
  } else {gslmod <- statsdataframe}
  gslF <-   if (nrow(filter(sitefilter, Type == 'ChangeGSL')) > 2) {Anova(gslmod)$`F value`[1]} else {NA}
  gslP <-   if (nrow(filter(sitefilter, Type == 'ChangeGSL')) > 2) {Anova(gslmod)$`Pr(>F)`[1]} else {NA}
  gslSLOPE <- if (nrow(filter(sitefilter, Type == "ChangeGSL")) > 2) { gslmod$coefficients[2] } else {NA}
  
  maxndvimod<-if (nrow(filter(sitefilter, Type == 'Change_ndvi_max')) > 2) {
    (lm(Change ~ gs, data = filter(sitefilter, Type == 'Change_ndvi_max')))
  } else {gslmod <- statsdataframe}
  maxndviF <-   if (nrow(filter(sitefilter, Type == 'Change_ndvi_max')) > 2) {Anova(maxndvimod)$`F value`[1]} else {NA}
  maxndviP <-   if (nrow(filter(sitefilter, Type == 'Change_ndvi_max')) > 2) {Anova(maxndvimod)$`Pr(>F)`[1]} else {NA}  
  maxNDVISLOPE <- if (nrow(filter(sitefilter, Type == "Change_ndvi_max")) > 2) { maxndvimod$coefficients[2] } else {NA}
  
  stats <- tibble(a, sosF, sosP, sosSLOPE, eosF, eosP, eosSLOPE, gslF, gslP, gslSLOPE, maxndviF, maxndviP, maxNDVISLOPE)
  statssite_summary <- rbind (stats, statssite_summary)
}


pheno_trend_df <- (dplyr::select(statssite_summary, a, sosF, eosF, gslF, maxndviF) %>% 
                  gather(Type, Value, -a) %>% mutate(Stat = 'F value')) %>%
  bind_rows((dplyr::select(statssite_summary, a, sosP, eosP, gslP, maxndviP) %>% 
               gather(Type, Value, -a) %>% 
               mutate(Stat = 'P value'))) %>%
  bind_rows(select(statssite_summary, a, sosSLOPE, eosSLOPE, gslSLOPE, maxNDVISLOPE) %>%
              gather(Type, Value, -a) %>%
              mutate(Stat = "Slope")) %>%
  rename(sitenum2 = a) %>%
  mutate(Type = ifelse(Type == 'sosF' | Type == 'sosP' | Type == "sosSLOPE", 'sos',
                       ifelse(Type == 'eosF' | Type == 'eosP' | Type == "eosSLOPE", 'eos',#,
                                     ifelse(Type == 'gslF' | Type == 'gslP' | Type == "gslSLOPE", 'gsl', 
                                            ifelse(Type =="maxndviF" | Type == "maxndviP" | Type == "maxNDVISLOPE", "maxndvi", NA))))) %>%
  full_join((longtermsites %>% select(-sitenum)))  %>% 
  dplyr::select(Type, Value, Stat, site ) %>%
  full_join(NutNetPheno_wide %>% dplyr::select(site, gs, ChangeEOS, ChangeSOS, ChangeGSL, Change_ndvi_max) %>% 
               gather(Type, CHANGE, -gs, -site) %>% 
               mutate(Type = ifelse(Type == 'ChangeEOS', 'eos', 
                                           ifelse(Type == 'ChangeSOS', 'sos', 
                                                  ifelse(Type == "ChangeGSL", "gsl",
                                                         ifelse(Type == "Change_ndvi_max", "maxndvi", 
                                                                ifelse(Type =="ChangeEOS", "eso", NA)))))))  %>%
  mutate(includeCHANGE = ifelse(Stat == 'P value' & Value <= 0.05, 'sig', 'ns')) %>%
  filter(Stat != "F value")

write_csv(pheno_trend_df, 'pheno_trend_df.csv')

} else 
{ print("Not merging weather and ndvi")}

pheno_trend_df <- read_csv("./pheno_trend_df.csv", col_types = cols())


kable(pheno_trend_df %>% 
  filter(Stat == "P value") %>%
  group_by(Type, Stat, site, includeCHANGE) %>%
  summarise(N = n()) %>%
  ungroup() %>%
  group_by(Type, includeCHANGE) %>%
  summarise(N = n()) %>%
  pivot_wider(names_from = includeCHANGE, values_from = N) %>%
  rename(`Response variable` = Type,
         `Sites w/ ns temporal trend` = ns,
         `Sites w/ sig temporal trend` = sig) %>%
    mutate(`Fraction sig. sites` = round(`Sites w/ sig temporal trend` / (`Sites w/ ns temporal trend` + `Sites w/ ns temporal trend`), 2)))

```


## Process interannual weather variability + interannual phenological anomolies

- Also interesting to look at yearly deviations in GS length + climate anomaly

```{r climatephenoLOOP, eval = T, echo = F, message = F, warning = F}
if (merge_climate_ndvi == TRUE) {

  
#all P values
ANOM_statsdataframe <- tibble(
  site = numeric(), 
  sos_sos2motemp_F = numeric(), sos_sos2motemp_P = numeric(), sos_sos2motemp_SLOPE = numeric(),
  gsl_max5motemp_F = numeric(), gsl_max5motemp_P = numeric(), gls_max5motemp_SLOPE = numeric(),
  maxNDVI_max5motemp_F = numeric(), maxNDVI_max5motemp_P = numeric(), maxNDVI_max5motemp_SLOPE = numeric(),
  
  sos_sos2moprecip_F = numeric(), sos_sos2moprecip_P = numeric(), sos_sos2moprecip_SLOPE = numeric(),
  gsl_max5moprecip_F = numeric(), gsl_max5moprecip_P = numeric(), gls_max5moprecip_SLOPE = numeric(),
  maxNDVI_max5moprecip_F = numeric(), maxNDVI_max5moprecip_P = numeric(), maxNDVI_max5moprecip_SLOPE = numeric(),

  gsl_maxNDVI_F = numeric(), gsl_maxNDVI_P = numeric(), gsl_maxNDVI_SLOPE = numeric())


eachyr <- climate_anom_df %>% select(-includeCHANGE, -Value) %>%
  pivot_wider(values_from = Anomaly, names_from = Type) %>% filter(Stat == "P value") %>%
  inner_join(NutNetPheno_wide) %>%
  select(site, gs, 
         ChangeSOS, ChangeEOS, anom_2moTemp_sos,anom_2moPrecip_sos, 
         ChangeGSL, anom_5moTemp_max, anom_5moPrecip_max,
         GSLength, Change_ndvi_max) %>%
  full_join(longtermsites %>% select(-sitenum)) %>%
  filter(!is.na(anom_2moTemp_sos)) %>%
  mutate(ChangeGSL = as.numeric(ChangeGSL),
         GSLength = as.numeric(GSLength))

anom_statssite_summary <- ANOM_statsdataframe
for (i in c(1:nrow(longtermsites))) {
  a <- i
  sitefilter <- eachyr %>% filter(sitenum2 == a)
  
  anom_sosmod<-if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) {
    lm(ChangeSOS ~ anom_2moTemp_sos, data = filter(sitefilter))
  } else {anom_sosmod <- ANOM_statsdataframe}
   sos_sos2motemp_F <-   if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) {Anova(anom_sosmod)$`F value`[1]} else {NA}
   sos_sos2motemp_P <-   if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) {Anova(anom_sosmod)$`Pr(>F)`[1]} else {NA}
  sos_sos2motemp_SLOPE <- if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) { anom_sosmod$coefficients[2] } else {NA}
    # sitefilter %>% ggplot(aes(x = anom_2moTemp_sos, y = ChangeSOS)) + geom_point()+geom_smooth(method = "lm")
    # Anova(lm(ChangeSOS ~ anom_2moTemp_sos, data = sitefilter))
    
  anom_maxmod<-if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {
    lm(ChangeGSL ~ anom_5moTemp_max, data = filter(sitefilter))
  } else {anom_maxmod <- ANOM_statsdataframe}
   gsl_max5motemp_F <-   if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {Anova(anom_maxmod)$`F value`[1]} else {NA}
   gsl_max5motemp_P <-   if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {Anova(anom_maxmod)$`Pr(>F)`[1]} else {NA}
   gsl_max5motemp_SLOPE <- if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) { anom_maxmod$coefficients[2] } else {NA}
  
  anom_maxNDVImod<-if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) {
    lm(Change_ndvi_max ~ anom_5moTemp_max, data = filter(sitefilter))
  } else {anom_maxNDVImod <- ANOM_statsdataframe}
   maxNDVI_max5motemp_F <-   if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) {Anova(anom_maxNDVImod)$`F value`[1]} else {NA}
   maxNDVI_max5motemp_P <-   if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) {Anova(anom_maxNDVImod)$`Pr(>F)`[1]} else {NA}
  maxNDVI_max5motemp_SLOPE <- if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) { anom_maxNDVImod$coefficients[2] } else {NA}
  
  anom_sos_sos2moprecip_mod<-if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) {
    lm(ChangeSOS ~ anom_2moPrecip_sos, data = filter(sitefilter))
  } else {anom_sos_sos2moprecip_mod <- ANOM_statsdataframe}
   sos_sos2moprecip_F <-   if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) {Anova(anom_sos_sos2moprecip_mod)$`F value`[1]} else {NA}
   sos_sos2moprecip_P <-   if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) {Anova(anom_sos_sos2moprecip_mod)$`Pr(>F)`[1]} else {NA}
  sos_sos2moprecip_SLOPE <- if (nrow(filter(sitefilter, !is.na(ChangeSOS))) > 2) { anom_sos_sos2moprecip_mod$coefficients[2] } else {NA}
  
  anom_maxprecipmod<-if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {
    lm(ChangeGSL ~ anom_5moPrecip_max, data = filter(sitefilter))
  } else {anom_maxprecipmod <- ANOM_statsdataframe}
   gsl_max5moprecip_F <-   if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {Anova(anom_maxprecipmod)$`F value`[1]} else {NA}
   gsl_max5moprecip_P <-   if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {Anova(anom_maxprecipmod)$`Pr(>F)`[1]} else {NA}
   gsl_max5moprecip_SLOPE <- if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) { anom_maxprecipmod$coefficients[2] } else {NA}
  
  anom_maxNDVIprecipmod<-if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) {
    lm(Change_ndvi_max ~ anom_5moPrecip_max, data = filter(sitefilter))
  } else {anom_maxNDVIprecipmod <- ANOM_statsdataframe}
   maxNDVI_max5moprecip_F <-   if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) {Anova(anom_maxNDVIprecipmod)$`F value`[1]} else {NA}
   maxNDVI_max5moprecip_P <-   if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) {Anova(anom_maxNDVIprecipmod)$`Pr(>F)`[1]} else {NA}
  maxNDVI_max5moprecip_SLOPE <- if (nrow(filter(sitefilter, !is.na(Change_ndvi_max))) > 2) { anom_maxNDVIprecipmod$coefficients[2] } else {NA}
  
  gsl_maxNDVImod<-if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {
    lm(Change_ndvi_max ~ ChangeGSL, data = filter(sitefilter))
  } else {gsl_maxNDVImod <- ANOM_statsdataframe}
   gsl_maxNDVI_F <-   if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {Anova(gsl_maxNDVImod)$`F value`[1]} else {NA}
   gsl_maxNDVI_P <-   if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) {Anova(gsl_maxNDVImod)$`Pr(>F)`[1]} else {NA}
   gsl_maxNDVI_SLOPE <- if (nrow(filter(sitefilter, !is.na(ChangeGSL))) > 2) { gsl_maxNDVImod$coefficients[2] } else {NA}  
  
  
  stats <- tibble(a, sos_sos2motemp_F, sos_sos2motemp_P, sos_sos2motemp_SLOPE,
                  gsl_max5motemp_F, gsl_max5motemp_P, gsl_max5motemp_SLOPE,
                  maxNDVI_max5motemp_F, maxNDVI_max5motemp_P, maxNDVI_max5motemp_SLOPE,
                  
                  sos_sos2moprecip_F, sos_sos2moprecip_P, sos_sos2moprecip_SLOPE,
                  gsl_max5moprecip_F, gsl_max5moprecip_P, gsl_max5moprecip_SLOPE,
                  maxNDVI_max5moprecip_F, maxNDVI_max5moprecip_P, maxNDVI_max5moprecip_SLOPE,
                  
                  gsl_maxNDVI_F, gsl_maxNDVI_P, gsl_maxNDVI_SLOPE)
  anom_statssite_summary <- rbind (stats, anom_statssite_summary)
}


overall_cc_mod <- ClimChange %>%
  split(.$site) %>%
  map(~lm(tmp_degrees_Celsius ~ plotdate, data = .))

overall_cc_site <- overall_cc_mod %>%
  map(summary)

overall_cc_p <- overall_cc_mod %>%
  map(summary) %>%
  map_dbl(~.$coefficients[2,4]) %>%
  as_tibble() %>%
  rename(OverallTemp_pvalue = value) %>%
  mutate(site = names(overall_cc_mod))

overall_cc_slope <- overall_cc_mod %>%
  map(summary) %>%
  map_dbl(~.$coefficients[2,1]) %>%
  as_tibble() %>%
  rename(OverallTemp_slope = value) %>%
  mutate(site = names(overall_cc_mod),
         )

overall_cc <- full_join(overall_cc_p, overall_cc_slope)

# ClimChange %>%
#   filter(site == "sereng") %>%
#   ggplot(aes(x = plotdate, y = tmp_degrees_Celsius)) +
#   geom_point()

anom_vs_climate <- anom_statssite_summary %>%
  transmute(sitenum2 = a,
    ChangeSOS_TempAnom_2mo_sos_pvalue = sos_sos2motemp_P, 
         ChangeSOS_TempAnom_2mo_sos_slope = sos_sos2motemp_SLOPE,
         ChangeSOS_PrecipAnom_2mo_sos_pvalue = sos_sos2moprecip_P,
         ChangeSOS_PrecipAnom_2mo_sos_slope = sos_sos2moprecip_SLOPE,
         
         Change_ndvi_max_TempAnom_5mo_max_pvalue = maxNDVI_max5motemp_P,
         Change_ndvi_max_TempAnom_5mo_max_slope = maxNDVI_max5motemp_SLOPE,
         Change_ndvi_max_PrecipAnom_5mo_max_pvalue = maxNDVI_max5moprecip_P,
         Change_ndvi_max_PrecipAnom_5mo_max_slope = maxNDVI_max5moprecip_SLOPE,
         Change_ndvi_max_ChangeGSL_pvalue = gsl_maxNDVI_P,
         Change_ndvi_max_ChangeGSL_slope = gsl_maxNDVI_SLOPE,
         
         ChangeGSL_TempAnom_5mo_max_pvalue= gsl_max5motemp_P,
         ChangeGSL_TempAnom_5mo_max_slope= gsl_max5motemp_SLOPE,
         ChangeGSL_PrecipAnom_5mo_max_pvalue= gsl_max5moprecip_P,
         ChangeGSL_PrecipAnom_5mo_max_slope= gsl_max5moprecip_SLOPE) %>%
    full_join((longtermsites %>% select(-sitenum))) %>%
  full_join(overall_cc) %>%
  select(-sitenum2)


write_csv(anom_vs_climate, 'anom_vs_climate.csv')

} else 
{ print("Not merging weather and ndvi")}

anom_vs_climate <- read_csv("./anom_vs_climate.csv", col_types = cols())
```


# Make final dataframes

`df_annual` contains a row for every growing season at each site. It includes information like climate and phenological anomalies, annual biomass, dates, etc.
`df_site` contains a single row for each site. It includes information like N deposition, soil nutrients, species richness. 

Please see excel sheet for metadata/column descriptions

These two data frames can be combined into a `df_merge`.

See the sites which are included in this final analysis. 


```{r finaldfs, echo=F, message=F, warning=F}
if (create_final_df == TRUE) {

#this is site summary
 df_site <-   (longtermsites %>% mutate(check = "included") %>% dplyr::select(site, check)) %>%
  full_join(SiteInfo) %>%
  full_join(climate_anom_df %>%
  filter(!is.na(site)) %>%
  select(site, Type, Stat, Value) %>%
  unique() %>% 
  pivot_wider(names_from = c(Type, Stat),
              values_from = Value) %>%
  rename(TempTrend36yr_2mo_sos_pvalue = `anom_2moTemp_sos_P value`,
         TempTrend36yr_2mo_sos_slope = anom_2moTemp_sos_Slope, 
         TempTrend36yr_5mo_max_pvalue = `anom_5moTemp_max_P value`,
         TempTrend36yr_5mo_max_slope = anom_5moTemp_max_Slope,
         
         PrecipTrend36yr_2mo_sos_pvalue = `anom_2moPrecip_sos_P value`,
         PrecipTrend36yr_2mo_sos_slope = anom_2moPrecip_sos_Slope, 
         PrecipTrend36yr_5mo_max_pvalue = `anom_5moPrecip_max_P value`,
         PrecipTrend36yr_5mo_max_slope = anom_5moPrecip_max_Slope)) %>%
  
  full_join(pheno_trend_df %>%
  filter(!is.na(site)) %>%
  select(site, Type, Stat, Value) %>%
  unique() %>% 
  pivot_wider(names_from = c(Type, Stat),
              values_from = Value) %>%
  rename(PhenoTrendXyr_sos_pvalue = `sos_P value`,
         PhenoTrendXyr_sos_slope = sos_Slope,
         
         PhenoTrendXyr_eos_pvalue = `eos_P value`,
         PhenoTrendXyr_eos_slope = eos_Slope,
         
         PhenoTrendXyr_gsl_pvalue = `gsl_P value`,
         PhenoTrendXyr_gsl_slope = gsl_Slope,
         
         PhenoTrendXyr_maxndvi_pvalue = `maxndvi_P value`,
         PhenoTrendXyr_maxndvi_slope = maxndvi_Slope)) %>%
  arrange(site) %>% 
  dplyr::select(-"anom_1moTemp_sos_P value", -"anom_3moTemp_sos_P value", -"anom_4moTemp_sos_P value", -"anom_1moTemp_max_P value", -"anom_3moTemp_max_P value",
                -"anom_7moTemp_max_P value", -"anom_9moTemp_max_P value", -"anom_1moPrecip_sos_P value", -"anom_3moPrecip_sos_P value", 
                -"anom_4moPrecip_sos_P value", -"anom_1moPrecip_max_P value" , -"anom_3moPrecip_max_P value", -"anom_7moPrecip_max_P value", -"anom_9moPrecip_max_P value") %>%
  full_join(anom_vs_climate)


#####
#working on creating the wide data  
tempanom_wide <- climate_anom_df %>%
  filter(Stat == "P value", !is.na(site)) %>%
  select(Type, site, gs, Anomaly) %>%
  unique() %>%
  pivot_wider(names_from = Type, 
              values_from = Anomaly) %>%
  rename(TempAnom_2mo_sos = anom_2moTemp_sos,
         TempAnom_5mo_max = anom_5moTemp_max,
         PrecipAnom_2mo_sos = anom_2moPrecip_sos,
         PrecipAnom_5mo_max = anom_5moPrecip_max)# %>%

# aggregated 
df_annual <- NutNetPheno_wide  %>%
  full_join(tempanom_wide) %>%
  dplyr::select(site, gs,
                SOStypicaldate, SOSdate, ChangeSOS, TempAnom_2mo_sos, PrecipAnom_2mo_sos, 
                MAXtypicaldate, MAXdate, ChangeMAX, TempAnom_5mo_max, PrecipAnom_5mo_max, 
                EOStypicaldate, EOSdate, ChangeEOS, 
                TypicalGSL, GSLength, ChangeGSL,
                TypicalMaxNDVI, MAXndvi, Change_ndvi_max) %>%
  full_join(sitenpp) %>%
  arrange(site, gs)


df_merge <- df_annual %>% full_join(df_site)


write_csv(df_annual, 'df_annual.csv')
write_csv(df_site, 'df_site.csv')
write_csv(df_merge, 'df_merge.csv')

} else {
  print("not creating final data sets")
}

df_annual <- read_csv("./df_annual.csv", col_types = cols())
df_site <- read_csv("./df_site.csv", col_types = cols())
df_merge <- read_csv("./df_merge.csv", col_types = cols())

(filter(df_site, check== "included")$site)
```




# Start analysis


## Overview

Sites included in this analyses occur across a wide range of locations (elevation, lat/long), soil characteristics (%C, ppmP, %N), climatic variables (MAP, MAT), and plant species richness. See fig `FigureS1.pdf`

```{r SiteHistogram, echo=F, message=F, warning=F}
var_labs <- as_labeller(c("SoilC" = "Soil C (%)",
                          "SoilN" = "Soil N (%)",
                          "SoilK" = "Soil K (ppmP",
                          "SoilP" = "Soil P (ppm)",
                          "latitude" = "Latitude",
                          "longitude" = "Longitude",
                          "TempMAT" = "Mean annual temperature (C)",
                          "RainMAP" = "Mean annual precipitation (mm)",
                          "ndep_2016" = "N deposition 2016 (mg N/m2/year)",
                          "Elevation" = "Elevation (m)",
                          "Richness" = "Species richness"))

site_histogram <- df_site %>%
  filter(check == "included") %>%
  select( ndep_2016, latitude, longitude, Elevation, TempMAT, RainMAP, Richness, SoilC, SoilN, SoilP, SoilK) %>%
  gather(Var, Val) %>%
  ggplot(aes(x = Val))+
  geom_histogram() + 
  theme_cowplot()+
  facet_wrap(~Var, scales = "free", labeller = var_labs)+
  labs(x = "Value") 

site_histogram
 
 ggsave("./FigureS1.pdf", site_histogram,dpi=300,width=12,height=7,units='in')

```


Sites also occur across continents and across different ecosystems (habitats). (Note, mcdan and vargrass are the sites that don't have habitat information in the latest dropbox file)

```{r SiteTables, echo=F, message=F, warning=F}
kable(df_site %>%
  filter(check == "included")  %>% 
        group_by(continent) %>%
        summarise(`N sites` = n()) %>% 
        arrange(-`N sites`))
 
kable(df_site %>%
  filter(check == "included")  %>% 
        group_by(habitat) %>%
        summarise(`N sites` = n()) %>% 
        arrange(-`N sites`))
``` 

Here is a map of the sites:

```{r sitemap, echo=F}
world <- ne_countries(scale = "medium", returnclass = "sf")
ggplot(data = world, label = site) +
  geom_sf() +
  geom_point(data = filter(df_site, check == "included"), aes(x = longitude, y = latitude), col = "orange2") +
  theme_bw() 
```

Also there has been a range of phenological change and change in maximum NDVI (from site-level means). See fig `FigureS4_histograms.pdf`

```{r, echo=F, message=F, warning=F} 
ChangeHist <- df_merge %>% 
  filter(check == "included", !is.na(ChangeGSL)) %>%
  select(ChangeGSL, ChangeSOS, ChangeEOS, Change_ndvi_max) %>%
  rename('Change in GSL (days)' = ChangeGSL,
         "Change in max NDVI (NDVI)" = Change_ndvi_max,
         "Change in green-up date (days)" = ChangeSOS,
         "Change in senescence date (days)" = ChangeEOS) %>%
  gather("Type",  "Value") %>%
  ggplot(aes(x=Value))+
  geom_histogram()+
  theme_cowplot()+
  facet_wrap(~Type, scales = "free")

ChangeHist
ggsave("./FigureS4_histograms.pdf", ChangeHist,dpi=600,width=7,height=5,units='in')
```


## NDVI is an okay predictor of productivity

See fig `FigureS3.pdf`

Of course, some arguments get made that NDVI is less subject to bias/error than on-the-ground data collection. But the more logical reason for any mismatch here is that NDVI was measured *adjacent* to the main NutNet plots (because we didn't want to capture the influence of nutrients during the years when nutrient addition started).

Logistically, I think it makes most sense to include sites that have >= 3 years of biomass+ndvi data in this trend. Because it's not reasonable to think that a tallgrass prairie is going to have the same ndvi-biomass relationship as an alpine meadow and we want to capture the site-trend.

```{r ndviprod, echo=F, message=F, warning=F, include=F}
# #whole dataset trend (all years) if want to actually go this route instead....
# df_annual %>%
#   filter(!is.na(ANPP_control), !is.na(MAXndvi)) %>% 
#   ggplot(aes(x = (ANPP_control), y = (MAXndvi), col = site)) +
#   geom_point(pch = 21) +
#   scale_x_continuous(trans =log_trans(), breaks = c(1, 5, 10, 50, 100, 400, 1000)) +
#   theme_cowplot() +
#   ylim(0,1) +
#   guides(col = F) +
#   geom_smooth(fill = NA, method = 'lm', col = 'black', formula = y ~ x - 1) +
#   geom_smooth(fill = NA, lwd = .4, method = 'lm') +
#   labs(y = 'Maximum NDVI', x = 'ANPP, Control plots avg. (g/m2)', title = "Max NDVI is okay predictor of ANPP")
# 
# summary(lm(ANPP_control ~ 0 + log(MAXndvi), data = filter(df_annual, !is.na(ANPP_control), !is.na(MAXndvi)))) 
# 
# 

#relationship between max NDVI and ANPP, only sites w/ 3+ years 
ndvi_bio_legend <- get_legend(tibble(Continent = c("Africa", "Asia", "Australia", "Europe", "N. America", "S. America"),
       color = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#a65628"),
       shape = c(21:25, 21)) %>%
  ggplot(aes(x = 1, y = 1, pch = Continent, col = Continent)) +
  geom_point() +
  scale_shape_manual(values = c(21:25, 21)) +
  scale_color_manual(values = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#a65628")) +
  theme_cowplot())

NDVI_biomass <- (df_merge %>%
  filter(!is.na(ANPP_control), !is.na(MAXndvi), YrsBiomass >= 3) %>% 
  ggplot(aes(y = (ANPP_control), x = (MAXndvi), col = site)) +
  theme_cowplot() +
  ylim(0,1839) +
  guides(col = F) +
    
  geom_point(inherit.aes = F, aes(x = MAXndvi, y = ANPP_control), pch = 21, data = filter(df_merge, YrsBiomass>=3, continent == "Africa"), col = "#e41a1c") +
  geom_smooth(fill = NA, lwd = .4, method = 'lm', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control, pch = site), data = filter(df_merge, YrsBiomass>=3, continent == "Africa"), col = "#e41a1c") +
  # geom_smooth(fill = NA, lwd = .4, method = 'lm') +

  geom_point(inherit.aes = F, aes(x = MAXndvi, y = ANPP_control), pch = 22, data = filter(df_merge, YrsBiomass>=3, continent == "Asia"), col = "#377eb8") +
  geom_smooth(fill = NA, lwd = .4, method = 'lm', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control, pch = site), data = filter(df_merge, YrsBiomass>=3, continent == "Asia"), col = "#377eb8") +
  # geom_smooth(fill = NA, lwd = .4, method = 'lm') +

  geom_point(inherit.aes = F, aes(x = MAXndvi, y = ANPP_control), pch = 23, data = filter(df_merge, YrsBiomass>=3, continent == "Australia"), col = "#4daf4a") +
  geom_smooth(fill = NA, lwd = .4, method = 'lm', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control, pch = site), data = filter(df_merge, YrsBiomass>=3, continent == "Australia"), col = "#4daf4a") +
  # geom_smooth(fill = NA, lwd = .4, method = 'lm') +
    
  geom_point(inherit.aes = F, aes(x = MAXndvi, y = ANPP_control), pch = 24, data = filter(df_merge, YrsBiomass>=3, continent == "Europe"), col = "#984ea3") +
  geom_smooth(fill = NA, lwd = .4, method = 'lm', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control, pch = site), data = filter(df_merge, YrsBiomass>=3, continent == "Europe"), col = "#984ea3") +
  # geom_smooth(fill = NA, lwd = .4, method = 'lm') +
                        
    geom_point(inherit.aes = F, aes(x = MAXndvi, y = ANPP_control), pch = 25, data = filter(df_merge, YrsBiomass>=3, continent == "North America"), col = "#ff7f00") +
  geom_smooth(fill = NA, lwd = .4, method = 'lm', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control, pch = site), data = filter(df_merge, YrsBiomass>=3, continent == "North America"), col = "#ff7f00") +
  # geom_smooth(fill = NA, lwd = .4, method = 'lm') +

                            
    geom_point(inherit.aes = F, aes(x = MAXndvi, y = ANPP_control), pch = 21, data = filter(df_merge, YrsBiomass>=3, continent == "South America"), col = "#a65628") +
  geom_smooth(fill = NA, lwd = .4, method = 'lm', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control, pch = site), data = filter(df_merge, YrsBiomass>=3, continent == "South America"), col = "#a65628") +
  # geom_smooth(fill = NA, lwd = .4, method = 'lm') +
        
  geom_smooth(fill = NA, method = 'lm', col = 'black', inherit.aes = F, aes(x = MAXndvi, y = ANPP_control)) + # if force thru 0, formula = y ~ x - 1) +
  labs(x = 'Maximum NDVI', y = 'ANPP_control, Control plots avg. (g/m2)'))#, title = "Max NDVI is okay predictor of ANPP_control\n(sites with 3+ years biomass)"))

PLOTNDVIBIO <-plot_grid(NDVI_biomass, ndvi_bio_legend, rel_widths = c(3, 1))
ggsave("./FigureS3.pdf", PLOTNDVIBIO,dpi=300,width=7,height=5,units='in')


m1 <- (lm(ANPP_control ~ 0 + log(MAXndvi), data = filter(df_annual, !is.na(ANPP_control), !is.na(MAXndvi), YrsBiomass >= 3))) #r2 = .27, p<0.001
m2 <- (lm(ANPP_control ~ 0 + (MAXndvi), data = filter(df_annual, !is.na(ANPP_control), !is.na(MAXndvi), YrsBiomass >= 3))) #r2 = .72, p<0.001
m3 <- (lm(ANPP_control ~  log(MAXndvi), data = filter(df_annual, !is.na(ANPP_control), !is.na(MAXndvi), YrsBiomass >= 3))) #r2 = .72, p<0.001
m4 <- (lm(ANPP_control ~ (MAXndvi), data = filter(df_annual, !is.na(ANPP_control), !is.na(MAXndvi), YrsBiomass >= 3))) #r2 = .72, p<0.001
AIC(m1, m2, m3, m4) #m4 is the best
summary(m4)


```

Statistically, AIC scores were compared between 4 different models relating maximum NDVI to ANPP. Because NDVI tends to saturate at high levels of biomass production, sometimes (but not always!) ANPP is related to the log of the maximum NDVI. Sometimes (but not always!) the relationships between NDVI and ANPP are forced through the origin (0,0) as zero NDVI value should indicate no biomass. All of these options were compared with the following models:

1) intercept through (0,0); ANPP ~ log(maxNDVI)
2) intercept through (0,0); ANPP ~ maxNDVI
3) no fixed intercept; ANPP ~ log(maxNDVI)
4) no fixed intercept; ANPP ~ maxNDVI

Model 4 had the lowest AIC score, and yields the following relationship with a r-squared value of ```r summary(m4)$r.squared```:

ANPP = ```r summary(m4)$coefficients[1]``` + maxNDVI * ```r summary(m4)$coefficients[2]```

```{r echo=F, message=F, warning=F}
PLOTNDVIBIO
```


```{r, include=F, echo=F, message=F, warning=F}
Anova(lmer(ChangeSOS ~ TempAnom_2mo_sos + (1|site), 
           data = filter(df_annual, !is.na(ChangeSOS))))

Anova(lmer(as.numeric(ChangeGSL) ~ TempAnom_5mo_max + (1|site), 
           data = filter(df_annual, !is.na(ChangeGSL))))



Anova(lm(ChangeSOS ~ TempAnom_2mo_sos + PrecipAnom_2mo_sos, 
         data = filter(df_annual, !is.na(ChangeSOS))))

Anova(lmer(as.numeric(ChangeGSL) ~ TempAnom_5mo_max + PrecipAnom_5mo_max + gs + TempMAT + RainMAP +
             (1|site), data = filter(df_merge, !is.na(ChangeSOS))))

```

## Temporal changes in phenology

Table 1 - repeated measures (phenological change through time). Have there been directional, temporal shifts towards "greening" and earlier springs since ~1980? Look at fig `fig1_temporaltrends.pdf`

```{r RepeatedMeasures, echo=F, message=F, warning=F}
a<-Anova(lmer(ChangeSOS ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeSOS))))$`Df`

b1<-Anova(lmer(ChangeSOS ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeSOS))))$`Chisq`
b2<-Anova(lmer(ChangeSOS ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeSOS))))$`Pr(>Chisq)`

c1 <-Anova(lmer(ChangeEOS ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeEOS))))$`Chisq`
c2 <-Anova(lmer(ChangeEOS ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeEOS))))$`Pr(>Chisq)`

d1 <- Anova(lmer(Change_ndvi_max ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(Change_ndvi_max))))$`Chisq`
d2 <- Anova(lmer(Change_ndvi_max ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(Change_ndvi_max))))$`Pr(>Chisq)`

e1 <- Anova(lmer(as.numeric(ChangeGSL) ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeGSL))))$`Chisq`
e2 <- Anova(lmer(as.numeric(ChangeGSL) ~ site*gs + (1|gs), data = filter(df_merge, check == "included", !is.na(ChangeGSL))))$`Pr(>Chisq)`

kable(tibble("Variable" = c("site", "gs", "site:gs"), 
       "d.f." = a,
       
       "SOS date X2-value" = round(b1, digits = 2),
       "SOS date p-value" = round(b2, digits = 3),
       
       "EOS date X2-value" = round(c1, digits = 2),
       "EOS date p-value" = round(c2, digits = 3),
       
       "Max NDVI X2-value" = round(d1, digits = 2),
       "Max NDVI p-value" = round(d2, digits = 3),
       
       "GS Length X2-value" = round(e1, digits = 2),
       "GS Length p-value" = round(e2, digits = 3)))
```



```{r, echo=F, message=F, warning=F}
fig_sos_gs <- df_merge %>%
  filter(!is.na(ChangeSOS), check == "included") %>%
  ggplot(aes(y = ChangeSOS, x = gs, col = site)) +
  geom_jitter(height = 0, width = .2, pch = 21, col = "grey60")+
  guides(col = F) +
  geom_smooth(method = "lm", fill =NA, lty = 2, lwd = .5, 
              data = filter(df_merge, PhenoTrendXyr_gsl_pvalue <=0.05, !is.na(ChangeSOS), check == "included"),
              aes(group = site), col = "black") +
  theme_cowplot() +
    scale_color_manual(values = rep(c(1), 45))+
    labs ( x = "Growing season (year)",
  y = "Change in green-up date")

fig_ndvi_gs <- df_merge %>%
  filter(!is.na(Change_ndvi_max), check == "included") %>%
  ggplot(aes(y = Change_ndvi_max, x = gs, col = site)) +
  geom_jitter(height = 0, width = .2, pch = 21, col = "grey60")+
  guides(col = F) +
  geom_smooth(method = "lm", fill =NA, lty = 2, lwd = .5, 
              data = filter(df_merge, PhenoTrendXyr_gsl_pvalue <=0.05, !is.na(Change_ndvi_max), check == "included"),
              aes(group = site), col = "black") +
  theme_cowplot() +
  # geom_hline(yintercept = 0, lty = 2, col = "grey60")+
    scale_color_manual(values = rep(c(1), 45))+
    labs ( x = "Growing season (year)",
  y = "Change in maximum NDVI")

fig_gsl_gs <- df_merge %>%
  filter(!is.na(ChangeGSL), check == "included") %>%
  ggplot(aes(y = as.numeric(ChangeGSL), x = gs, col = site)) +
  geom_jitter(height = 0, width = .2, pch = 21, col = "grey60")+
  guides(col = F) +
  geom_smooth(method = "lm", fill =NA, lty = 2, lwd = .5, 
              data = filter(df_merge, PhenoTrendXyr_gsl_pvalue <=0.05, !is.na(ChangeGSL), check == "included"),
              aes(group = site), col = "black") +
  theme_cowplot() +
  # geom_hline(yintercept = 0, lty = 2, col = "grey60")+
    scale_color_manual(values = rep(c(1), 45))+
    labs ( x = "Growing season (year)",
  y = "Change in GS length")

# cor.test(pheno_long_df$Change_ndvi_max, pheno_long_df$ChangeGSL)

fig1_temporaltrends <- plot_grid(fig_sos_gs, 
                                 fig_ndvi_gs,
                                 fig_gsl_gs, 
                                 nrow = 3, labels = "AUTO")

fig1_temporaltrends
ggsave("./fig1_temporaltrends.pdf", fig1_temporaltrends,dpi=600,width=4.5,height=10,units='in')

```


## Interannual weather changes + phenological anomalies

Have warmer or wetter springs or growing seasons had an impact on phenological dates or productivity? Look at fig `Fig2_phenoENV.pdf`


```{r, echo=F, message=F, warning=F}
# #temp ----
f1 <- Anova(lmer(ChangeSOS ~ TempAnom_2mo_sos + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeSOS))  ))$`Chisq`
f2 <- Anova(lmer(ChangeSOS ~ TempAnom_2mo_sos + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeSOS))  ))$`Pr(>Chisq)`

g1 <-Anova(lmer(Change_ndvi_max ~ TempAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(Change_ndvi_max))  ))$`Chisq`
g2 <-Anova(lmer(Change_ndvi_max ~ TempAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(Change_ndvi_max))  ))$`Pr(>Chisq)`

h1 <- Anova(lmer(as.numeric(ChangeGSL) ~ TempAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeGSL))  ))$`Chisq`
h2 <- Anova(lmer(as.numeric(ChangeGSL) ~ TempAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeGSL))  ))$`Pr(>Chisq)`

# #precip ----

i1 <- Anova(lmer(ChangeSOS ~ PrecipAnom_2mo_sos + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeSOS))  ))$`Chisq`
i2 <- Anova(lmer(ChangeSOS ~ PrecipAnom_2mo_sos + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeSOS))  ))$`Pr(>Chisq)`

j1 <- Anova(lmer(Change_ndvi_max ~ PrecipAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(Change_ndvi_max))  ))$`Chisq`
j2 <- Anova(lmer(Change_ndvi_max ~ PrecipAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(Change_ndvi_max))  ))$`Pr(>Chisq)`

k1 <- Anova(lmer(as.numeric(ChangeGSL) ~ PrecipAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeGSL))  ))$`Chisq`
k2 <- Anova(lmer(as.numeric(ChangeGSL) ~ PrecipAnom_5mo_max + (1|site), data = filter(df_merge, check == "included", !is.na(ChangeGSL))  ))$`Pr(>Chisq)`

# #kable ----

kable(tribble(~"Phenology var.", ~ "Weather var.", ~"d.f.", ~"X2 value", ~"p value",
        "Change SOS date", "Temp anamoly 2mo prior to GU", 1, round(f1, digits = 2), round(f2, digits = 3),
        "Change SOS date", "Precip anamoly 2mo prior to GU", 1, round(i1, digits = 2), round(i2, digits = 3),
       
        "Change max NDVI", "Temp anamoly of 5mo GS", 1, round(g1, digits = 2), round(g2, digits = 3),
        "Change max NDVI", "Precip anamoly of 5mo GS", 1, round(j1, digits = 2), round(j2, digits = 3),

        "Change GS length", "Temp anamoly of 5mo GS", 1, round(h1, digits = 2), round(h2, digits = 3),
        "Change GS length", "Precip anamoly of 5mo GS", 1, round(k1, digits = 2), round(k2, digits = 3)))

```


```{r, echo=F, message=F, warning=F}
# #temp figs ----
fig_sostemp_gudate <- df_merge %>%
  filter(check == "included",!is.na(ChangeSOS)) %>%
  ggplot(aes(y = ChangeSOS, x = TempAnom_2mo_sos, col = site)) +
  geom_point(pch = 21, col = "grey60") +
  guides(col = F) +
  geom_smooth(method = "lm", fill = NA, lty = 2, lwd = .5,
              aes(group = site), col = "black",
              data = filter(df_merge, check == "included", !is.na(ChangeSOS), ChangeSOS_TempAnom_2mo_sos_pvalue <= 0.05)) +
  theme_cowplot() +
  scale_color_manual(values = rep(c(1), 45)) +
  geom_vline(xintercept = 0, lty = 2, col = "grey60") +
  geom_hline(yintercept = 0, lty = 2, col = "grey60") +
  labs(x = "Temp anamoly (C) 2mo prior to GU", y = "Change in \ngreen-up date (days)") +
  scale_x_continuous(limits = c(-11, 11),
                     breaks = c(-10, -5, 0, 5, 10))


fig_gutemp_maxndvi <- df_merge %>%
  filter(check == "included",!is.na(Change_ndvi_max)) %>%
  ggplot(aes(y = Change_ndvi_max, x = TempAnom_5mo_max, col = site)) +
  geom_point(pch = 21, col = "grey60") +
  guides(col = F) +
  geom_smooth(method = "lm", fill = NA, lty = 2, lwd = .5,
              aes(group = site), col = "black",
              data = filter(df_merge, check == "included", !is.na(Change_ndvi_max), Change_ndvi_max_TempAnom_5mo_max_pvalue <= 0.05)) +
  theme_cowplot() +
  scale_color_manual(values = rep(c(1), 45)) +
  geom_vline(xintercept = 0, lty = 2, col = "grey60") +
  geom_hline(yintercept = 0, lty = 2, col = "grey60") +
  labs(x = "Temp anamoly (C) of 5mo GS", y = "Change in \nmaximum NDVI") +
  scale_x_continuous(limits = c(-11, 11),
                     breaks = c(-10, -5, 0, 5, 10))

fig_gutemp_gsl <- df_merge %>%
  filter(check == "included",!is.na(ChangeGSL)) %>%
  ggplot(aes(y = ChangeGSL, x = TempAnom_5mo_max, col = site)) +
  geom_point(pch = 21, col = "grey60") +
  guides(col = F) +
  geom_smooth(method = "lm", fill = NA, lty = 2, lwd = .5,
              aes(group = site), col = "black",
              data = filter(df_merge, check == "included", !is.na(ChangeGSL), ChangeGSL_TempAnom_5mo_max_pvalue <= 0.05)) +
  theme_cowplot() +
  scale_color_manual(values = rep(c(1), 45)) +
  geom_vline(xintercept = 0, lty = 2, col = "grey60") +
  geom_hline(yintercept = 0, lty = 2, col = "grey60") +
  labs(x = "Temp anamoly (C) of 5mo GS", y = "Change in \ngrowing season length (days)") +
  scale_x_continuous(limits = c(-11, 11),
                     breaks = c(-10, -5, 0, 5, 10))


# #precip figs ----
fig_sosprecip_gudate <- df_merge %>%
  filter(check == "included",!is.na(ChangeSOS)) %>%
  ggplot(aes(y = ChangeSOS, x = PrecipAnom_2mo_sos, col = site)) +
  geom_point(pch = 21, col = "grey60") +
  guides(col = F) +
  geom_smooth(method = "lm", fill = NA, lty = 2, lwd = .5,
              aes(group = site), col = "black",
              data = filter(df_merge, check == "included", !is.na(ChangeSOS), ChangeSOS_PrecipAnom_2mo_sos_pvalue <= 0.05)) +
  theme_cowplot() +
  scale_color_manual(values = rep(c(1), 45)) +
  geom_vline(xintercept = 0, lty = 2, col = "grey60") +
  geom_hline(yintercept = 0, lty = 2, col = "grey60") +
  labs(x = "Precip anamoly (mm) 2mo prior to GU", y = "Change in \ngreen-up date (days)") +
    scale_x_continuous(limits = c(-500, 750), 
                       breaks = c(-500, -250, 0, 250, 500, 750))


fig_guprecip_maxndvi <- df_merge %>%
  filter(check == "included",!is.na(Change_ndvi_max)) %>%
  ggplot(aes(y = Change_ndvi_max, x = PrecipAnom_5mo_max, col = site)) +
  geom_point(pch = 21, col = "grey60") +
  guides(col = F) +
  geom_smooth(method = "lm", fill = NA, lty = 2, lwd = .5,
              aes(group = site), col = "black",
              data = filter(df_merge, check == "included", !is.na(Change_ndvi_max), Change_ndvi_max_PrecipAnom_5mo_max_pvalue <= 0.05)) +
  theme_cowplot() +
  scale_color_manual(values = rep(c(1), 45)) +
  geom_vline(xintercept = 0, lty = 2, col = "grey60") +
  geom_hline(yintercept = 0, lty = 2, col = "grey60") +
  labs(x = "Precip anamoly (mm) of 5mo GS", y = "Change in \nmaximum NDVI") +
    scale_x_continuous(limits = c(-500, 750), 
                       breaks = c(-500, -250, 0, 250, 500, 750))

fig_guprecip_gsl <- df_merge %>%
  filter(check == "included",!is.na(ChangeGSL)) %>%
  ggplot(aes(y = ChangeGSL, x = PrecipAnom_5mo_max, col = site)) +
  geom_point(pch = 21, col = "grey60") +
  guides(col = F) +
  geom_smooth(method = "lm", fill = NA, lty = 2, lwd = .5,
              aes(group = site), col = "black",
              data = filter(df_merge, check == "included", !is.na(ChangeGSL), ChangeGSL_PrecipAnom_5mo_max_pvalue <= 0.05)) +
  theme_cowplot() +
  scale_color_manual(values = rep(c(1), 45)) +
  geom_vline(xintercept = 0, lty = 2, col = "grey60") +
  geom_hline(yintercept = 0, lty = 2, col = "grey60") +
  labs(x = "Precip anamoly (mm) of 5mo GS", y = "Change in \ngrowing season length (days)") +
    scale_x_continuous(limits = c(-500, 750), 
                       breaks = c(-500, -250, 0, 250, 500, 750))


Fig2_phenoENV <- plot_grid(fig_sostemp_gudate, fig_sosprecip_gudate,
                           fig_gutemp_maxndvi, fig_guprecip_maxndvi, 
                           fig_gutemp_gsl, fig_guprecip_gsl,
                           nrow = 3, labels = "AUTO")
ggsave("./Fig2_phenoENV.pdf", Fig2_phenoENV,dpi=600,width=9,height=10,units='in')

Fig2_phenoENV
```



## PCAs

Could be a useful part of the fishing expedition...

```{r PCAs, echo=F, message=F, warning=F}
library(vegan)

pca_data <- df_merge %>%
  select(site, gs, Change_ndvi_max, 
         ChangeSOS, ChangeEOS, TempAnom_2mo_sos, PrecipAnom_2mo_sos,
         ChangeGSL, TempAnom_5mo_max, PrecipAnom_5mo_max,
         Elevation, RainMAP, TempMAT, 
         Richness, FractionExotic,
         ndep_change_2016_1984) %>%
  filter(!is.na(ChangeGSL), !is.na(PrecipAnom_2mo_sos), !is.na(Elevation), !is.na(FractionExotic)) %>%#, !is.na(MeanAsynch)))
  mutate(ChangeGSL = as.numeric(ChangeGSL))

data.rda <- rda(pca_data[,-1], scale=TRUE)

biplot(data.rda,
       display = c("species"), 
                   # "sites"),
       type = c("text"),
       # "points"),
       main = "PCA of all site-year combos")

pca_soil_data <- df_merge %>%
  select(site, gs, Change_ndvi_max, 
         ChangeSOS, ChangeEOS, TempAnom_2mo_sos, PrecipAnom_2mo_sos,
         ChangeGSL, TempAnom_5mo_max, PrecipAnom_5mo_max,
         Elevation, RainMAP, TempMAT, 
         Richness, FractionExotic,
         ndep_change_2016_1984, 
         SoilN, SoilP, SoilC, SoilK) %>%
  mutate(ChangeGSL = as.numeric(ChangeGSL)) %>%
  filter(!is.na(ChangeGSL), !is.na(PrecipAnom_2mo_sos), !is.na(Elevation), !is.na(FractionExotic)) %>%#, !is.na(MeanAsynch)))
  filter(!is.na(SoilP))#, !is.na(MeanAsynch))

soildata.rda <- rda(pca_soil_data[,-1], scale=TRUE)

 biplot(soildata.rda,
       display = c("species"), 
                   # "sotes"),
       type = c("text"),
       # "points"),
       main = "PCA of site-year combos WITH soil info")



# ggsave("./PCAfig.pdf", PCAFig,dpi=600,width=9,height=4,units='in')
# 
# ANOM_finalstats %>% 
#   filter(sig != "o") %>%
#   ggplot(aes(x = GSLength, y = anom_5moTemp_max)) + geom_point() + geom_smooth()
# 
# summary(lm(GSLength ~ anom_5moTemp_max, ANOM_finalstats))
# summary(lm(GSLength ~ anom_5moPrecip_max, ANOM_finalstats))
# 
# summary(lm(Change_ndvi_max ~ anom_5moTemp_max, ANOM_finalstats))
# summary(lm(Change_ndvi_max ~ anom_5moPrecip_max, ANOM_finalstats))
# summary(lm(Change_ndvi_max ~ GSLength, ANOM_finalstats))

```

In looking at sites which have *most* site-level data, there are ```r length(unique(pca_data$site))``` sites. 

If we want to look at soil nutrient levels, there are only ```r  length(unique(pca_soil_data$site))``` sites with complete data. 


## NutNet 2020 discussions

Running into issues with singularities, this never got resolved.....

Basically the goal is to go on a fishing expedition...
```{r, include=F, echo=F, message=F, warning=F}
library("cAIC4")
library("MuMIn")
# #Eric suggestions
summary(lmer(ChangeSOS ~ #change in green-up date from long-term avg.
             gs + #site * growing season (year)
             (gs | site),
           data = filter(df_merge, check == "included", !is.na(ChangeSOS))))

# then go on a fishing expedition - does the *temporal* signature of phenology (e.g. the SLOPE of green-up date thru time) map onto any abiotic or biotic site-level characteristics?
# year, or growing season, is implicit within the slopes being analyzed - this this is just a straight linear model
Anova(lmer(PhenoTrendXyr_sos_slope ~ #slope of green-up date change thru time
             abs(latitude) + #absolute value of latitude (since southern hemi sites!)
             Elevation + #elevation
             RainMAP + #long-term mean annual precip
             TempMAT + #long-term mean annual temp
             ndep_change_2016_1984 + #modeled nitrogen deposition
             Richness + #alpha diversity
             FractionExotic +  #percent exotic species
             # ObsYrMeanEven + #NAs in this variable; so removed
             # SoilC + SoilN + SoilP + #NAs in this variable; so removed
             (1 | continent),
             data = filter(df_merge, check=="included", !is.na(PhenoTrendXyr_sos_slope))))

# #dredge my fishing expedition
# #but it's all unhappy w/ singualiities
# m4 <- (lmer(PhenoTrendXyr_sos_slope ~ #slope of green-up date change thru time
#              abs(latitude) + #absolute value of latitude (since southern hemi sites!)
#              Elevation + #elevation
#              RainMAP + #long-term mean annual precip
#              TempMAT + #long-term mean annual temp
#              ndep_change_2016_1984 + #modeled nitrogen deposition
#              Richness + #alpha diversity
#              FractionExotic +  #percent exotic species
#              # ObsYrMeanEven + #NAs in this variable; so removed
#              # SoilC + SoilN + SoilP + #NAs in this variable; so removed
#              (1 | continent),
#              data = filter(df_merge, check=="included", !is.na(PhenoTrendXyr_sos_slope))))
# m4.step <- stepcAIC(m4,direction="backward", trace=TRUE, data=df) #cant do w/singularities
# options(na.action = "na.fail")
# dd <- dredge(m4) #hates singularities
# results <- subset(dd, delta < 4)
# importance(results) #mat, exotic
# Anova(lmer(PhenoTrendXyr_sos_slope ~ FractionExotic + (gs|site),
#            data = filter(df_merge, !is.na(PhenoTrendXyr_sos_slope), !is.na(FractionExotic))))

# #Eric suggestion -- singularities
# Anova(lmer(as.numeric(ChangeSOS) ~ 
#              gs *
#              (abs(latitude) + #absolute value of latitude (since southern hemi sites!)
#              Elevation + #elevation
#              RainMAP + #long-term mean annual precip
#              TempMAT + #long-term mean annual temp
#              ndep_change_2016_1984 + #modeled nitrogen deposition
#              Richness + #alpha diversity
#              FractionExotic) +  #percent exotic species
#              (gs | site),
#              data = filter(df_merge, !is.na(ChangeSOS), !is.na(FractionExotic),
#                            !is.na(Elevation))))
# 
# model <- lmer(ChangeSOS ~ gs *
#              (abs(latitude) + Elevation + RainMAP + TempMAT + 
#                 ndep_change_2016_1984 + Richness + FractionExotic) + 
#                (gs | site),
#              data = filter(df_merge, !is.na(ChangeSOS), !is.na(Elevation), !is.na(FractionExotic)))
# model.step <- stepcAIC(model,direction="backward", trace=TRUE, 
#                        data=filter(df_merge, !is.na(ChangeSOS), !is.na(Elevation), !is.na(FractionExotic))) #cant do w/singularities
# 
# model2 <- lmer(PhenoTrendXyr_sos_slope ~ 
#                  abs(latitude) +  Elevation + RainMAP + TempMAT + 
#                  ndep_change_2016_1984 + Richness + FractionExotic +  
#              (1 | continent),
#              data = filter(df_merge, !is.na(PhenoTrendXyr_sos_slope), !is.na(FractionExotic),
#                            !is.na(Elevation)))
# 
# test <- filter(df_merge, !is.na(PhenoTrendXyr_sos_slope), !is.na(FractionExotic),
#                            !is.na(Elevation))
# model2.step <- stepcAIC(model2,
#                         groupCandidates = c("continent") ,direction="backward", trace=TRUE, data=test)
# 
# 
# # finally, we'll address interannual variability. Do phenological variations across years map onto annual weather patterns? 
# # hmm, currently there is singularity in these models, will fix eventually
# Anova(lmer((ChangeSOS) ~ #change in green-up date from site long-term average
#              TempAnom_2mo_sos + #change in spring temp from site long-term average
#              PrecipAnom_2mo_sos + #change in spring precip from site long-term average
#              gs + #growing season (aka year)
#              as.factor(gs) + #year as a categorical factor
#              (1 | site / continent), 
#            data = filter(df_merge,
#                          !is.na(ChangeSOS), !is.na(continent), !is.na(TempAnom_2mo_sos))
#            ))
# 
# #eric
# data3 <- filter(df_merge, !is.na(ChangeSOS), !is.na(Elevation), !is.na(continent), !is.na(PrecipAnom_2mo_sos))
# model3 <- (lmer(ChangeSOS ~ 
#              gs*(TempAnom_2mo_sos + PrecipAnom_2mo_sos + 
#                Elevation + RainMAP + TempMAT + ndep_change_2016_1984 + Richness) +
#              (1 | site / continent), 
#            data = data3))
# 
# Anova(model3)
# a<-stepcAIC(model3,direction="backward", trace=TRUE, data=data3) #cant do w/singularities

```


## Make trendline of every site
`this code is available if needed`


